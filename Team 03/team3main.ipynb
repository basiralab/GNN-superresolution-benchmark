{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KGSBVsMyggJr"
   },
   "source": [
    "Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1isjrtnGggJt"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from sklearn.metrics import  mean_absolute_error\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "\n",
    "import csv\n",
    "import networkx as nx\n",
    "import community.community_louvain as community_louvain\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z_CULjiFggJv"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ubd7ADmggJv"
   },
   "source": [
    "# **Set the parameters**\n",
    "\n",
    "<p>Use option = 0 if you want to do training and validation using 2 of the folds (keeping the 3rd fold for final training and testing).</p>\n",
    "\n",
    "<p>Use option = 1 if you want to do final training (using 2 folds) and testing (with the 3rd fold).</p>\n",
    "\n",
    "Be sure to update the name of the files - doing 3 cross-validation in this project is done manually with the prepared data to create comparable results with the rest of the projects.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MakW1DwfggJw"
   },
   "outputs": [],
   "source": [
    "option = 1\n",
    "\n",
    "#model parameters\n",
    "model_parameter = {\n",
    "    \"batch_size\" : 16,\n",
    "    \"architecture\" : [250,500,800,268,268,268],\n",
    "    \"dropout\" : 0.4,\n",
    "    \"input_dim\" : 160,\n",
    "    \"output_dim\" : 268,\n",
    "    \"epoch\" : 15,\n",
    "    \"learning_rate\" : 0.001\n",
    "}\n",
    "\n",
    "#seed\n",
    "random_seed = 42\n",
    "\n",
    "# path\n",
    "# path for training and validation (option 0)\n",
    "path_lr_data = 'lr_split_AandB_training.csv'\n",
    "path_hr_data = 'hr_split_AandB_training.csv'\n",
    "path_lr_data_test = 'lr_split_AandB_validation.csv'\n",
    "path_hr_data_test = 'hr_split_AandB_validation.csv'\n",
    "\n",
    "# path for final training and testing (option 1)\n",
    "path_lr_data = 'lr_split_AandB_finaltraining.csv'\n",
    "path_hr_data = 'hr_split_AandB_finaltraining.csv'\n",
    "path_lr_data_test = 'lr_clusterC.csv'\n",
    "path_hr_data_test = 'hr_clusterC.csv'\n",
    "\n",
    "# path to save the evaluation metrics to - this means that Cluster C will be used for testing\n",
    "path_eval_matrics = '03-clusterCV-split3.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BwG9EsabggJw"
   },
   "source": [
    "Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F5JOo8lhggJw",
    "outputId": "23fac5e5-1569-4ae3-d1ed-a70d9340234c"
   },
   "outputs": [],
   "source": [
    "# Set a fixed random seed for reproducibility across multiple libraries\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "# Check for CUDA (GPU support) and set device accordingly\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"CUDA is available. Using GPU.\")\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed)  # For multi-GPU setups\n",
    "    # Additional settings for ensuring reproducibility on CUDA\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDA not available. Using CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tbzm_S1bggJx"
   },
   "source": [
    "Intermediate functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i1e5XYAlggJx"
   },
   "outputs": [],
   "source": [
    "class MatrixVectorizer:\n",
    "    \"\"\"\n",
    "    A class for transforming between matrices and vector representations.\n",
    "\n",
    "    This class provides methods to convert a symmetric matrix into a vector (vectorize)\n",
    "    and to reconstruct the matrix from its vector form (anti_vectorize), focusing on\n",
    "    vertical (column-based) traversal and handling of elements.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes the MatrixVectorizer instance.\n",
    "\n",
    "        The constructor currently does not perform any actions but is included for\n",
    "        potential future extensions where initialization parameters might be required.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def vectorize(matrix, include_diagonal=False):\n",
    "        \"\"\"\n",
    "        Converts a matrix into a vector by vertically extracting elements.\n",
    "\n",
    "        This method traverses the matrix column by column, collecting elements from the\n",
    "        upper triangle, and optionally includes the diagonal elements immediately below\n",
    "        the main diagonal based on the include_diagonal flag.\n",
    "\n",
    "        Parameters:\n",
    "        - matrix (numpy.ndarray): The matrix to be vectorized.\n",
    "        - include_diagonal (bool, optional): Flag to include diagonal elements in the vectorization.\n",
    "          Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "        - numpy.ndarray: The vectorized form of the matrix.\n",
    "        \"\"\"\n",
    "        # Determine the size of the matrix based on its first dimension\n",
    "        matrix_size = matrix.shape[0]\n",
    "\n",
    "        # Initialize an empty list to accumulate vector elements\n",
    "        vector_elements = []\n",
    "\n",
    "        # Iterate over columns and then rows to collect the relevant elements\n",
    "        for col in range(matrix_size):\n",
    "            for row in range(matrix_size):\n",
    "                # Skip diagonal elements if not including them\n",
    "                if row != col:\n",
    "                    if row < col:\n",
    "                        # Collect upper triangle elements\n",
    "                        vector_elements.append(matrix[row, col])\n",
    "                    elif include_diagonal and row == col + 1:\n",
    "                        # Optionally include the diagonal elements immediately below the diagonal\n",
    "                        vector_elements.append(matrix[row, col])\n",
    "\n",
    "        return np.array(vector_elements)\n",
    "\n",
    "    @staticmethod\n",
    "    def anti_vectorize(vector, matrix_size, include_diagonal=False):\n",
    "        \"\"\"\n",
    "        Reconstructs a matrix from its vector form, filling it vertically.\n",
    "\n",
    "        The method fills the matrix by reflecting vector elements into the upper triangle\n",
    "        and optionally including the diagonal elements based on the include_diagonal flag.\n",
    "\n",
    "        Parameters:\n",
    "        - vector (numpy.ndarray): The vector to be transformed into a matrix.\n",
    "        - matrix_size (int): The size of the square matrix to be reconstructed.\n",
    "        - include_diagonal (bool, optional): Flag to include diagonal elements in the reconstruction.\n",
    "          Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "        - numpy.ndarray: The reconstructed square matrix.\n",
    "        \"\"\"\n",
    "        # Initialize a square matrix of zeros with the specified size\n",
    "        matrix = np.zeros((matrix_size, matrix_size))\n",
    "\n",
    "        # Index to keep track of the current position in the vector\n",
    "        vector_idx = 0\n",
    "\n",
    "        # Fill the matrix by iterating over columns and then rows\n",
    "        for col in range(matrix_size):\n",
    "            for row in range(matrix_size):\n",
    "                # Skip diagonal elements if not including them\n",
    "                if row != col:\n",
    "                    if row < col:\n",
    "                        # Reflect vector elements into the upper triangle and its mirror in the lower triangle\n",
    "                        matrix[row, col] = vector[vector_idx]\n",
    "                        matrix[col, row] = vector[vector_idx]\n",
    "                        vector_idx += 1\n",
    "                    elif include_diagonal and row == col + 1:\n",
    "                        # Optionally fill the diagonal elements after completing each column\n",
    "                        matrix[row, col] = vector[vector_idx]\n",
    "                        matrix[col, row] = vector[vector_idx]\n",
    "                        vector_idx += 1\n",
    "\n",
    "        return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n4GFadNvggJy"
   },
   "outputs": [],
   "source": [
    "def load_train_data(path_lr_data,path_hr_data, path_lr_data_test, path_hr_data_test):\n",
    "    # Load the data\n",
    "\n",
    "    lr_data = pd.read_csv(path_lr_data)\n",
    "    hr_data = pd.read_csv(path_hr_data)\n",
    "    lr_data_test = pd.read_csv(path_lr_data_test)\n",
    "    hr_data_test = pd.read_csv(path_hr_data_test)\n",
    "\n",
    "    return lr_data,hr_data, lr_data_test, hr_data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M87YLYjSggJz"
   },
   "outputs": [],
   "source": [
    "def dataloader(lr_data,hr_data,k=1,shuffle=True):\n",
    "    \"\"\"\n",
    "    Create a torch DataLoader\n",
    "    \"\"\"\n",
    "    tuple_matrices = []\n",
    "    matrix_size_lr = 160\n",
    "    matrix_size_hr = 268\n",
    "\n",
    "    for i in range(lr_data.shape[0]):\n",
    "\n",
    "        # Preprocess lr_data\n",
    "        sample_lr = lr_data.iloc[i]\n",
    "        matrix_lr = MatrixVectorizer.anti_vectorize(sample_lr, matrix_size_lr, include_diagonal=False)\n",
    "\n",
    "        # Preprocess hr_data\n",
    "        sample_hr = hr_data.iloc[i]\n",
    "        matrix_hr = MatrixVectorizer.anti_vectorize(sample_hr, matrix_size_hr, include_diagonal=False)\n",
    "\n",
    "        # Append lists\n",
    "        tuple_matrices.append((torch.tensor(matrix_lr), torch.tensor(matrix_hr)))\n",
    "\n",
    "    # Loading data\n",
    "    loader = DataLoader(tuple_matrices, batch_size=k, shuffle=shuffle, pin_memory=True)\n",
    "\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7kDUe1GyzS3P"
   },
   "outputs": [],
   "source": [
    "# Early stop implementation to not overfit model\n",
    "class EarlyStopper:\n",
    "    def __init__(self, patience=4):\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = float('inf')\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "        elif validation_loss > (self.min_validation_loss):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "07XmoXXtggJz"
   },
   "outputs": [],
   "source": [
    "def train(model, data_loader, epochs, learning_rate):\n",
    "    \"\"\"\n",
    "    Create a torch DataLoader\n",
    "    \"\"\"\n",
    "    optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "    model.train()\n",
    "    if option == 0:\n",
    "        early_stopper = EarlyStopper(patience=4)\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        model.train()\n",
    "        for X_lr, X_hr in data_loader:\n",
    "            X_lr, X_hr = X_lr.to(device), X_hr.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_lr)\n",
    "            loss = F.l1_loss(outputs.flatten(), X_hr.flatten())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        if option == 0 and early_stopper.early_stop(total_loss/len(data_loader)):\n",
    "            break\n",
    "        print(f\"Epoch {epoch+1}, Loss Train: {total_loss/len(data_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xe2VSnKDggJz"
   },
   "outputs": [],
   "source": [
    "def predict(model,test_data_loader):\n",
    "    \"\"\"\n",
    "    Make prediction using model across test data\n",
    "    \"\"\"\n",
    "    predicted = torch.empty((len(test_data_loader),268,268),device=device,dtype=torch.float64)\n",
    "    real = torch.empty((len(test_data_loader),268,268),device=device,dtype=torch.float64)\n",
    "    model.eval()\n",
    "\n",
    "    for i,(X_lr, X_hr) in enumerate(test_data_loader):\n",
    "            X_lr, X_hr = X_lr.to(device), X_hr.to(device)\n",
    "            outputs = model(X_lr)[0]\n",
    "            predicted[i] = outputs\n",
    "            real[i] = X_hr[0]\n",
    "\n",
    "    return predicted,real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d1d1E9A4ggJ0"
   },
   "outputs": [],
   "source": [
    "def matrix_list_to_data(matrix_list:np.ndarray)->list:\n",
    "    \"\"\"\n",
    "    convert list of vectorize array to suitable shape\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    for i in range(len(matrix_list)):\n",
    "        for j in range(matrix_list.shape[1]):\n",
    "            data.append([i*matrix_list.shape[1]+j+1]+[matrix_list[i][j]])\n",
    "    return data\n",
    "\n",
    "def to_csv(data:list, fold_num:int=0):\n",
    "    \"\"\"\n",
    "    creates a csv file with the given data\n",
    "    \"\"\"\n",
    "    print(f\"writing {len(data)} out of rows 4007136 to predictions_fold_{fold_num}.csv\")\n",
    "    with open(f\"predictions_fold_{fold_num}.csv\", 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"ID\",\"Predicted\"])\n",
    "        writer.writerows(data)\n",
    "\n",
    "def write(matrix_list,fold):\n",
    "    \"\"\"\n",
    "    Write a csv from list of array\n",
    "    \"\"\"\n",
    "    data = matrix_list_to_data(matrix_list)\n",
    "    to_csv(data,fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HRb4IunJggJ0"
   },
   "outputs": [],
   "source": [
    "def convert_nvec_to_vec(data):\n",
    "    \"\"\"\n",
    "    Convert non vectorize array to vectorize one\n",
    "    \"\"\"\n",
    "    data_vec = np.empty((data.shape[0],35778))\n",
    "\n",
    "    for i in range(data.shape[0]):\n",
    "        data_vec[i] = MatrixVectorizer.vectorize(data[i],include_diagonal=False)\n",
    "    return data_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R2at1uuuggJ0"
   },
   "outputs": [],
   "source": [
    "def analysis_non_vectorize(predicted,real):\n",
    "    \"\"\"\n",
    "    Compute several metrics over non vectorize array\n",
    "    \"\"\"\n",
    "    # Initialize lists to store MAEs for each centrality measure\n",
    "    mae_bc = []\n",
    "    mae_ec = []\n",
    "    mae_pc = []\n",
    "\n",
    "    # Iterate over each test sample\n",
    "    for i in range(predicted.shape[0]):\n",
    "        # Convert adjacency matrices to NetworkX graphs\n",
    "        pred_graph = nx.from_numpy_array(predicted[i], edge_attr=\"weight\")\n",
    "        gt_graph = nx.from_numpy_array(real[i], edge_attr=\"weight\")\n",
    "\n",
    "        # Compute centrality measures\n",
    "        pred_bc = nx.betweenness_centrality(pred_graph, weight=\"weight\")\n",
    "        pred_ec = nx.eigenvector_centrality(pred_graph, weight=\"weight\")\n",
    "        pred_pc = nx.pagerank(pred_graph, weight=\"weight\")\n",
    "\n",
    "        gt_bc = nx.betweenness_centrality(gt_graph, weight=\"weight\")\n",
    "        gt_ec = nx.eigenvector_centrality(gt_graph, weight=\"weight\")\n",
    "        gt_pc = nx.pagerank(gt_graph, weight=\"weight\")\n",
    "\n",
    "        # Convert centrality dictionaries to lists\n",
    "        pred_bc_values = list(pred_bc.values())\n",
    "        pred_ec_values = list(pred_ec.values())\n",
    "        pred_pc_values = list(pred_pc.values())\n",
    "\n",
    "        gt_bc_values = list(gt_bc.values())\n",
    "        gt_ec_values = list(gt_ec.values())\n",
    "        gt_pc_values = list(gt_pc.values())\n",
    "\n",
    "        # Compute MAEs\n",
    "        mae_bc.append(mean_absolute_error(pred_bc_values, gt_bc_values))\n",
    "        mae_ec.append(mean_absolute_error(pred_ec_values, gt_ec_values))\n",
    "        mae_pc.append(mean_absolute_error(pred_pc_values, gt_pc_values))\n",
    "\n",
    "    # Compute average MAEs\n",
    "    avg_mae_bc = sum(mae_bc) / len(mae_bc)\n",
    "    avg_mae_ec = sum(mae_ec) / len(mae_ec)\n",
    "    avg_mae_pc = sum(mae_pc) / len(mae_pc)\n",
    "\n",
    "    return avg_mae_bc, avg_mae_ec, avg_mae_pc\n",
    "\n",
    "def analysis_vectorize(predicted,real):\n",
    "    \"\"\"\n",
    "    Compute several metrics over vectorize array\n",
    "    \"\"\"\n",
    "    # Compute metrics\n",
    "    mae = mean_absolute_error(predicted.reshape(-1), real.reshape(-1))\n",
    "    pcc = pearsonr(predicted.reshape(-1), real.reshape(-1))[0]\n",
    "    js_dis = jensenshannon(predicted.reshape(-1), real.reshape(-1))\n",
    "\n",
    "    return mae, pcc, js_dis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m7f9jxjyggJ1"
   },
   "outputs": [],
   "source": [
    "def plot_hist(l_mae, l_pcc, l_js_dis, l_avg_mae_bc, l_avg_mae_ec, l_avg_mae_pc):\n",
    "    \"\"\"\n",
    "    Plot histogram of several metrics over several fold of data\n",
    "    \"\"\"\n",
    "    metrics = [\"MAE\",\"PCC\",\"JSD\",\"MAE(PC)\",\"MAE(EC)\",\"MAE(BC)\"]\n",
    "    c=['pink','blue','green','yellow','grey',\"orange\"]\n",
    "    L = [l_mae, l_pcc, l_js_dis, l_avg_mae_pc, l_avg_mae_ec, l_avg_mae_bc]\n",
    "    for i in range(len(l_mae)):\n",
    "        plt.title(\"Fold \" +str(i+1))\n",
    "        plt.bar(metrics,[L[j][i] for j in range(len(L))],color=c)\n",
    "        plt.show()\n",
    "\n",
    "    array = np.array(L)\n",
    "    average = np.mean(array,1)\n",
    "    std = np.std(array,1)\n",
    "\n",
    "    plt.title(\"Average across fold \")\n",
    "    plt.bar(metrics,average,color=c)\n",
    "    plt.errorbar(metrics, average, yerr=std, fmt='none', ecolor='red', capsize=4, capthick=2)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FSGe9vQkggJ1"
   },
   "outputs": [],
   "source": [
    "def normalization(a):\n",
    "    \"\"\"\n",
    "    Adjacency normalization\n",
    "    \"\"\"\n",
    "    s = ((a**2).sum(-1))**0.5+0.000000001\n",
    "    ds = torch.diag(s[0]**-1)\n",
    "    return a@ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1y_Vazi6jrHt"
   },
   "outputs": [],
   "source": [
    "def calculate_centralities(adj_matrix):\n",
    "    if adj_matrix.shape[0] != adj_matrix.shape[1]:\n",
    "        raise ValueError(f\"Adjacency matrix is not square: shape={adj_matrix.shape}\")\n",
    "    print(f\"Processing adjacency matrix of shape: {adj_matrix.shape}\")\n",
    "\n",
    "    G = nx.from_numpy_array(adj_matrix)\n",
    "    partition = community_louvain.best_partition(G)\n",
    "\n",
    "    # Calculate the participation coefficient with the partition\n",
    "    pc_dict = participation_coefficient(G, partition)\n",
    "\n",
    "    # Calculate averages of centrality measures\n",
    "    pr = nx.pagerank(G, alpha=0.9)\n",
    "    ec = nx.eigenvector_centrality_numpy(G, max_iter=100)\n",
    "    bc = nx.betweenness_centrality(G, normalized=True, endpoints=False)\n",
    "    ns = np.array(list(nx.degree_centrality(G).values())) * (len(G.nodes()) - 1)\n",
    "    acc = nx.average_clustering(G, weight=None)\n",
    "\n",
    "    # Average participation coefficient\n",
    "    pc_avg = np.mean(list(pc_dict.values()))\n",
    "\n",
    "    return {\n",
    "        'pr': np.mean(list(pr.values())),\n",
    "        'ec': np.mean(list(ec.values())),\n",
    "        'bc': np.mean(list(bc.values())),\n",
    "        'ns': ns,\n",
    "        'pc': pc_avg,\n",
    "        'acc': acc\n",
    "    }\n",
    "\n",
    "def participation_coefficient(G, partition):\n",
    "    # Initialize dictionary for participation coefficients\n",
    "    pc_dict = {}\n",
    "\n",
    "    # Calculate participation coefficient for each node\n",
    "    for node in G.nodes():\n",
    "        node_degree = G.degree(node)\n",
    "        if node_degree == 0:\n",
    "            pc_dict[node] = 0.0\n",
    "        else:\n",
    "            # Count within-module connections\n",
    "            within_module_degree = sum(1 for neighbor in G[node] if partition[neighbor] == partition[node])\n",
    "            # Calculate participation coefficient\n",
    "            pc_dict[node] = 1 - (within_module_degree / node_degree) ** 2\n",
    "\n",
    "    return pc_dict\n",
    "\n",
    "def evaluate_all(true_hr_matrices, predicted_hr_matrices, output_path=path_eval_matrics):\n",
    "    print(true_hr_matrices.shape)\n",
    "    print(predicted_hr_matrices.shape)\n",
    "\n",
    "    num_subjects = true_hr_matrices.shape[0]\n",
    "    results = []\n",
    "\n",
    "    for i in range(num_subjects):\n",
    "        true_matrix = true_hr_matrices[i, :, :]\n",
    "        pred_matrix = predicted_hr_matrices[i, :, :]\n",
    "\n",
    "        print(f\"Evaluating subject {i+1} with matrix shapes: true={true_matrix.shape}, pred={pred_matrix.shape}\")\n",
    "\n",
    "        if true_matrix.shape != pred_matrix.shape or true_matrix.shape[0] != true_matrix.shape[1]:\n",
    "            print(f\"Error: Matrix shape mismatch or not square for subject {i+1}: true={true_matrix.shape}, pred={pred_matrix.shape}\")\n",
    "            continue\n",
    "\n",
    "        metrics = {\n",
    "            'ID': i + 1,\n",
    "            'MAE': mean_absolute_error(true_matrix.flatten(), pred_matrix.flatten()),\n",
    "            'PCC': pearsonr(true_matrix.flatten(), pred_matrix.flatten())[0],\n",
    "            'JSD': jensenshannon(true_matrix.flatten(), pred_matrix.flatten()),\n",
    "        }\n",
    "\n",
    "        true_metrics = calculate_centralities(true_matrix)\n",
    "        pred_metrics = calculate_centralities(pred_matrix)\n",
    "\n",
    "        for key in ['NS', 'PR', 'EC', 'BC', 'PC', 'ACC']:\n",
    "            metrics[f'MAE in {key}'] = mean_absolute_error([true_metrics[key.lower()]], [pred_metrics[key.lower()]])\n",
    "\n",
    "        results.append(metrics)\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    if not df.empty:\n",
    "        # Check if the file exists to decide whether to write headers\n",
    "        file_exists = os.path.isfile(output_path)\n",
    "\n",
    "        df.to_csv(output_path, mode='a', header=not file_exists, index=False)\n",
    "        print(f\"Results appended to {output_path}.\")\n",
    "    else:\n",
    "        print(\"No data to save.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LE9SoDDrggJ2"
   },
   "source": [
    "Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ozNErDWAggJ2"
   },
   "outputs": [],
   "source": [
    "def cross_validation(model_ini, train_function, predict_function, path_lr_data, path_hr_data, path_lr_data_test, path_hr_data_test, model_parameter, seed):\n",
    "    lr_data,hr_data, lr_data_test, hr_data_test = load_train_data(path_lr_data,path_hr_data, path_lr_data_test, path_hr_data_test)\n",
    "\n",
    "    # Store the fold results\n",
    "    fold_results = []\n",
    "\n",
    "    # save metrics\n",
    "    l_mae = []\n",
    "    l_pcc = []\n",
    "    l_js_dis = []\n",
    "    l_avg_mae_bc = []\n",
    "    l_avg_mae_ec = []\n",
    "    l_avg_mae_pc = []\n",
    "\n",
    "    #retrieve model parameter\n",
    "    batch_size = model_parameter[\"batch_size\"]\n",
    "    architecture = model_parameter[\"architecture\"]\n",
    "    dropout = model_parameter[\"dropout\"]\n",
    "    input_dim = model_parameter[\"input_dim\"]\n",
    "    output_dim = model_parameter[\"output_dim\"]\n",
    "    epoch = model_parameter[\"epoch\"]\n",
    "    learning_rate = model_parameter[\"learning_rate\"]\n",
    "\n",
    "\n",
    "        # train and test data\n",
    "    train_data_lr = lr_data\n",
    "    train_data_hr = hr_data\n",
    "    test_data_lr = lr_data_test\n",
    "    test_data_hr = hr_data_test\n",
    "\n",
    "        # train/test data loader\n",
    "    train_loader = dataloader(train_data_lr,train_data_hr,k=batch_size)\n",
    "    test_loader = dataloader(test_data_lr,test_data_hr,shuffle=False)\n",
    "\n",
    "        # new model\n",
    "    model = model_ini(input_dim,output_dim,dropout,architecture)\n",
    "    model.to(device)\n",
    "\n",
    "        # train\n",
    "    train_function(model, train_loader, epoch, learning_rate)\n",
    "\n",
    "        # predict\n",
    "    predicted, real = predict_function(model,test_loader)\n",
    "\n",
    "        # numpy\n",
    "    np_predicted = predicted.detach().cpu().numpy()\n",
    "    np_real = real.cpu().numpy()\n",
    "\n",
    "        # Evaluate the model on the test set and log the results\n",
    "    metrics = evaluate_all(\n",
    "        np_real, np_predicted\n",
    "    )\n",
    "    fold_results.append(metrics)\n",
    "\n",
    "        # analysis non vectorize - to generate plots as cited in report\n",
    "    avg_mae_bc, avg_mae_ec, avg_mae_pc = analysis_non_vectorize(np_predicted,np_real)\n",
    "\n",
    "        # vectorize - to generate plots as cited in report\n",
    "    np_predicted = convert_nvec_to_vec(np_predicted)\n",
    "    np_real = convert_nvec_to_vec(np_real)\n",
    "\n",
    "        # write file\n",
    "    write(np_predicted, 1)\n",
    "\n",
    "        # analysis vectorize\n",
    "    mae, pcc, js_dis = analysis_vectorize(np_predicted,np_real)\n",
    "\n",
    "        # save\n",
    "    l_mae += [mae]\n",
    "    l_pcc += [pcc]\n",
    "    l_js_dis += [js_dis]\n",
    "    l_avg_mae_bc += [avg_mae_bc]\n",
    "    l_avg_mae_ec += [avg_mae_ec]\n",
    "    l_avg_mae_pc += [avg_mae_pc]\n",
    "\n",
    "    return l_mae, l_pcc, l_js_dis, l_avg_mae_bc, l_avg_mae_ec, l_avg_mae_pc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f7BlWIJAggJ3"
   },
   "source": [
    "Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R20yiKt2ggJ3"
   },
   "outputs": [],
   "source": [
    "class ETN(nn.Module):\n",
    "    \"\"\"\n",
    "    ETn : Edge to node\n",
    "    Create a form of node from an adjacency matrix\n",
    "    \"\"\"\n",
    "    def __init__(self, i_s, o_s, activation=None):\n",
    "        super(ETN, self).__init__()\n",
    "        self.weight = nn.Parameter(torch.DoubleTensor(i_s,o_s), requires_grad=True)\n",
    "        self.bias = nn.Parameter(torch.zeros(i_s,o_s), requires_grad=True)\n",
    "        self.activation = activation\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / np.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self,x):\n",
    "        # apply layer\n",
    "        f = x@(self.weight) + self.bias\n",
    "        # activation\n",
    "        h = self.activation(f) if self.activation else f\n",
    "        return h\n",
    "\n",
    "\n",
    "class GAT(nn.Module):\n",
    "    \"\"\"\n",
    "    A basic implementation of the GAT layer.\n",
    "\n",
    "    This layer applies an attention mechanism in the graph convolution process,\n",
    "    allowing the model to focus on different parts of the neighborhood\n",
    "    of each node.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features, activation=None):\n",
    "        super(GAT, self).__init__()\n",
    "        # Initialize the weights, bias, and attention parameters as\n",
    "        # trainable parameters\n",
    "        self.weight = nn.Parameter(torch.DoubleTensor(in_features, out_features), requires_grad=True)\n",
    "        self.bias = nn.Parameter(torch.zeros(out_features), requires_grad=True)\n",
    "        self.phi = nn.Parameter(torch.DoubleTensor(2 * out_features, 1), requires_grad=True)\n",
    "        self.activation = activation\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / np.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "\n",
    "        stdv = 1. / np.sqrt(self.phi.size(1))\n",
    "        self.phi.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input, a):\n",
    "        \"\"\"\n",
    "        Forward pass of the GAT layer.\n",
    "\n",
    "        Parameters:\n",
    "        input (Tensor): The input features of the nodes.\n",
    "        adj (Tensor): The adjacency matrix of the graph.\n",
    "\n",
    "        Returns:\n",
    "        Tensor: The output features of the nodes after applying the GAT layer.\n",
    "        \"\"\"\n",
    "        input = input.to(device)\n",
    "        a = a.to(device)\n",
    "\n",
    "        #normalize adjacency\n",
    "        adj = normalization(a).to(device)\n",
    "\n",
    "        # linear transformation and bias\n",
    "        ltb = (self.bias.reshape(1,-1)+ input@self.weight)\n",
    "\n",
    "        # Similarity\n",
    "        S = ((ltb@self.phi[0:self.bias.size()[-1]]).reshape((-1,ltb.size()[-2],1))\n",
    "             +(ltb@self.phi[self.bias.size()[-1]:]).reshape((-1,1,ltb.size()[-2])))\n",
    "        S = F.relu(S)\n",
    "\n",
    "        # mask\n",
    "        mask = ((adj + torch.eye(adj.size()[-1], device=device)).bool()).to(device)\n",
    "        S_masked = torch.where(mask, S, torch.tensor(-1e10, device=device))\n",
    "\n",
    "        h = torch.nn.functional.softmax(S_masked,-1)@ltb\n",
    "\n",
    "        return self.activation(h) if self.activation else h\n",
    "\n",
    "class Scaler(nn.Module):\n",
    "    def __init__(self, d):\n",
    "        super(Scaler, self).__init__()\n",
    "        self.weight = nn.Parameter(torch.DoubleTensor(d,d), requires_grad=True)\n",
    "        self.bias = nn.Parameter(torch.zeros(d,d), requires_grad=True)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv1 = 1. / np.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv1, stdv1)\n",
    "\n",
    "    def forward(self,x):\n",
    "        # rescale the adjacency to make it similar to the target\n",
    "        h= x*(self.weight) + self.bias\n",
    "        return h\n",
    "\n",
    "class MD(nn.Module):\n",
    "    def __init__(self, i_s, o_s, activation1=None, activation2=None, activation=None):\n",
    "        super(MD, self).__init__()\n",
    "        self.gat = GAT(o_s,o_s,activation=activation1)\n",
    "        self.etn = ETN(i_s,o_s,activation=activation2)\n",
    "        self.scaler = Scaler(o_s)\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self,a):\n",
    "        # \"node\" generation\n",
    "        x = self.etn(a)\n",
    "\n",
    "        # use of attention\n",
    "        xx = self.gat(x,a)\n",
    "\n",
    "        # compute self product to obtain good dimension and symmetry\n",
    "        h = xx.resize(xx.size(0),xx.size(2),xx.size(1))@xx\n",
    "\n",
    "        # rescale\n",
    "        h = self.scaler(h)\n",
    "\n",
    "        # activation\n",
    "        h = self.activation(h) if self.activation else h\n",
    "        h = (1+h)/2\n",
    "        return h\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P3tje1AsggJ4"
   },
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nd46lcCkggJ4"
   },
   "outputs": [],
   "source": [
    "class ADAPT(nn.Module):\n",
    "    \"\"\"\n",
    "    Attentive Deep grAph super resoluTion\n",
    "    \"\"\"\n",
    "    def __init__(self,i,o,p,l_s):\n",
    "        super(ADAPT, self).__init__()\n",
    "        # dropout\n",
    "        self.drop = nn.Dropout(p=p)\n",
    "\n",
    "        # add input and output\n",
    "        l_s = [i]+l_s+[o]\n",
    "\n",
    "        # several layers of MD\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for j in range(len(l_s)-1):\n",
    "            self.layers.append(MD(l_s[j], l_s[j+1], activation1=F.tanh, activation2=F.tanh, activation=F.tanh))\n",
    "\n",
    "    def forward(self,a):\n",
    "        # apply every layer\n",
    "        for i in self.layers:\n",
    "            a = self.drop(a)\n",
    "            a = i.forward(a)\n",
    "        return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CCx6eos8ggJ4"
   },
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MX_SHCVOggJ4",
    "outputId": "c811495f-f214-4c53-dbe9-1aca6bda8b85"
   },
   "outputs": [],
   "source": [
    "l_mae, l_pcc, l_js_dis, l_avg_mae_bc, l_avg_mae_ec, l_avg_mae_pc = cross_validation(ADAPT, train, predict, path_lr_data, path_hr_data, path_lr_data_test, path_hr_data_test, model_parameter, random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rWq-QIXLggJ5"
   },
   "outputs": [],
   "source": [
    "# histogram\n",
    "plot_hist(l_mae, l_pcc, l_js_dis, l_avg_mae_bc, l_avg_mae_ec, l_avg_mae_pc)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
