{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import KFold, cross_validate, LeaveOneOut\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from time import sleep\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.spatial.distance import jensenshannon # for maybe JSD\n",
    "from MatrixVectorizer import MatrixVectorizer\n",
    "import networkx as nx # for maybe centrality measures\n",
    "import random\n",
    "from torch.nn import Linear, ReLU, Sequential, Flatten, Unflatten, ModuleList, Sigmoid, LeakyReLU\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import time\n",
    "import psutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr, structural_similarity as ssim\n",
    "import community.community_louvain as community_louvain\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_centralities(adj_matrix):\n",
    "    if adj_matrix.shape[0] != adj_matrix.shape[1]:\n",
    "        raise ValueError(f\"Adjacency matrix is not square: shape={adj_matrix.shape}\")\n",
    "    print(f\"Processing adjacency matrix of shape: {adj_matrix.shape}\")\n",
    "    adj_matrix = np.array(adj_matrix)\n",
    "    G = nx.from_numpy_array(adj_matrix)\n",
    "    partition = community_louvain.best_partition(G)\n",
    "\n",
    "    # Calculate the participation coefficient with the partition\n",
    "    pc_dict = participation_coefficient(G, partition)\n",
    "\n",
    "    # Calculate averages of centrality measures\n",
    "    pr = nx.pagerank(G, alpha=0.9)\n",
    "    ec = nx.eigenvector_centrality_numpy(G, max_iter=100)\n",
    "    bc = nx.betweenness_centrality(G, normalized=True, endpoints=False)\n",
    "    ns = np.array(list(nx.degree_centrality(G).values())) * (len(G.nodes()) - 1)\n",
    "    acc = nx.average_clustering(G, weight=None)\n",
    "\n",
    "    # Average participation coefficient\n",
    "    pc_avg = np.mean(list(pc_dict.values()))\n",
    "\n",
    "    return {\n",
    "        'pr': np.mean(list(pr.values())),\n",
    "        'ec': np.mean(list(ec.values())),\n",
    "        'bc': np.mean(list(bc.values())),\n",
    "        'ns': ns,\n",
    "        'pc': pc_avg,\n",
    "        'acc': acc\n",
    "    }\n",
    "\n",
    "def participation_coefficient(G, partition):\n",
    "    # Initialize dictionary for participation coefficients\n",
    "    pc_dict = {}\n",
    "\n",
    "    # Calculate participation coefficient for each node\n",
    "    for node in G.nodes():\n",
    "        node_degree = G.degree(node)\n",
    "        if node_degree == 0:\n",
    "            pc_dict[node] = 0.0\n",
    "        else:\n",
    "            # Count within-module connections\n",
    "            within_module_degree = sum(1 for neighbor in G[node] if partition[neighbor] == partition[node])\n",
    "            # Calculate participation coefficient\n",
    "            pc_dict[node] = 1 - (within_module_degree / node_degree) ** 2\n",
    "\n",
    "    return pc_dict\n",
    "\n",
    "\n",
    "def evaluate_all(true_hr_matrices, predicted_hr_matrices, output_path='ID-randomCV.csv'):\n",
    "    print(true_hr_matrices.shape)\n",
    "\n",
    "    # print(predicted_hr_matrices.shape)\n",
    "    \n",
    "    num_subjects = true_hr_matrices.shape[0]\n",
    "    print(\"numsubjects matrix\", num_subjects)\n",
    "    print(\"pred\", len(predicted_hr_matrices))\n",
    "    results = []\n",
    "\n",
    "    for i in range(num_subjects):\n",
    "        true_matrix = true_hr_matrices[i]\n",
    "        pred_matrix = predicted_hr_matrices[i]\n",
    "        print(\"true matrix\", true_matrix.shape)\n",
    "        print(\"pred\", pred_matrix.shape)\n",
    "\n",
    "        print(f\"Evaluating subject {i+1} with matrix shapes: true={true_matrix.shape}, pred={pred_matrix.shape}\")\n",
    "\n",
    "        if true_matrix.shape != pred_matrix.shape or true_matrix.shape[0] != true_matrix.shape[1]:\n",
    "            print(f\"Error: Matrix shape mismatch or not square for subject {i+1}: true={true_matrix.shape}, pred={pred_matrix.shape}\")\n",
    "            continue\n",
    "\n",
    "        metrics = {\n",
    "            'ID': i + 1,\n",
    "            'MAE': mean_absolute_error(true_matrix.flatten(), pred_matrix.flatten()),\n",
    "            'PCC': pearsonr(true_matrix.flatten(), pred_matrix.flatten())[0],\n",
    "            'JSD': jensenshannon(true_matrix.flatten(), pred_matrix.flatten()),\n",
    "        }\n",
    "\n",
    "        true_metrics = calculate_centralities(true_matrix)\n",
    "        pred_metrics = calculate_centralities(pred_matrix)\n",
    "\n",
    "        for key in ['NS', 'PR', 'EC', 'BC', 'PC', 'ACC']:\n",
    "            metrics[f'MAE in {key}'] = mean_absolute_error([true_metrics[key.lower()]], [pred_metrics[key.lower()]])\n",
    "\n",
    "        results.append(metrics)\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    if not df.empty:\n",
    "        # Check if the file exists to decide whether to write headers\n",
    "        file_exists = os.path.isfile(output_path)\n",
    "\n",
    "        df.to_csv(output_path, mode='a', header=not file_exists, index=False)\n",
    "        print(f\"Results appended to {output_path}.\")\n",
    "    else:\n",
    "        print(\"No data to save.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a fixed random seed for reproducibility across multiple libraries\n",
    "random_seed = 42\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "# Check for CUDA (GPU support) and set device accordingly\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"CUDA is available. Using GPU.\")\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed)  # For multi-GPU setups\n",
    "    # Additional settings for ensuring reproducibility on CUDA\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDA not available. Using CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anti-Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorization\n",
    "def vectorize_symmetric_matrix(matrix):\n",
    "    \"\"\"Vectorize a symmetric matrix\"\"\"\n",
    "    vectorizer = MatrixVectorizer()\n",
    "    return vectorizer.vectorize(matrix)\n",
    "\n",
    "def devectorize_symmetric_matrix(vector, size):\n",
    "    \"\"\"Devectorize a symmetric matrix\"\"\"\n",
    "    vectorizer = MatrixVectorizer()\n",
    "    return vectorizer.anti_vectorize(vector, size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "lr_train1 = pd.read_csv('../Cluster-CV/Fold1/lr_clusterA.csv').values\n",
    "hr_train1 = pd.read_csv('../Cluster-CV/Fold1/hr_clusterA.csv').values\n",
    "\n",
    "lr_train2 = pd.read_csv('../Cluster-CV/Fold2/lr_clusterB.csv').values\n",
    "hr_train2 = pd.read_csv('../Cluster-CV/Fold2/hr_clusterB.csv').values\n",
    "\n",
    "lr_train3 = pd.read_csv('../Cluster-CV/Fold3/lr_clusterC.csv').values\n",
    "hr_train3 = pd.read_csv('../Cluster-CV/Fold3/hr_clusterC.csv').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_train_anti_vectorized1 = torch.tensor([devectorize_symmetric_matrix(x, 160) for x in lr_train1])\n",
    "hr_train_anti_vectorized1 = torch.tensor([devectorize_symmetric_matrix(x, 268) for x in hr_train1])\n",
    "\n",
    "lr_train_anti_vectorized2 = torch.tensor([devectorize_symmetric_matrix(x, 160) for x in lr_train2])\n",
    "hr_train_anti_vectorized2 = torch.tensor([devectorize_symmetric_matrix(x, 268) for x in hr_train2])\n",
    "\n",
    "lr_train_anti_vectorized3 = torch.tensor([devectorize_symmetric_matrix(x, 160) for x in lr_train3])\n",
    "hr_train_anti_vectorized3 = torch.tensor([devectorize_symmetric_matrix(x, 268) for x in hr_train3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(lr_train_anti_vectorized1))\n",
    "print(len(hr_train_anti_vectorized1))\n",
    "print(lr_train_anti_vectorized1.shape)\n",
    "print(len(lr_train_anti_vectorized2))\n",
    "print(len(hr_train_anti_vectorized2))\n",
    "print(lr_train_anti_vectorized2.shape)\n",
    "print(len(lr_train_anti_vectorized3))\n",
    "print(len(hr_train_anti_vectorized3))\n",
    "print(lr_train_anti_vectorized3.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_graph_to_expected_size(matrix, expected_size=160):\n",
    "    # Ensure the matrix is on CPU for manipulation\n",
    "    matrix_np = matrix.detach().cpu().numpy() if isinstance(matrix, torch.Tensor) else matrix\n",
    "    current_size = matrix_np.shape[0]\n",
    "    \n",
    "    if current_size < expected_size:\n",
    "        padding_size = expected_size - current_size\n",
    "        padded_matrix_np = np.pad(matrix_np, ((0, padding_size), (0, padding_size)), mode='constant', constant_values=0)\n",
    "        padded_matrix = torch.tensor(padded_matrix_np, dtype=torch.float).to(device)\n",
    "    else:\n",
    "        padded_matrix = torch.tensor(matrix_np, dtype=torch.float).to(device) if not isinstance(matrix, torch.Tensor) else matrix\n",
    "    \n",
    "    return padded_matrix\n",
    "\n",
    "# Dataset\n",
    "class BrainConnectivityDataset(Dataset):\n",
    "    def __init__(self, features, labels, augment=False, augmentation_methods=None):\n",
    "        self.features = features.clone().detach().float().to(device)\n",
    "        if labels is not None:\n",
    "            self.labels = labels.clone().detach().float().to(device)\n",
    "        else: \n",
    "            self.labels = None\n",
    "        self.augment = augment\n",
    "        self.augmentation_methods = augmentation_methods\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        feature = self.features[idx]\n",
    "        if self.labels is not None:\n",
    "            label = self.labels[idx] \n",
    "        else:\n",
    "            label = None\n",
    "\n",
    "        # Apply data augmentation if enabled\n",
    "        if self.augment and self.augmentation_methods is not None:\n",
    "            for augment in self.augmentation_methods:\n",
    "                feature = augment(feature)\n",
    "\n",
    "        feature = pad_graph_to_expected_size(feature, expected_size=160)\n",
    "\n",
    "        return feature, label\n",
    "\n",
    "\n",
    "# Data Augmentation Methods\n",
    "def node_dropping(matrix, drop_rate=0.2):\n",
    "    \"\"\"\n",
    "    Randomly drops nodes (and their corresponding connections) from the graph.\n",
    "    drop_rate: Proportion of nodes to drop.\n",
    "    \"\"\"\n",
    "    # Convert to numpy array for manipulation if it's a torch tensor\n",
    "    if isinstance(matrix, torch.Tensor):\n",
    "        matrix = matrix.cpu().numpy()\n",
    "    \n",
    "    num_nodes = matrix.shape[0]\n",
    "    num_drop = int(num_nodes * drop_rate)\n",
    "    drop_indices = np.random.choice(num_nodes, num_drop, replace=False)\n",
    "    \n",
    "    # Create a mask to keep the nodes not in drop_indices\n",
    "    keep_mask = np.ones(num_nodes, dtype=bool)\n",
    "    keep_mask[drop_indices] = False\n",
    "\n",
    "    # Filter the matrix to keep only the remaining nodes and connections\n",
    "    matrix_filtered = matrix[np.ix_(keep_mask, keep_mask)]\n",
    "\n",
    "    return torch.tensor(matrix_filtered, dtype=torch.float).to(device)\n",
    "\n",
    "def edge_perturbation(matrix, perturb_rate=0.05):\n",
    "    \"\"\"\n",
    "    Randomly perturbs edges in the graph by adding or removing them.\n",
    "    perturb_rate: Proportion of edges to perturb.\n",
    "    \"\"\"\n",
    "    if isinstance(matrix, torch.Tensor):\n",
    "        matrix = matrix.cpu().numpy()\n",
    "\n",
    "    num_edges = int(np.triu(matrix, 1).sum())\n",
    "    num_perturb = int(num_edges * perturb_rate)\n",
    "\n",
    "    # Get upper triangular indices excluding the diagonal\n",
    "    triu_indices = np.triu_indices(matrix.shape[0], k=1)\n",
    "    \n",
    "    for _ in range(num_perturb):\n",
    "        edge_index = np.random.randint(len(triu_indices[0]))\n",
    "        i, j = triu_indices[0][edge_index], triu_indices[1][edge_index]\n",
    "        # Flip the edge state\n",
    "        matrix[i, j] = matrix[j, i] = 1 - matrix[i, j]\n",
    "        \n",
    "    if isinstance(matrix, torch.Tensor):\n",
    "        matrix = torch.tensor(matrix_np, dtype=torch.float).to(device)\n",
    "\n",
    "    return matrix\n",
    "\n",
    "def subgraph_sampling(matrix, sample_size=0.8):\n",
    "    \"\"\"\n",
    "    Samples a subgraph from the original graph by randomly selecting a subset of nodes.\n",
    "    sample_size: Proportion of nodes to include in the subgraph.\n",
    "    \"\"\"\n",
    "    if isinstance(matrix, torch.Tensor):\n",
    "        matrix_np = matrix.cpu().numpy()\n",
    "    else:\n",
    "        matrix_np = matrix\n",
    "\n",
    "    num_nodes = matrix.shape[0]\n",
    "    sample_num = int(num_nodes * sample_size)\n",
    "    sampled_indices = np.random.choice(num_nodes, sample_num, replace=False)\n",
    "    \n",
    "    matrix_sampled = matrix_np[np.ix_(sampled_indices, sampled_indices)]\n",
    "\n",
    "    if isinstance(matrix, torch.Tensor):\n",
    "        return torch.tensor(matrix_sampled, dtype=torch.float).to(device)\n",
    "    else:\n",
    "        return matrix_sampled\n",
    "\n",
    "def feature_noising(matrix, noise_level=0.01):\n",
    "    \"\"\"\n",
    "    Adds Gaussian noise to the features (edge weights) of the graph.\n",
    "    noise_level: Standard deviation of the Gaussian noise to add.\n",
    "    \"\"\"\n",
    "    if isinstance(matrix, torch.Tensor):\n",
    "        noise = torch.normal(0, noise_level, size=matrix.size(), device=matrix.device)\n",
    "        matrix_noised = matrix + noise\n",
    "        matrix_noised.fill_diagonal_(0) \n",
    "        matrix_noised = torch.clamp(matrix_noised, 0, 1)  # Clamp values to ensure they're within [0, 1]\n",
    "        return matrix_noised\n",
    "    else:\n",
    "        noise = np.random.normal(0, noise_level, matrix.shape)\n",
    "        matrix_noised = matrix + noise\n",
    "        np.fill_diagonal(matrix_noised, 0)\n",
    "        matrix_noised = np.clip(matrix_noised, 0, 1)  # Clip values for numpy arrays\n",
    "        return torch.tensor(matrix_noised, dtype=torch.float).to(device)\n",
    "\n",
    "augmentation_methods = [\n",
    "    lambda x: node_dropping(x, drop_rate=0.2),\n",
    "    lambda x: edge_perturbation(x, perturb_rate=0.05),\n",
    "    lambda x: subgraph_sampling(x, sample_size=0.8),\n",
    "    lambda x: feature_noising(x, noise_level=0.01)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_centrality_metrics(pred_matrices, gt_matrices):\n",
    "    mae_bc = []\n",
    "    mae_ec = []\n",
    "    mae_pc = []\n",
    "    pred_1d_list = []\n",
    "    gt_1d_list = []\n",
    "\n",
    "    # Iterate over each test sample\n",
    "    for i in range(len(pred_matrices)):\n",
    "        # Convert adjacency matrices to NetworkX graphs\n",
    "        pred_graph = nx.from_numpy_array(pred_matrices[i], edge_attr=\"weight\")\n",
    "        gt_graph = nx.from_numpy_array(gt_matrices[i], edge_attr=\"weight\")\n",
    "\n",
    "        # Compute centrality measures\n",
    "        pred_bc = nx.betweenness_centrality(pred_graph, weight=\"weight\")\n",
    "        pred_ec = nx.eigenvector_centrality(pred_graph, weight=\"weight\")\n",
    "        pred_pc = nx.pagerank(pred_graph, weight=\"weight\")\n",
    "\n",
    "        gt_bc = nx.betweenness_centrality(gt_graph, weight=\"weight\")\n",
    "        gt_ec = nx.eigenvector_centrality(gt_graph, weight=\"weight\")\n",
    "        gt_pc = nx.pagerank(gt_graph, weight=\"weight\")\n",
    "\n",
    "        # Convert centrality dictionaries to lists\n",
    "        pred_bc_values = list(pred_bc.values())\n",
    "        pred_ec_values = list(pred_ec.values())\n",
    "        pred_pc_values = list(pred_pc.values())\n",
    "\n",
    "        gt_bc_values = list(gt_bc.values())\n",
    "        gt_ec_values = list(gt_ec.values())\n",
    "        gt_pc_values = list(gt_pc.values())\n",
    "\n",
    "        # Compute MAEs\n",
    "        mae_bc.append(mean_absolute_error(pred_bc_values, gt_bc_values))\n",
    "        mae_ec.append(mean_absolute_error(pred_ec_values, gt_ec_values))\n",
    "        mae_pc.append(mean_absolute_error(pred_pc_values, gt_pc_values))\n",
    "\n",
    "        # Vectorize matrices\n",
    "        pred_1d_list.append(MatrixVectorizer.vectorize(pred_matrices[i]))\n",
    "        gt_1d_list.append(MatrixVectorizer.vectorize(gt_matrices[i]))\n",
    "\n",
    "    # Compute average MAEs\n",
    "    avg_mae_bc = sum(mae_bc) / len(mae_bc)\n",
    "    avg_mae_ec = sum(mae_ec) / len(mae_ec)\n",
    "    avg_mae_pc = sum(mae_pc) / len(mae_pc)\n",
    "\n",
    "    # Concatenate flattened matrices\n",
    "    pred_1d = np.concatenate(pred_1d_list)\n",
    "    gt_1d = np.concatenate(gt_1d_list)\n",
    "\n",
    "    # Compute metrics\n",
    "    mae = mean_absolute_error(pred_1d, gt_1d)\n",
    "    pcc = pearsonr(pred_1d, gt_1d)[0]\n",
    "    js_dis = jensenshannon(pred_1d, gt_1d)\n",
    "\n",
    "    return mae, pcc, js_dis, avg_mae_bc, avg_mae_ec, avg_mae_pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3-fold cross-validation\n",
    "foldPred = None\n",
    "def k_fold_cross_validation(get_model, loss_fn, evaluate, optimizer_fn, num_epochs, lr, step_size=10, gamma=0.1, training_only=True):\n",
    "    losses = []\n",
    "    all_metrics = {\n",
    "        'mae': [],\n",
    "        'pcc': [],\n",
    "        'jsd': [],\n",
    "        'avg_mae_bc': [],\n",
    "        'avg_mae_ec': [],\n",
    "        'avg_mae_pc': [],\n",
    "        'ram_usage': [],\n",
    "        'time': []\n",
    "    }\n",
    "    X = np.array([lr_train_anti_vectorized1, lr_train_anti_vectorized2, lr_train_anti_vectorized3], dtype=\"object\")\n",
    "    y = np.array([hr_train_anti_vectorized1, hr_train_anti_vectorized2, hr_train_anti_vectorized3], dtype=\"object\")\n",
    "    \n",
    "    loo = LeaveOneOut()\n",
    "\n",
    "\n",
    "    k = loo.get_n_splits(X)\n",
    "    for i, (train_index, val_index) in enumerate(loo.split(X)):\n",
    "        checkFold = i + 1\n",
    "        print(f'Fold {i+1}/{k}')\n",
    "        start_ram = psutil.Process().memory_info().rss / (1024 * 1024) # in MB\n",
    "        X_train, X_val = X[train_index], X[val_index]\n",
    "        y_train, y_val = y[train_index], y[val_index]\n",
    "        X_val_epoch = X_val[0][:20]\n",
    "        y_val_epoch = y_val[0][:20] \n",
    "        \n",
    "        # train_dataset = BrainConnectivityDataset(torch.from_numpy(X_train[0]), torch.from_numpy(y_train[0]), augment=True, augmentation_methods=augmentation_methods)\n",
    "        # val_dataset = BrainConnectivityDataset(torch.from_numpy(X_val), torch.from_numpy(y_val))\n",
    "\n",
    "        train_dataset = BrainConnectivityDataset(X_train[0], y_train[0], augment=True, augmentation_methods=augmentation_methods)\n",
    "        val_dataset = BrainConnectivityDataset(X_val[0], y_val[0])\n",
    "\n",
    "        # val_dataset_epoch = BrainConnectivityDataset(torch.from_numpy(X_val_epoch), torch.from_numpy(y_val_epoch))\n",
    "\n",
    "        model = get_model()\n",
    "        optimizer = optimizer_fn(model.parameters(), lr=lr)\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "        start_time = time.time()\n",
    "        Final_epoch = 0\n",
    "        breaks = False\n",
    "        epochLosses = 0\n",
    "        x = 0\n",
    "        # to get number of epochs\n",
    "        print(\"!!!!!!!GETTING NUMBER OF EPOCHS!!!!!!!\")\n",
    "        for epoch in range(num_epochs):\n",
    "            if breaks == False:\n",
    "                model.train()\n",
    "                train_loss = 0\n",
    "                for j in range(len(train_dataset)):\n",
    "                    optimizer.zero_grad()\n",
    "                    lr_graph, hr_graph = train_dataset[j]\n",
    "                    loss = loss_fn(lr_graph, hr_graph, model)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    train_loss += loss.item()\n",
    "                scheduler.step()\n",
    "                print(f'epoch: {epoch+1}, loss: {train_loss/len(train_dataset)}')\n",
    "                Final_epoch = epoch + 1\n",
    "                # 0.00005\n",
    "                if abs(epochLosses - loss.item()) < 0.00005:\n",
    "                    x+= 1\n",
    "                    print(\"x: \", x)\n",
    "                    if x >= 10:\n",
    "                        print(\"break\")\n",
    "                        breaks = True\n",
    "                epochLosses = loss.item()\n",
    "    \n",
    "        # Actual training\n",
    "        print(\"!!!!!!!!!ACTUAL TRAINING!!!!!!!!!\")\n",
    "        print(\"FINAL EPOCHS: \", Final_epoch)\n",
    "        for epoch in range(Final_epoch):\n",
    "            model.train()\n",
    "            train_loss = 0\n",
    "            for j in range(len(train_dataset)):\n",
    "                optimizer.zero_grad()\n",
    "                lr_graph, hr_graph = train_dataset[j]\n",
    "                loss = loss_fn(lr_graph, hr_graph, model)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_loss += loss.item()\n",
    "            scheduler.step()\n",
    "            print(f'epoch: {epoch+1}, loss: {train_loss/len(train_dataset)}')\n",
    "        \n",
    "        final_ram = psutil.Process().memory_info().rss / (1024 * 1024) # in MB\n",
    "        final_time = time.time()\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = 0\n",
    "            pred_matrices = []\n",
    "            gt_matrices = []\n",
    "            for j in range(len(val_dataset)):\n",
    "                lr_graph, hr_graph = val_dataset[j]\n",
    "                hr_graph_pred, _ = model(lr_graph)\n",
    "                val_loss += evaluate(hr_graph_pred, hr_graph).item()\n",
    "\n",
    "                if not training_only:\n",
    "                    pred_matrices.append(hr_graph_pred.cpu().numpy())\n",
    "                    gt_matrices.append(hr_graph.cpu().numpy())\n",
    "\n",
    "            val_loss /= len(val_dataset)\n",
    "            print(f'val_loss: {val_loss}')\n",
    "            losses.append(val_loss)\n",
    "\n",
    "            if not training_only:\n",
    "                mae, pcc, jsd, avg_mae_bc, avg_mae_ec, avg_mae_pc = compute_centrality_metrics(pred_matrices, gt_matrices)\n",
    "                \n",
    "                time_per_epoch = (final_time - start_time) / num_epochs\n",
    "                ram_usage = final_ram - start_ram\n",
    "\n",
    "                # Print metrics\n",
    "                print(\"mae\", mae)\n",
    "                print(\"pcc\", pcc)\n",
    "                print(\"jsd\", jsd)\n",
    "                print(\"avg_mae_bc\", avg_mae_bc)\n",
    "                print(\"avg_mae_ec\", avg_mae_ec)\n",
    "                print(\"avg_mae_pc\", avg_mae_pc)\n",
    "                print(f'Training time per epoch: {time_per_epoch} seconds')\n",
    "                print(f'Approximate RAM usage: {ram_usage} MB')\n",
    "\n",
    "                all_metrics['mae'].append(mae)\n",
    "                all_metrics['pcc'].append(pcc)\n",
    "                all_metrics['jsd'].append(jsd)\n",
    "                all_metrics['avg_mae_bc'].append(avg_mae_bc)\n",
    "                all_metrics['avg_mae_ec'].append(avg_mae_ec)\n",
    "                all_metrics['avg_mae_pc'].append(avg_mae_pc)\n",
    "                all_metrics['ram_usage'].append(ram_usage)\n",
    "                all_metrics['time'].append(time_per_epoch)              \n",
    "\n",
    "            # Save predictions\n",
    "\n",
    "            if checkFold == 1:\n",
    "                foldPred = write_result_to_file(model, val_dataset, f'23-05predictions_fold_{i+1}!!.csv')\n",
    "                evaluate_all(hr_train_anti_vectorized1, foldPred, \"fold1CSV-Cluster.csv\")\n",
    "            elif checkFold == 2:\n",
    "                foldPred = write_result_to_file(model, val_dataset, f'23-05predictions_fold_{i+1}!!.csv')\n",
    "                evaluate_all(hr_train_anti_vectorized2, foldPred, \"fold2CSV-Cluster.csv\")\n",
    "            else:\n",
    "                foldPred = write_result_to_file(model, val_dataset, f'23-05predictions_fold_{i+1}!!.csv')\n",
    "                evaluate_all(hr_train_anti_vectorized3, foldPred, \"fold3CSV-Cluster.csv\")\n",
    "                \n",
    "\n",
    "            \n",
    "            del model\n",
    "            del optimizer\n",
    "            del scheduler\n",
    "            del train_dataset\n",
    "            del val_dataset\n",
    "            torch.cuda.empty_cache()\n",
    "            sleep(5)\n",
    "            \n",
    "\n",
    "    return losses, all_metrics\n",
    "\n",
    "def write_result_to_file(model, test_dataset, filename='submission.csv'):\n",
    "    graph_prediction = []\n",
    "    # TODO: change to return all of the predictions \n",
    "    with open(filename, 'w') as file:\n",
    "        file.write('ID,Predicted\\n')\n",
    "        model.eval()\n",
    "        id = 1\n",
    "        with torch.no_grad():\n",
    "            for j in range(len(test_dataset)):\n",
    "                lr_graph, _  = test_dataset[j]\n",
    "                hr_graph_pred, _ = model(lr_graph)\n",
    "                hr_graph_pred = hr_graph_pred.cpu().numpy()\n",
    "                # hr_graph_pred = vectorize_symmetric_matrix(hr_graph_pred)\n",
    "                graph_prediction.append(hr_graph_pred)\n",
    "                for i, val in enumerate(hr_graph_pred):\n",
    "                    file.write('x')\n",
    "                    # graph_prediction.append((id, val, \"Public\"))\n",
    "                    id += 1\n",
    "    return graph_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Linear, ReLU, Sequential, Flatten, Unflatten, ModuleList, Sigmoid, BatchNorm1d\n",
    "\n",
    "def normalize(adj):\n",
    "    inv_sqrt_sum = torch.pow(torch.sum(adj, axis=1), -0.5)\n",
    "    inv_sqrt_sum[torch.isinf(inv_sqrt_sum)] = 0\n",
    "    D_hat_inv_sqrt = torch.diag(inv_sqrt_sum)\n",
    "    adj_norm = D_hat_inv_sqrt @ adj @ D_hat_inv_sqrt\n",
    "    return adj_norm\n",
    "\n",
    "\n",
    "def unpad(image, pad_width=26):\n",
    "    return image[pad_width:-pad_width, pad_width:-pad_width]\n",
    "\n",
    "\n",
    "def pad(image, pad_width=26):\n",
    "    padded= torch.nn.functional.pad(image, (pad_width, pad_width, pad_width, pad_width), 'constant', 0)\n",
    "    padded[torch.eye(padded.shape[0]).bool()] = 1\n",
    "    return padded\n",
    "\n",
    "\n",
    "class Pool(nn.Module):\n",
    "    def __init__(self, num_features, top_k):\n",
    "        super(Pool, self).__init__()\n",
    "        self.proj = Linear(num_features, 1) \n",
    "        self.activation = Sigmoid()\n",
    "        self.top_k = top_k\n",
    "        \n",
    "    def forward(self, x, adj):\n",
    "        scores = self.activation(self.proj(x).squeeze())\n",
    "        weights, idx = torch.topk(scores, self.top_k)\n",
    "        H_top_k = x[idx][:, idx]\n",
    "        adj_top_k = adj[idx][:, idx]\n",
    "        return H_top_k, adj_top_k, idx\n",
    "\n",
    "\n",
    "class GINConv(nn.Module):\n",
    "    def __init__(self, mlp):\n",
    "        super(GINConv, self).__init__()\n",
    "        self.mlp = mlp \n",
    "        self.eps = nn.Parameter(torch.zeros(1), requires_grad=True)\n",
    "\n",
    "    def forward(self, X, adj):\n",
    "        X = self.mlp((1+self.eps) * X + adj @ X)\n",
    "        \n",
    "        return X\n",
    "\n",
    "    \n",
    "class GSR(nn.Module):\n",
    "    def __init__(self, lr_dim=160, hr_dim=320):\n",
    "        super(GSR, self).__init__()\n",
    "        self.hr_to_lr_ratio = hr_dim // lr_dim\n",
    "        self.hr_dim = hr_dim\n",
    "        self.lr_dim = lr_dim\n",
    "        init_range = torch.sqrt(torch.tensor(6.0) / (self.hr_dim + self.hr_dim))\n",
    "        self.proj = nn.Parameter(torch.nn.init.uniform_(torch.empty(self.hr_dim, self.hr_dim), -init_range, init_range), requires_grad=True)\n",
    "    \n",
    "    def forward(self, x, adj): \n",
    "        _, eigen_vectors_lr = torch.linalg.eigh(adj, UPLO='U')\n",
    "        Sd = torch.cat([eigen_vectors_lr for _ in range(self.hr_to_lr_ratio)], dim=0)\n",
    "        \n",
    "        A_hr = self.proj @ Sd @ x\n",
    "        A_hr = torch.abs(A_hr)\n",
    "        A_hr[torch.eye(A_hr.shape[0]).bool()] = 0\n",
    "        A_hr = normalize(A_hr)\n",
    "        \n",
    "        H = A_hr @ A_hr.T\n",
    "        H = (H + H.T) / 2\n",
    "        # set diagonal to 1 \n",
    "        H[torch.eye(H.shape[0]).bool()] = 1\n",
    "        H = torch.abs(H)\n",
    "        \n",
    "        return H, A_hr  \n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_features, out_features, dropout=0.2, hidden_dim=320):\n",
    "        super(MLP, self).__init__()\n",
    "        self.proj1 = Linear(in_features, hidden_dim)\n",
    "        self.proj2 = Linear(hidden_dim, out_features)\n",
    "        self.activation = ReLU()\n",
    "        self.bn1 = BatchNorm1d(hidden_dim)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.proj2(x)\n",
    "        # x = self.activation(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class GraphSuperResolutionNet(nn.Module):\n",
    "    def __init__(self, num_encoders=1, num_decoders=2):\n",
    "        super(GraphSuperResolutionNet, self).__init__()\n",
    "        # GCN Layers for feature extraction\n",
    "        self.encoder = ModuleList([GINConv(MLP(160, 320))] + [GINConv(MLP(320, 320)) for _ in range(num_encoders-1)])\n",
    "        self.super_res_layer = GSR()\n",
    "        self.decoder = ModuleList([GINConv(MLP(268, 268)) for _ in range(num_decoders)])\n",
    "        self.pool = Pool(320, 268)\n",
    "\n",
    "    def forward(self, weighted_edges):\n",
    "        H = torch.eye(160).float().to(weighted_edges.device)\n",
    "        weighted_edges = normalize(weighted_edges)\n",
    "        \n",
    "        for layer in self.encoder:\n",
    "            H = layer(H, weighted_edges)\n",
    "        H, A_hr = self.super_res_layer(H, weighted_edges)\n",
    "        \n",
    "        H, A_hr, idx = self.pool(H, A_hr)\n",
    "        for layer in self.decoder:\n",
    "            H = layer(H, A_hr)\n",
    "        \n",
    "        H = (H + H.T) / 2\n",
    "        # set diagonal to 1\n",
    "        H[torch.eye(H.shape[0]).bool()] = 0\n",
    "        H = torch.abs(H)\n",
    "        \n",
    "        return H, idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "def get_model():\n",
    "  model = GraphSuperResolutionNet(2, 2)\n",
    "  model.to(device)\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(pred, target):\n",
    "    criterion = nn.L1Loss()\n",
    "    # mask the diagonal and lower triangular part of the matrix\n",
    "    mask = torch.triu(torch.ones_like(target, dtype=torch.bool), diagonal=1)\n",
    "    loss = criterion(pred[mask], target[mask])\n",
    "    return loss\n",
    "\n",
    "def calculate_gsr_loss(lr, hr, model):\n",
    "    hr_pred, idx = model(lr)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # hr_padded = pad(hr)\n",
    "    _, U_hr = torch.linalg.eigh(hr, UPLO='U')\n",
    "\n",
    "    loss = criterion(model.super_res_layer.proj[idx][:, idx], U_hr) + criterion(hr_pred, hr)\n",
    "\n",
    "    return loss\n",
    "\n",
    "# Hyperparameters\n",
    "optimizer_fn = optim.Adam\n",
    "epochs = 1000 # initial epochs to be changed\n",
    "lr = 0.00038638729584238843\n",
    "\n",
    "# 3-fold cross-validation using KFold\n",
    "k_folds = 3\n",
    "kf = KFold(n_splits=k_folds, shuffle=True, random_state=random_seed)\n",
    "scores, all_metrics = k_fold_cross_validation(get_model, calculate_gsr_loss, evaluate, optimizer_fn, epochs, lr, step_size=15, gamma=0.993088589189151, training_only=True)\n",
    "print(\"Mean validation loss:\", np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the submission file\n",
    "fold1Pred = getPred(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_centralities(adj_matrix):\n",
    "    if adj_matrix.shape[0] != adj_matrix.shape[1]:\n",
    "        raise ValueError(f\"Adjacency matrix is not square: shape={adj_matrix.shape}\")\n",
    "    print(f\"Processing adjacency matrix of shape: {adj_matrix.shape}\")\n",
    "    adj_matrix = np.array(adj_matrix)\n",
    "    G = nx.from_numpy_array(adj_matrix)\n",
    "    partition = community_louvain.best_partition(G)\n",
    "\n",
    "    # Calculate the participation coefficient with the partition\n",
    "    pc_dict = participation_coefficient(G, partition)\n",
    "\n",
    "    # Calculate averages of centrality measures\n",
    "    pr = nx.pagerank(G, alpha=0.9)\n",
    "    ec = nx.eigenvector_centrality_numpy(G, max_iter=100)\n",
    "    bc = nx.betweenness_centrality(G, normalized=True, endpoints=False)\n",
    "    ns = np.array(list(nx.degree_centrality(G).values())) * (len(G.nodes()) - 1)\n",
    "    acc = nx.average_clustering(G, weight=None)\n",
    "\n",
    "    # Average participation coefficient\n",
    "    pc_avg = np.mean(list(pc_dict.values()))\n",
    "\n",
    "    return {\n",
    "        'pr': np.mean(list(pr.values())),\n",
    "        'ec': np.mean(list(ec.values())),\n",
    "        'bc': np.mean(list(bc.values())),\n",
    "        'ns': ns,\n",
    "        'pc': pc_avg,\n",
    "        'acc': acc\n",
    "    }\n",
    "\n",
    "def participation_coefficient(G, partition):\n",
    "    # Initialize dictionary for participation coefficients\n",
    "    pc_dict = {}\n",
    "\n",
    "    # Calculate participation coefficient for each node\n",
    "    for node in G.nodes():\n",
    "        node_degree = G.degree(node)\n",
    "        if node_degree == 0:\n",
    "            pc_dict[node] = 0.0\n",
    "        else:\n",
    "            # Count within-module connections\n",
    "            within_module_degree = sum(1 for neighbor in G[node] if partition[neighbor] == partition[node])\n",
    "            # Calculate participation coefficient\n",
    "            pc_dict[node] = 1 - (within_module_degree / node_degree) ** 2\n",
    "\n",
    "    return pc_dict\n",
    "\n",
    "\n",
    "def evaluate_all(true_hr_matrices, predicted_hr_matrices, output_path='ID-randomCV.csv'):\n",
    "    print(true_hr_matrices.shape)\n",
    "\n",
    "    # print(predicted_hr_matrices.shape)\n",
    "    \n",
    "    num_subjects = true_hr_matrices.shape[0]\n",
    "    results = []\n",
    "\n",
    "    for i in range(num_subjects):\n",
    "        true_matrix = true_hr_matrices[i]\n",
    "        pred_matrix = predicted_hr_matrices[i]\n",
    "        print(\"true matrix\", true_matrix.shape)\n",
    "        print(\"pred\", pred_matrix.shape)\n",
    "\n",
    "        print(f\"Evaluating subject {i+1} with matrix shapes: true={true_matrix.shape}, pred={pred_matrix.shape}\")\n",
    "\n",
    "        if true_matrix.shape != pred_matrix.shape or true_matrix.shape[0] != true_matrix.shape[1]:\n",
    "            print(f\"Error: Matrix shape mismatch or not square for subject {i+1}: true={true_matrix.shape}, pred={pred_matrix.shape}\")\n",
    "            continue\n",
    "\n",
    "        metrics = {\n",
    "            'ID': i + 1,\n",
    "            'MAE': mean_absolute_error(true_matrix.flatten(), pred_matrix.flatten()),\n",
    "            'PCC': pearsonr(true_matrix.flatten(), pred_matrix.flatten())[0],\n",
    "            'JSD': jensenshannon(true_matrix.flatten(), pred_matrix.flatten()),\n",
    "        }\n",
    "\n",
    "        true_metrics = calculate_centralities(true_matrix)\n",
    "        pred_metrics = calculate_centralities(pred_matrix)\n",
    "\n",
    "        for key in ['NS', 'PR', 'EC', 'BC', 'PC', 'ACC']:\n",
    "            metrics[f'MAE in {key}'] = mean_absolute_error([true_metrics[key.lower()]], [pred_metrics[key.lower()]])\n",
    "\n",
    "        results.append(metrics)\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    if not df.empty:\n",
    "        # Check if the file exists to decide whether to write headers\n",
    "        file_exists = os.path.isfile(output_path)\n",
    "\n",
    "        df.to_csv(output_path, mode='a', header=not file_exists, index=False)\n",
    "        print(f\"Results appended to {output_path}.\")\n",
    "    else:\n",
    "        print(\"No data to save.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = pd.read_csv('./fold1CSVX-Cluster.csv').values\n",
    "x2 = pd.read_csv('./fold2CSVX-Cluster.csv').values\n",
    "x3 = pd.read_csv('./fold3CSVX-Cluster.csv').values\n",
    "comb1 = np.concatenate((x1,x2), axis=0)\n",
    "final = np.concatenate((comb1, x3), axis=0)\n",
    "\n",
    "df = pd.DataFrame(final)\n",
    "output_path = 'clusterCV.csv'\n",
    "if not df.empty:\n",
    "    # Check if the file exists to decide whether to write headers\n",
    "    file_exists = os.path.isfile(output_path)\n",
    "\n",
    "    df.to_csv(output_path, mode='a', header=not file_exists, index=False)\n",
    "    print(f\"Results appended to {output_path}.\")\n",
    "else:\n",
    "        print(\"No data to save.\")\n",
    "\n",
    "means = np.mean(final, axis=0)\n",
    "std_devs = np.std(final, axis=0)\n",
    "\n",
    "print(means)\n",
    "print(std_devs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "def plot(data):\n",
    "    avg_data = {metric: np.mean(values) for metric, values in data.items()}\n",
    "    std_data = {metric: np.std(values) for metric, values in data.items()}\n",
    "\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(8, 8), sharey=True)\n",
    "    colors = ['orange', 'green', 'steelblue', 'yellow', 'lightblue', 'lightgreen']\n",
    "    ind = np.arange(len(data))\n",
    "    width = 0.7\n",
    "\n",
    "    for i in range(3):\n",
    "        axs[i//2, i%2].bar(ind, [data[metric][i] for metric in data], width, color=colors)\n",
    "\n",
    "    ax_avg = axs[1, 1]\n",
    "\n",
    "    # Adding the average data with error bars to the last subplot\n",
    "    ax_avg.bar(ind, list(avg_data.values()), width, color=colors, yerr=list(std_data.values()), capsize=5)\n",
    "\n",
    "    # Adding some text for labels and custom x-axis tick labels, and set titles\n",
    "    for i, ax in enumerate(axs.flat):\n",
    "        if i < 3:  # For individual fold axes\n",
    "            ax.set_title(f'Fold {i+1}')\n",
    "        else:  # For the average plot\n",
    "            ax.set_title('Avg. Across Folds')\n",
    "        ax.set_xticks(ind)\n",
    "        ax.set_xticklabels(data.keys())\n",
    "        # ax.set_ylim(left=0)\n",
    "        for tick in ax.get_xticklabels():\n",
    "            tick.set_rotation(45)\n",
    "\n",
    "    # Remove the empty subplot (if any)\n",
    "    for i in range(len(data), 3):\n",
    "        fig.delaxes(axs.flat[i])\n",
    "\n",
    "    # Set the y-axis label\n",
    "    axs[0, 0].set_ylabel('Metric Value')\n",
    "\n",
    "    # Tight layout to adjust subplots automatically\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "\n",
    "data = {}\n",
    "data['MAE'] = all_metrics['mae']\n",
    "data['PCC'] = all_metrics['pcc']\n",
    "data['JSD'] = all_metrics['jsd']\n",
    "\n",
    "plot(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "data['MAE (PC)'] = all_metrics['avg_mae_pc']\n",
    "data['MAE (EC)'] = all_metrics['avg_mae_ec']\n",
    "data['MAE (BC)'] = all_metrics['avg_mae_bc']\n",
    "\n",
    "plot(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ram_mean = np.mean(all_metrics['ram_usage'])\n",
    "ram_std = np.std(all_metrics['ram_usage'])\n",
    "time_mean = np.mean(all_metrics['time'])\n",
    "time_std = np.std(all_metrics['time'])\n",
    "\n",
    "print(f'Average RAM usage: {ram_mean} ± {ram_std} MB')\n",
    "print(f'Average time: {time_mean} ± {time_std} seconds')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
