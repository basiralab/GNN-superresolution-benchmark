{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import KFold\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import scipy.io\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "import networkx as nx\n",
    "from evaluation import *\n",
    "from utils import *\n",
    "import math\n",
    "\n",
    "\n",
    "from scipy.io import savemat\n",
    "\n",
    "\n",
    "from scipy.io import loadmat\n",
    "import seaborn as sns\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatrixVectorizer:\n",
    "    \"\"\"\n",
    "    A class for transforming between matrices and vector representations.\n",
    "\n",
    "    This class provides methods to convert a symmetric matrix into a vector (vectorize)\n",
    "    and to reconstruct the matrix from its vector form (anti_vectorize), focusing on\n",
    "    vertical (column-based) traversal and handling of elements.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes the MatrixVectorizer instance.\n",
    "\n",
    "        The constructor currently does not perform any actions but is included for\n",
    "        potential future extensions where initialization parameters might be required.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def vectorize(matrix, include_diagonal=False):\n",
    "        \"\"\"\n",
    "        Converts a matrix into a vector by vertically extracting elements.\n",
    "\n",
    "        This method traverses the matrix column by column, collecting elements from the\n",
    "        upper triangle, and optionally includes the diagonal elements immediately below\n",
    "        the main diagonal based on the include_diagonal flag.\n",
    "\n",
    "        Parameters:\n",
    "        - matrix (numpy.ndarray): The matrix to be vectorized.\n",
    "        - include_diagonal (bool, optional): Flag to include diagonal elements in the vectorization.\n",
    "          Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "        - numpy.ndarray: The vectorized form of the matrix.\n",
    "        \"\"\"\n",
    "        # Determine the size of the matrix based on its first dimension\n",
    "        matrix_size = matrix.shape[0]\n",
    "\n",
    "        # Initialize an empty list to accumulate vector elements\n",
    "        vector_elements = []\n",
    "\n",
    "        # Iterate over columns and then rows to collect the relevant elements\n",
    "        for col in range(matrix_size):\n",
    "            for row in range(matrix_size):\n",
    "                # Skip diagonal elements if not including them\n",
    "                if row != col:\n",
    "                    if row < col:\n",
    "                        # Collect upper triangle elements\n",
    "                        vector_elements.append(matrix[row, col])\n",
    "                    elif include_diagonal and row == col + 1:\n",
    "                        # Optionally include the diagonal elements immediately below the diagonal\n",
    "                        vector_elements.append(matrix[row, col])\n",
    "\n",
    "        return np.array(vector_elements)\n",
    "\n",
    "    @staticmethod\n",
    "    def anti_vectorize(vector, matrix_size, include_diagonal=False):\n",
    "        \"\"\"\n",
    "        Reconstructs a matrix from its vector form, filling it vertically.\n",
    "\n",
    "        The method fills the matrix by reflecting vector elements into the upper triangle\n",
    "        and optionally including the diagonal elements based on the include_diagonal flag.\n",
    "\n",
    "        Parameters:\n",
    "        - vector (numpy.ndarray): The vector to be transformed into a matrix.\n",
    "        - matrix_size (int): The size of the square matrix to be reconstructed.\n",
    "        - include_diagonal (bool, optional): Flag to include diagonal elements in the reconstruction.\n",
    "          Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "        - numpy.ndarray: The reconstructed square matrix.\n",
    "        \"\"\"\n",
    "        # Initialize a square matrix of zeros with the specified size\n",
    "        matrix = np.zeros((matrix_size, matrix_size))\n",
    "\n",
    "        # Index to keep track of the current position in the vector\n",
    "        vector_idx = 0\n",
    "\n",
    "        # Fill the matrix by iterating over columns and then rows\n",
    "        for col in range(matrix_size):\n",
    "            for row in range(matrix_size):\n",
    "                # Skip diagonal elements if not including them\n",
    "                if row != col:\n",
    "                    if row < col:\n",
    "                        # Reflect vector elements into the upper triangle and its mirror in the lower triangle\n",
    "                        matrix[row, col] = vector[vector_idx]\n",
    "                        matrix[col, row] = vector[vector_idx]\n",
    "                        vector_idx += 1\n",
    "                    elif include_diagonal and row == col + 1:\n",
    "                        # Optionally fill the diagonal elements after completing each column\n",
    "                        matrix[row, col] = vector[vector_idx]\n",
    "                        matrix[col, row] = vector[vector_idx]\n",
    "                        vector_idx += 1\n",
    "\n",
    "        return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_HR_adj(label, split):\n",
    "    \"\"\"\n",
    "    Pads a high-resolution (HR) adjacency matrix with zeros on all sides.\n",
    "\n",
    "    Parameters:\n",
    "    - label (torch.Tensor): The HR adjacency matrix to be padded.\n",
    "    - split (int): The number of zeros to add to each side of the matrix.\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: The padded HR adjacency matrix.\n",
    "    \"\"\"\n",
    "    padded_label = F.pad(label, pad=(split, split, split, split), mode=\"constant\", value=0)\n",
    "    return padded_label\n",
    "\n",
    "def unpad(data, split):\n",
    "    idx_0 = data.shape[0]-split\n",
    "    idx_1 = data.shape[1]-split\n",
    "    # print(idx_0,idx_1)\n",
    "    train = data[split:idx_0, split:idx_1]\n",
    "    return train\n",
    "\n",
    "def normalize_adj_torch(mx):\n",
    "    # mx = mx.to_dense()\n",
    "    rowsum = mx.sum(1)\n",
    "    r_inv_sqrt = torch.pow(rowsum, -0.5).flatten()\n",
    "    r_inv_sqrt[torch.isinf(r_inv_sqrt)] = 0.\n",
    "    r_mat_inv_sqrt = torch.diag(r_inv_sqrt)\n",
    "    mx = torch.matmul(mx, r_mat_inv_sqrt)\n",
    "    mx = torch.transpose(mx, 0, 1)\n",
    "    mx = torch.matmul(mx, r_mat_inv_sqrt)\n",
    "    return mx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def weight_variable_glorot(output_dim):\n",
    "\n",
    "    input_dim = output_dim\n",
    "    init_range = np.sqrt(6.0 / (input_dim + output_dim))\n",
    "    initial = np.random.uniform(-init_range, init_range,(input_dim, output_dim))\n",
    "\n",
    "    return initial\n",
    "\n",
    "# %% [markdown]\n",
    "# ##2. Layers\n",
    "\n",
    "# %%\n",
    "class GSRLayer(nn.Module):\n",
    "\n",
    "  def __init__(self, hr_dim):\n",
    "    super(GSRLayer, self).__init__()\n",
    "\n",
    "    self.weights = torch.from_numpy(weight_variable_glorot(hr_dim)).type(torch.FloatTensor)\n",
    "    self.weights = torch.nn.Parameter(data=self.weights, requires_grad = True)\n",
    "\n",
    "  def forward(self,A,X):\n",
    "    lr = A\n",
    "    lr_dim = lr.shape[0]\n",
    "    f = X\n",
    "    # eig_val_lr, U_lr = torch.symeig(lr, eigenvectors=True,upper=True)\n",
    "    eig_val_lr, U_lr = torch.linalg.eigh(lr, UPLO='U')\n",
    "    # U_lr = torch.abs(U_lr)\n",
    "    eye_mat = torch.eye(lr_dim).type(torch.FloatTensor)\n",
    "    s_d = torch.cat((eye_mat,eye_mat),0)\n",
    "\n",
    "    a = torch.matmul(self.weights, s_d)\n",
    "    b = torch.matmul(a ,torch.t(U_lr))\n",
    "    f_d = torch.matmul(b ,f)\n",
    "    f_d = torch.abs(f_d)\n",
    "    self.f_d = f_d.fill_diagonal_(1)\n",
    "    adj = normalize_adj_torch(self.f_d)\n",
    "    X = torch.mm(adj, adj.t())\n",
    "    X = (X + X.t())/2\n",
    "    idx = torch.eye(320, dtype=bool)\n",
    "    X[idx]=1\n",
    "    return adj, torch.abs(X)\n",
    "\n",
    "\n",
    "\n",
    "class GraphConvolution(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
    "    \"\"\"\n",
    "    #160x320 320x320 =  160x320\n",
    "    def __init__(self, in_features, out_features, dropout=0., act=F.relu):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.dropout = dropout\n",
    "        self.act = act\n",
    "        self.weight = torch.nn.Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        torch.nn.init.xavier_uniform_(self.weight)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        # input = F.dropout(input, self.dropout, self.training)\n",
    "        support = torch.mm(input, self.weight)\n",
    "        output = torch.mm(adj, support)\n",
    "        # output = self.act(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphUnpool(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(GraphUnpool, self).__init__()\n",
    "\n",
    "    def forward(self, A, X, idx):\n",
    "        new_X = torch.zeros([A.shape[0], X.shape[1]])\n",
    "        new_X[idx] = X\n",
    "        return A, new_X\n",
    "\n",
    "\n",
    "class GraphPool(nn.Module):\n",
    "\n",
    "    def __init__(self, k, in_dim):\n",
    "        super(GraphPool, self).__init__()\n",
    "        self.k = k\n",
    "        self.proj = nn.Linear(in_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, A, X):\n",
    "        scores = self.proj(X)\n",
    "        # scores = torch.abs(scores)\n",
    "        scores = torch.squeeze(scores)\n",
    "        scores = self.sigmoid(scores/100)\n",
    "        num_nodes = A.shape[0]\n",
    "        values, idx = torch.topk(scores, int(self.k*num_nodes))\n",
    "        new_X = X[idx, :]\n",
    "        values = torch.unsqueeze(values, -1)\n",
    "        new_X = torch.mul(new_X, values)\n",
    "        A = A[idx, :]\n",
    "        A = A[:, idx]\n",
    "        return A, new_X, idx\n",
    "\n",
    "\n",
    "class GCN(nn.Module):\n",
    "\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(GCN, self).__init__()\n",
    "        self.proj = nn.Linear(in_dim, out_dim)\n",
    "        self.drop = nn.Dropout(p=0)\n",
    "\n",
    "    def forward(self, A, X):\n",
    "\n",
    "        X = self.drop(X)\n",
    "        # X = torch.matmul(A, X)\n",
    "        X = self.proj(X)\n",
    "        return X\n",
    "\n",
    "class GraphUnet(nn.Module):\n",
    "\n",
    "    def __init__(self, ks, in_dim, out_dim, dim=320):\n",
    "        super(GraphUnet, self).__init__()\n",
    "        self.ks = ks\n",
    "\n",
    "        self.start_gcn = GCN(in_dim, dim)\n",
    "        self.bottom_gcn = GCN(dim, dim)\n",
    "        self.end_gcn = GCN(2*dim, out_dim)\n",
    "        self.down_gcns = []\n",
    "        self.up_gcns = []\n",
    "        self.pools = []\n",
    "        self.unpools = []\n",
    "        self.l_n = len(ks)\n",
    "        for i in range(self.l_n):\n",
    "            self.down_gcns.append(GCN(dim, dim))\n",
    "            self.up_gcns.append(GCN(dim, dim))\n",
    "            self.pools.append(GraphPool(ks[i], dim))\n",
    "            self.unpools.append(GraphUnpool())\n",
    "\n",
    "    def forward(self, A, X):\n",
    "        adj_ms = []\n",
    "        indices_list = []\n",
    "        down_outs = []\n",
    "        X = self.start_gcn(A, X)\n",
    "        start_gcn_outs = X\n",
    "        org_X = X\n",
    "        for i in range(self.l_n):\n",
    "\n",
    "            X = self.down_gcns[i](A, X)\n",
    "            adj_ms.append(A)\n",
    "            down_outs.append(X)\n",
    "            A, X, idx = self.pools[i](A, X)\n",
    "            indices_list.append(idx)\n",
    "        X = self.bottom_gcn(A, X)\n",
    "        for i in range(self.l_n):\n",
    "            up_idx = self.l_n - i - 1\n",
    "\n",
    "            A, idx = adj_ms[up_idx], indices_list[up_idx]\n",
    "            A, X = self.unpools[i](A, X, idx)\n",
    "            X = self.up_gcns[i](A, X)\n",
    "            X = X.add(down_outs[up_idx])\n",
    "        X = torch.cat([X, org_X], 1)\n",
    "        X = self.end_gcn(A, X)\n",
    "\n",
    "        return X, start_gcn_outs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GSRNet(nn.Module):\n",
    "\n",
    "  def __init__(self, ks, lr_dim, hr_dim, hidden_dim):\n",
    "    super(GSRNet, self).__init__()\n",
    "\n",
    "    self.lr_dim = lr_dim\n",
    "    self.hr_dim = hr_dim\n",
    "    self.hidden_dim = hidden_dim\n",
    "    self.layer = GSRLayer(self.hr_dim)\n",
    "    self.net = GraphUnet(ks, self.lr_dim, self.hr_dim)\n",
    "    self.gc1 = GraphConvolution(self.hr_dim, self.hidden_dim, 0, act=F.relu)\n",
    "    self.gc2 = GraphConvolution(self.hidden_dim, self.hr_dim, 0, act=F.relu)\n",
    "\n",
    "  def forward(self,lr):\n",
    "\n",
    "    I = torch.eye(self.lr_dim).type(torch.FloatTensor)\n",
    "    A = normalize_adj_torch(lr).type(torch.FloatTensor)\n",
    "\n",
    "    self.net_outs, self.start_gcn_outs = self.net(A, I)\n",
    "    self.outputs, self.Z = self.layer(A, self.net_outs)\n",
    "\n",
    "    self.hidden1 = self.gc1(self.Z, self.outputs)\n",
    "    self.hidden2 = self.gc2(self.hidden1, self.outputs)\n",
    "\n",
    "    z = self.hidden2\n",
    "    z = (z + z.t())/2\n",
    "    idx = torch.eye(self.hr_dim, dtype=bool)\n",
    "    z[idx]=1\n",
    "\n",
    "    return torch.abs(z), self.net_outs, self.start_gcn_outs, self.outputs\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, in_dim):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.query = nn.Linear(in_dim, in_dim)\n",
    "        self.key = nn.Linear(in_dim, in_dim)\n",
    "        self.value = nn.Linear(in_dim, in_dim)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        query = self.query(x)\n",
    "        key = self.key(x)\n",
    "        value = self.value(x)\n",
    "        attention_scores = torch.matmul(query, key.transpose(-2, -1))\n",
    "        attention_scores = self.softmax(attention_scores)\n",
    "        attended_values = torch.matmul(attention_scores, value)\n",
    "        return attended_values\n",
    "\n",
    "        \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, in_dim, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = in_dim // num_heads\n",
    "        \n",
    "        self.query = nn.Linear(in_dim, in_dim)\n",
    "        self.key = nn.Linear(in_dim, in_dim)\n",
    "        self.value = nn.Linear(in_dim, in_dim)\n",
    "        \n",
    "        self.fc = nn.Linear(in_dim, in_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        seq_len, _ = x.size()\n",
    "        \n",
    "        query = self.query(x).view(seq_len, self.num_heads, self.head_dim).transpose(0, 1)\n",
    "        key = self.key(x).view(seq_len, self.num_heads, self.head_dim).transpose(0, 1)\n",
    "        value = self.value(x).view(seq_len, self.num_heads, self.head_dim).transpose(0, 1)\n",
    "        \n",
    "        attention_scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        attention_probs = F.softmax(attention_scores, dim=-1)\n",
    "        \n",
    "        context = torch.matmul(attention_probs, value).transpose(0, 1).contiguous().view(seq_len, -1)\n",
    "        output = self.fc(context)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "import torch_geometric as tg\n",
    "\n",
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim):\n",
    "        super(SimpleMLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, out_dim)\n",
    "         \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, in_dim)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "class AttentionResidualBlock(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, num_heads):\n",
    "        super(AttentionResidualBlock, self).__init__()\n",
    "        self.self_attention = MultiHeadAttention(in_dim, num_heads)\n",
    "        self.fc1 = nn.Linear(in_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, in_dim)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.self_attention(x)\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "class AGSRNet(nn.Module):\n",
    "    def __init__(self, ks, args):\n",
    "        super(AGSRNet, self).__init__()\n",
    "        self.lr_dim = args.lr_dim\n",
    "        self.hr_dim = args.hr_dim\n",
    "        self.hidden_dim = args.hidden_dim\n",
    "        \n",
    "        self.layer = GSRLayer(self.hr_dim)\n",
    "        self.net = GraphUnet(ks, self.lr_dim, self.hr_dim)\n",
    "        \n",
    "        self.self_attention = MultiHeadAttention(self.hr_dim, num_heads=8)\n",
    "        self.attention_residual_block1 = AttentionResidualBlock(self.hr_dim, self.hidden_dim, num_heads=8)\n",
    "        self.attention_residual_block2 = AttentionResidualBlock(self.hr_dim, self.hidden_dim, num_heads=8)\n",
    "        \n",
    "        self.gin1 = tg.nn.dense.DenseGINConv(SimpleMLP(self.hr_dim, self.hidden_dim, self.hidden_dim))\n",
    "        self.gin2 = tg.nn.dense.DenseGINConv(SimpleMLP(self.hidden_dim, self.hr_dim, self.hr_dim))\n",
    "        \n",
    "        self.skip_connection = nn.Linear(self.hr_dim, self.hr_dim)\n",
    "\n",
    "    def forward(self, lr):\n",
    "        I = torch.eye(self.lr_dim).type(torch.FloatTensor)\n",
    "        A = normalize_adj_torch(lr).type(torch.FloatTensor)\n",
    "        \n",
    "        self.net_outs, self.start_gcn_outs = self.net(A, I)\n",
    "        self.outputs, self.Z = self.layer(A, self.net_outs)\n",
    "        \n",
    "        self.hidden1 = F.relu(self.gin1(self.Z, self.outputs))\n",
    "        self.hidden2 = F.relu(self.gin2(self.hidden1, self.outputs).squeeze(0))\n",
    "        \n",
    "        z = self.hidden2\n",
    "        z = self.self_attention(z)\n",
    "        z = F.dropout(z, p=0.5, training=self.training)\n",
    "        \n",
    "        z = self.attention_residual_block1(z)\n",
    "        z = self.attention_residual_block2(z)\n",
    "        \n",
    "        skip_z = self.skip_connection(self.Z)\n",
    "        z = z + skip_z\n",
    "        \n",
    "        z = (z + z.t()) / 2\n",
    "        z = z.fill_diagonal_(1)\n",
    "        \n",
    "        return torch.abs(z), self.net_outs, self.start_gcn_outs, self.outputs\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.dense_1 = Dense(args.hr_dim, args.hr_dim, args)\n",
    "        self.bn1 = nn.BatchNorm1d(args.hr_dim)\n",
    "        self.relu_1 = nn.ReLU(inplace=False)\n",
    "        \n",
    "        self.residual_block1 = ResidualBlock(args.hr_dim, args.hr_dim)\n",
    "        self.residual_block2 = ResidualBlock(args.hr_dim, args.hr_dim)\n",
    "        \n",
    "        self.dense_2 = Dense(args.hr_dim, args.hr_dim, args)\n",
    "        self.bn2 = nn.BatchNorm1d(args.hr_dim)\n",
    "        self.relu_2 = nn.ReLU(inplace=False)\n",
    "        \n",
    "        self.dense_3 = Dense(args.hr_dim, 1, args)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        dc_den1 = self.relu_1(self.bn1(self.dense_1(inputs)))\n",
    "        dc_den1 = self.residual_block1(dc_den1)\n",
    "        \n",
    "        dc_den2 = self.relu_2(self.bn2(self.dense_2(dc_den1)))\n",
    "        dc_den2 = self.residual_block2(dc_den2)\n",
    "        \n",
    "        output = self.dense_3(dc_den2)\n",
    "        output = self.sigmoid(output)\n",
    "        \n",
    "        return torch.abs(output)\n",
    "class Dense(nn.Module):\n",
    "    def __init__(self, n1, n2, args):\n",
    "        super(Dense, self).__init__()\n",
    "        self.weights = torch.nn.Parameter(\n",
    "            torch.FloatTensor(n1, n2), requires_grad=True)\n",
    "        nn.init.normal_(self.weights, mean=args.mean_dense, std=args.std_dense)\n",
    "\n",
    "    def forward(self, x):\n",
    "        np.random.seed(1)\n",
    "        torch.manual_seed(1)\n",
    "\n",
    "        out = torch.mm(x, self.weights)\n",
    "        return out\n",
    "\n",
    "\n",
    "  \n",
    "def gaussian_noise_layer(input_layer, args):\n",
    "    z = torch.empty_like(input_layer)\n",
    "    noise = z.normal_(mean=args.mean_gaussian, std=args.std_gaussian)\n",
    "    z = torch.abs(input_layer + noise)\n",
    "\n",
    "    z = (z + z.t())/2\n",
    "    z = z.fill_diagonal_(1)\n",
    "    return z\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def perturb_graph(adj, drop_rate=0.1):\n",
    "    mask = torch.rand(adj.shape) > drop_rate\n",
    "    perturbed_adj = adj * mask\n",
    "    return perturbed_adj\n",
    "\n",
    "criterion = nn.L1Loss()  # Changed from MSELoss to L1Loss for MAE\n",
    "\n",
    "def train_asgr(model, subjects_adj, subjects_labels, args):\n",
    "\n",
    "    bce_loss = nn.BCELoss()\n",
    "    netD = Discriminator(args)\n",
    "    optimizerG = optim.Adam(model.parameters(), lr=args.lr)\n",
    "    optimizerD = optim.Adam(netD.parameters(), lr=args.lr)\n",
    "\n",
    "    all_epochs_loss = []\n",
    "    for epoch in range(args.epochs):\n",
    "        with torch.autograd.set_detect_anomaly(True):\n",
    "            epoch_loss = []\n",
    "            epoch_error = []\n",
    "            for lr, hr in zip(subjects_adj, subjects_labels):\n",
    "                optimizerD.zero_grad()\n",
    "                optimizerG.zero_grad()\n",
    "                \n",
    "                # Perturb the LR graph\n",
    "                #lr = torch.from_numpy(lr).type(torch.FloatTensor)\n",
    "                lr = perturb_graph(lr, drop_rate=0.1)  # Apply edge dropout\n",
    "                \n",
    "                # Pad HR adjacency matrix and ensure it is a PyTorch tensor\n",
    "                hr_padded = pad_HR_adj(hr, args.padding)\n",
    "                padded_hr = hr_padded.type(torch.FloatTensor)\n",
    "    \n",
    "\n",
    "                # Use torch.linalg.eigh() instead of deprecated torch.symeig()\n",
    "                eig_val_hr, U_hr = torch.linalg.eigh(padded_hr, UPLO='U')\n",
    "\n",
    "                model_outputs, net_outs, start_gcn_outs, layer_outs = model(\n",
    "                    lr)\n",
    "\n",
    "                mse_loss = args.lmbda * criterion(net_outs, start_gcn_outs) + criterion(\n",
    "                    model.layer.weights, U_hr) + criterion(model_outputs, padded_hr)\n",
    "\n",
    "                error = criterion(model_outputs, padded_hr)\n",
    "                real_data = model_outputs.detach()\n",
    "                fake_data = gaussian_noise_layer(padded_hr, args)\n",
    "\n",
    "                d_real = netD(real_data)\n",
    "                d_fake = netD(fake_data)\n",
    "\n",
    "                dc_loss_real = bce_loss(d_real, torch.ones(args.hr_dim, 1))\n",
    "                dc_loss_fake = bce_loss(d_fake, torch.zeros(args.hr_dim, 1))\n",
    "                dc_loss = dc_loss_real + dc_loss_fake\n",
    "\n",
    "                dc_loss.backward()\n",
    "                optimizerD.step()\n",
    "\n",
    "                d_fake = netD(gaussian_noise_layer(padded_hr, args))\n",
    "\n",
    "                gen_loss = bce_loss(d_fake, torch.ones(args.hr_dim, 1))\n",
    "                generator_loss = gen_loss + mse_loss\n",
    "                generator_loss.backward()\n",
    "                optimizerG.step()\n",
    "\n",
    "                epoch_loss.append(generator_loss.item())\n",
    "                epoch_error.append(error.item())\n",
    "\n",
    "            print(\"Epoch: \", epoch, \"Loss: \", np.mean(epoch_loss),\n",
    "                  \"Error: \", np.mean(epoch_error)*100, \"%\", flush=True)  # Error now represents MAE\n",
    "            all_epochs_loss.append(np.mean(epoch_loss))\n",
    "    return all_epochs_loss\n",
    "\n",
    "\n",
    "def train_asgr_validation(model, subjects_adj, subjects_labels, val_adj, val_labels, args):\n",
    "\n",
    "    bce_loss = nn.BCELoss()\n",
    "    netD = Discriminator(args)\n",
    "    optimizerG = optim.Adam(model.parameters(), lr=args.lr)\n",
    "    optimizerD = optim.Adam(netD.parameters(), lr=args.lr)\n",
    "\n",
    "    all_epochs_loss = []\n",
    "    all_epochs_val_loss = []\n",
    "\n",
    "    # Early stopping parameters\n",
    "    best_val_mae = float('inf')\n",
    "    patience = 10\n",
    "    counter = 0\n",
    "    for epoch in range(args.epochs):\n",
    "        with torch.autograd.set_detect_anomaly(True):\n",
    "            epoch_loss = []\n",
    "            epoch_error = []\n",
    "            for lr, hr in zip(subjects_adj, subjects_labels):\n",
    "                optimizerD.zero_grad()\n",
    "                optimizerG.zero_grad()\n",
    "                \n",
    "                # Perturb the LR graph\n",
    "                #lr = torch.from_numpy(lr).type(torch.FloatTensor)\n",
    "                lr = perturb_graph(lr, drop_rate=0.1)  # Apply edge dropout\n",
    "                \n",
    "                # Pad HR adjacency matrix and ensure it is a PyTorch tensor\n",
    "                hr_padded = pad_HR_adj(hr, args.padding)\n",
    "                padded_hr = hr_padded.type(torch.FloatTensor)\n",
    "    \n",
    "\n",
    "                # Use torch.linalg.eigh() instead of deprecated torch.symeig()\n",
    "                eig_val_hr, U_hr = torch.linalg.eigh(padded_hr, UPLO='U')\n",
    "\n",
    "                model_outputs, net_outs, start_gcn_outs, layer_outs = model(\n",
    "                    lr)\n",
    "\n",
    "                mse_loss = args.lmbda * criterion(net_outs, start_gcn_outs) + criterion(\n",
    "                    model.layer.weights, U_hr) + criterion(model_outputs, padded_hr)\n",
    "\n",
    "                error = criterion(model_outputs, padded_hr)\n",
    "                real_data = model_outputs.detach()\n",
    "                fake_data = gaussian_noise_layer(padded_hr, args)\n",
    "\n",
    "                d_real = netD(real_data)\n",
    "                d_fake = netD(fake_data)\n",
    "\n",
    "                dc_loss_real = bce_loss(d_real, torch.ones(args.hr_dim, 1))\n",
    "                dc_loss_fake = bce_loss(d_fake, torch.zeros(args.hr_dim, 1))\n",
    "                dc_loss = dc_loss_real + dc_loss_fake\n",
    "\n",
    "                dc_loss.backward()\n",
    "                optimizerD.step()\n",
    "\n",
    "                d_fake = netD(gaussian_noise_layer(padded_hr, args))\n",
    "\n",
    "                gen_loss = bce_loss(d_fake, torch.ones(args.hr_dim, 1))\n",
    "                generator_loss = gen_loss + mse_loss\n",
    "                generator_loss.backward()\n",
    "                optimizerG.step()\n",
    "\n",
    "                epoch_loss.append(generator_loss.item())\n",
    "                epoch_error.append(error.item())\n",
    "\n",
    "            all_epochs_loss.append(np.mean(epoch_loss))\n",
    "\n",
    "            val_mae = test_asgr(model, val_adj, val_labels, args) # Test on validation set\n",
    "            all_epochs_val_loss.append(val_mae)\n",
    "\n",
    "            print(\"Epoch: \", epoch, \"Loss: \", np.mean(epoch_loss),\n",
    "                  \"Error: \", np.mean(epoch_error)*100, \"%\", \"Validation MAE: \", val_mae, flush=True)  # Error now represents MAE\n",
    "            if val_mae < best_val_mae:\n",
    "                best_val_mae = val_mae\n",
    "                counter = 0\n",
    "            else:\n",
    "                counter += 1\n",
    "                if counter >= patience and epoch > 100:\n",
    "                    print(\"Early stopping at epoch \", epoch)\n",
    "                    return all_epochs_loss, all_epochs_val_loss\n",
    "    return all_epochs_loss, all_epochs_val_loss\n",
    "\n",
    "\n",
    "def test_asgr(model, test_adj, test_labels, args):\n",
    "\n",
    "    g_t = []\n",
    "    test_error = []\n",
    "    preds_list = []\n",
    "\n",
    "    for lr, hr in zip(test_adj, test_labels):\n",
    "        #all_zeros_lr = not np.any(lr)\n",
    "        all_zeros_lr = not torch.any(lr)\n",
    "        #all_zeros_hr = not np.any(hr)\n",
    "        all_zeros_hr = not torch.any(hr)\n",
    "        if all_zeros_lr == False and all_zeros_hr == False:\n",
    "            #lr = torch.from_numpy(lr).type(torch.FloatTensor)\n",
    "            #np.fill_diagonal(hr, 1)\n",
    "            hr = hr.fill_diagonal_(1)\n",
    "\n",
    "            hr = pad_HR_adj(hr, args.padding)\n",
    "            preds, a, b, c = model(lr)    \n",
    "\n",
    "            preds_list.append(preds.flatten().detach().numpy())\n",
    "            error = criterion(preds, hr)\n",
    "            g_t.append(hr.flatten())\n",
    "            test_error.append(error.item())\n",
    "\n",
    "    print(\"Test error MAE: \", np.mean(test_error), flush=True)  # Changed MSE to MAE in print statement\n",
    "    return np.mean(test_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 400 # bring to 200\n",
    "lr = 0.0001\n",
    "splits = 10\n",
    "lmbda = 16\n",
    "lr_dim = 160\n",
    "hr_dim = 320\n",
    "hidden_dim = 320\n",
    "padding = 26\n",
    "\n",
    "class Args:\n",
    "    epochs = epochs\n",
    "    lr = 0.0001\n",
    "    lmbda = 0.1\n",
    "    lr_dim = lr_dim\n",
    "    hr_dim = hr_dim\n",
    "    hidden_dim = hidden_dim\n",
    "    padding = 26\n",
    "    mean_dense = 0\n",
    "    std_dense = 0.01\n",
    "    mean_gaussian = 0\n",
    "    std_gaussian = 0.1\n",
    "\n",
    "args = Args()\n",
    "ks = [0.9, 0.7, 0.6, 0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLIT_1_LR_PATH = 'RandomCV/Train/Fold1/lr_split_1.csv'\n",
    "SPLIT_1_HR_PATH = 'RandomCV/Train/Fold1/hr_split_1.csv'\n",
    "SPLIT_2_LR_PATH = 'RandomCV/Train/Fold2/lr_split_2.csv'\n",
    "SPLIT_2_HR_PATH = 'RandomCV/Train/Fold2/hr_split_2.csv'\n",
    "SPLIT_3_LR_PATH = 'RandomCV/Train/Fold3/lr_split_3.csv'\n",
    "SPLIT_3_HR_PATH = 'RandomCV/Train/Fold3/hr_split_3.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_output(test_adj, model):\n",
    "    outputs = []\n",
    "    model.eval()\n",
    "\n",
    "    for lr_graph in test_adj:\n",
    "            #output = model(lr_graph)\n",
    "            model_outputs, net_outs, start_gcn_outs, layer_outs = model(lr_graph)\n",
    "            #model_outputs = unpad(model_outputs, padding)\n",
    "            #unpad and refactorize this\n",
    "            idx_0 = model_outputs.shape[0]-26\n",
    "            idx_1 = model_outputs.shape[1]-26\n",
    "            model_outputs = model_outputs[26:idx_0, 26:idx_1]\n",
    "            # append clipped outputs clipped between 0 and 1\n",
    "            outputs.append(model_outputs.detach().numpy())\n",
    "\n",
    "    outputs = np.array(outputs)\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seeds\n",
    "SEED = 42\n",
    "GET_METRICS = True\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Load Data\n",
    "split_1_adj, split_1_ground_truth = load_matrix_data(SPLIT_1_LR_PATH, SPLIT_1_HR_PATH, 93)\n",
    "split_2_adj, split_2_ground_truth = load_matrix_data(SPLIT_2_LR_PATH, SPLIT_2_HR_PATH, 93)\n",
    "split_3_adj, split_3_ground_truth = load_matrix_data(SPLIT_3_LR_PATH, SPLIT_3_HR_PATH, 93)\n",
    "\n",
    "train_losses_all_with_val = []\n",
    "val_losses_all = []\n",
    "train_losses_all_no_val = []\n",
    "fold_results = []\n",
    "\n",
    "# Run 3-fold CV\n",
    "for i in range(3):\n",
    "    print(f\"Fold {i+1}:\")\n",
    "    \n",
    "    # Determine train, validation, and test splits\n",
    "    if i == 0:\n",
    "        train_adj = torch.cat((split_2_adj[:-20], split_3_adj[:-20]), dim=0)\n",
    "        train_ground_truth = torch.cat((split_2_ground_truth[:-20], split_3_ground_truth[:-20]), dim=0)\n",
    "        val_adj = torch.cat((split_2_adj[-20:], split_3_adj[-20:]), dim=0)\n",
    "        val_ground_truth = torch.cat((split_2_ground_truth[-20:], split_3_ground_truth[-20:]), dim=0)\n",
    "        test_adj = split_1_adj\n",
    "        test_ground_truth = split_1_ground_truth\n",
    "    elif i == 1:\n",
    "        train_adj = torch.cat((split_1_adj[:-20], split_3_adj[:-20]), dim=0)\n",
    "        train_ground_truth = torch.cat((split_1_ground_truth[:-20], split_3_ground_truth[:-20]), dim=0)\n",
    "        val_adj = torch.cat((split_1_adj[-20:], split_3_adj[-20:]), dim=0)\n",
    "        val_ground_truth = torch.cat((split_1_ground_truth[-20:], split_3_ground_truth[-20:]), dim=0)\n",
    "        test_adj = split_2_adj\n",
    "        test_ground_truth = split_2_ground_truth\n",
    "    else:\n",
    "        train_adj = torch.cat((split_1_adj[:-20], split_2_adj[:-20]), dim=0)\n",
    "        train_ground_truth = torch.cat((split_1_ground_truth[:-20], split_2_ground_truth[:-20]), dim=0)\n",
    "        val_adj = torch.cat((split_1_adj[-20:], split_2_adj[-20:]), dim=0)\n",
    "        val_ground_truth = torch.cat((split_1_ground_truth[-20:], split_2_ground_truth[-20:]), dim=0)\n",
    "        test_adj = split_3_adj\n",
    "        test_ground_truth = split_3_ground_truth\n",
    "    \n",
    "    # Find early stopping epoch\n",
    "    model = AGSRNet(ks, args)\n",
    "    lr = 0.0001\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    train_losses, val_losses = train_asgr_validation(model, train_adj, train_ground_truth, val_adj, val_ground_truth, args)\n",
    "    train_losses_all_with_val.append(train_losses)\n",
    "    val_losses_all.append(val_losses)\n",
    "    num_epochs = len(train_losses)\n",
    "    \n",
    "    # Retrain model on full training set (without validation)\n",
    "    full_train_adj = torch.cat((train_adj, val_adj), dim=0)\n",
    "    full_train_ground_truth = torch.cat((train_ground_truth, val_ground_truth), dim=0)\n",
    "    \n",
    "    model = AGSRNet(ks, args)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    args.epochs = num_epochs\n",
    "    train_losses = train_asgr(model, full_train_adj, full_train_ground_truth, args)\n",
    "    train_losses_all_no_val.append(train_losses)\n",
    "    \n",
    "    # Get metrics for the left-out fold\n",
    "    test_outputs = compute_output(test_adj, model)\n",
    "    metrics = evaluate_all(test_ground_truth.detach().numpy(), test_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create plots for each fold\n",
    "for i in range(3):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    #plt.plot(train_losses_all[i], label='Training Loss')\n",
    "    plt.plot(val_losses_all[i], label='Validation MAE')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f'Fold {i+1} - Validation MAE')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create plots for each fold\n",
    "for i in range(3):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses_all_with_val[i], label='Training Loss (with validation)')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f'Fold {i+1} - Training Loss')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create plots for each fold\n",
    "for i in range(3):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses_all_no_val[i], label='Training Loss (No Validation)')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f'Fold {i+1} - Training Loss')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "identity_df = pd.read_csv('15-randomCV_old.csv', index_col=0, header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "identity_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add row averaging and std columns except for the first column and top row\n",
    "identity_df.loc['mean'] = identity_df.mean()\n",
    "identity_df.loc['std'] = identity_df.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "identity_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dataframe to a csv file\n",
    "identity_df.to_csv('randomCV.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ClusterCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 400 # bring to 200\n",
    "lr = 0.0001\n",
    "splits = 10\n",
    "lmbda = 16\n",
    "lr_dim = 160\n",
    "hr_dim = 320\n",
    "hidden_dim = 320\n",
    "padding = 26\n",
    "\n",
    "class Args:\n",
    "    epochs = epochs\n",
    "    lr = 0.0001\n",
    "    lmbda = 0.1\n",
    "    lr_dim = lr_dim\n",
    "    hr_dim = hr_dim\n",
    "    hidden_dim = hidden_dim\n",
    "    padding = 26\n",
    "    mean_dense = 0\n",
    "    std_dense = 0.01\n",
    "    mean_gaussian = 0\n",
    "    std_gaussian = 0.1\n",
    "\n",
    "args = Args()\n",
    "ks = [0.9, 0.7, 0.6, 0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLIT_1_LR_PATH = 'Cluster-CV/Fold1/lr_clusterA.csv'\n",
    "SPLIT_1_HR_PATH = 'Cluster-CV/Fold1/hr_clusterA.csv'\n",
    "SPLIT_2_LR_PATH = 'Cluster-CV/Fold2/lr_clusterB.csv'\n",
    "SPLIT_2_HR_PATH = 'Cluster-CV/Fold2/hr_clusterB.csv'\n",
    "SPLIT_3_LR_PATH = 'Cluster-CV/Fold3/lr_clusterC.csv'\n",
    "SPLIT_3_HR_PATH = 'Cluster-CV/Fold3/hr_clusterC.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_output(test_adj, model):\n",
    "    outputs = []\n",
    "    model.eval()\n",
    "\n",
    "    for lr_graph in test_adj:\n",
    "            #output = model(lr_graph)\n",
    "            model_outputs, net_outs, start_gcn_outs, layer_outs = model(lr_graph)\n",
    "            #model_outputs = unpad(model_outputs, padding)\n",
    "            #unpad and refactorize this\n",
    "            idx_0 = model_outputs.shape[0]-26\n",
    "            idx_1 = model_outputs.shape[1]-26\n",
    "            model_outputs = model_outputs[26:idx_0, 26:idx_1]\n",
    "            # append clipped outputs clipped between 0 and 1\n",
    "            outputs.append(model_outputs.detach().numpy())\n",
    "\n",
    "    outputs = np.array(outputs)\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seeds\n",
    "SEED = 42\n",
    "GET_METRICS = True\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Load Data\n",
    "split_1_adj, split_1_ground_truth = load_matrix_data(SPLIT_1_LR_PATH, SPLIT_1_HR_PATH, 103)\n",
    "split_2_adj, split_2_ground_truth = load_matrix_data(SPLIT_2_LR_PATH, SPLIT_2_HR_PATH, 103)\n",
    "split_3_adj, split_3_ground_truth = load_matrix_data(SPLIT_3_LR_PATH, SPLIT_3_HR_PATH, 76)\n",
    "\n",
    "train_losses_all_with_val = []\n",
    "val_losses_all = []\n",
    "train_losses_all_no_val = []\n",
    "fold_results = []\n",
    "\n",
    "# Run 3-fold CV\n",
    "for i in range(3):\n",
    "    print(f\"Fold {i+1}:\")\n",
    "    \n",
    "    # Determine train, validation, and test splits\n",
    "    if i == 0:\n",
    "        train_adj = torch.cat((split_2_adj[:-20], split_3_adj[:-20]), dim=0)\n",
    "        train_ground_truth = torch.cat((split_2_ground_truth[:-20], split_3_ground_truth[:-20]), dim=0)\n",
    "        val_adj = torch.cat((split_2_adj[-20:], split_3_adj[-20:]), dim=0)\n",
    "        val_ground_truth = torch.cat((split_2_ground_truth[-20:], split_3_ground_truth[-20:]), dim=0)\n",
    "        test_adj = split_1_adj\n",
    "        test_ground_truth = split_1_ground_truth\n",
    "    elif i == 1:\n",
    "        train_adj = torch.cat((split_1_adj[:-20], split_3_adj[:-20]), dim=0)\n",
    "        train_ground_truth = torch.cat((split_1_ground_truth[:-20], split_3_ground_truth[:-20]), dim=0)\n",
    "        val_adj = torch.cat((split_1_adj[-20:], split_3_adj[-20:]), dim=0)\n",
    "        val_ground_truth = torch.cat((split_1_ground_truth[-20:], split_3_ground_truth[-20:]), dim=0)\n",
    "        test_adj = split_2_adj\n",
    "        test_ground_truth = split_2_ground_truth\n",
    "    else:\n",
    "        train_adj = torch.cat((split_1_adj[:-20], split_2_adj[:-20]), dim=0)\n",
    "        train_ground_truth = torch.cat((split_1_ground_truth[:-20], split_2_ground_truth[:-20]), dim=0)\n",
    "        val_adj = torch.cat((split_1_adj[-20:], split_2_adj[-20:]), dim=0)\n",
    "        val_ground_truth = torch.cat((split_1_ground_truth[-20:], split_2_ground_truth[-20:]), dim=0)\n",
    "        test_adj = split_3_adj\n",
    "        test_ground_truth = split_3_ground_truth\n",
    "    \n",
    "    # Find early stopping epoch\n",
    "    model = AGSRNet(ks, args)\n",
    "    lr = 0.0001\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    train_losses, val_losses = train_asgr_validation(model, train_adj, train_ground_truth, val_adj, val_ground_truth, args)\n",
    "    train_losses_all_with_val.append(train_losses)\n",
    "    val_losses_all.append(val_losses)\n",
    "    num_epochs = len(train_losses)\n",
    "    \n",
    "    # Retrain model on full training set (without validation)\n",
    "    full_train_adj = torch.cat((train_adj, val_adj), dim=0)\n",
    "    full_train_ground_truth = torch.cat((train_ground_truth, val_ground_truth), dim=0)\n",
    "    \n",
    "    model = AGSRNet(ks, args)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    args.epochs = num_epochs\n",
    "    train_losses = train_asgr(model, full_train_adj, full_train_ground_truth, args)\n",
    "    train_losses_all_no_val.append(train_losses)\n",
    "    \n",
    "    # Get metrics for the left-out fold\n",
    "    test_outputs = compute_output(test_adj, model)\n",
    "    metrics = evaluate_all(test_ground_truth.detach().numpy(), test_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create plots for each fold\n",
    "for i in range(3):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    #plt.plot(train_losses_all[i], label='Training Loss')\n",
    "    plt.plot(val_losses_all[i], label='Validation MAE')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f'Fold {i+1} - Validation MAE')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create plots for each fold\n",
    "for i in range(3):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses_all_with_val[i], label='Training Loss (with validation)')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f'Fold {i+1} - Training Loss')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create plots for each fold\n",
    "for i in range(3):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses_all_no_val[i], label='Training Loss (No Validation)')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f'Fold {i+1} - Training Loss')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "identity_df = pd.read_csv('ID-randomCV.csv', index_col=0, header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "identity_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add row averaging and std columns except for the first column and top row\n",
    "identity_df.loc['mean'] = identity_df.mean()\n",
    "identity_df.loc['std'] = identity_df.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "identity_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dataframe to a csv file\n",
    "identity_df.to_csv('clusterCV.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dglp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
