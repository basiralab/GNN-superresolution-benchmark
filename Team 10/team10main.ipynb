{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gnasgjf15mcQ",
    "outputId": "3a89ff19-5469-4cdb-fd2e-09dfbe29d8c7"
   },
   "outputs": [],
   "source": [
    "# !pip install -r requirements10.txt (file removed, needed dependeices below)\n",
    "\n",
    "# torch~=2.2.0\n",
    "# numpy~=1.24.3\n",
    "# matplotlib~=3.8.0\n",
    "# psutil~=5.9.6\n",
    "# tqdm~=4.66.1\n",
    "# scipy~=1.12.0\n",
    "# pandas~=2.2.0\n",
    "# seaborn~=0.13.0\n",
    "# networkx~=3.2.1\n",
    "# scikit-learn~=1.4.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LBttvFPQINH9"
   },
   "source": [
    "# **Set the file paths and notes**\n",
    "\n",
    "<p>Use option = 0 if you want to do training and validation using 2 of the folds (keeping the 3rd fold for final training and testing).</p>\n",
    "\n",
    "<p>Use option = 1 if you want to do final training (using 2 folds) and testing (with the 3rd fold).</p>\n",
    "\n",
    "Be sure to update the name of the files - doing 3 cross-validation in this project is done manually with the prepared data to create comparable results with the rest of the projects. See an example below.\n",
    "\n",
    "Due to the nature of this team's model, early stopping is carried out by manual experimentation during testing and validation for 3CV.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gNKmBPLG5mXQ"
   },
   "outputs": [],
   "source": [
    "option = 1\n",
    "\n",
    "# path for training and validation (without using the 3rd fold). This is an example from the ClusterCV folder\n",
    "path_lr_data_tr = 'lr_split_AandB_training.csv'\n",
    "path_hr_data_tr = 'hr_split_AandB_training.csv'\n",
    "path_lr_data_valid = 'lr_split_AandB_validation.csv'\n",
    "path_hr_data_valid = 'hr_split_AandB_validation.csv'\n",
    "\n",
    "# path for final training and testing (using fold A and B for training and C for testing).This is an example from the ClusterCV folder\n",
    "path_lr_data = 'lr_split_AandB_finaltraining.csv'\n",
    "path_hr_data = 'hr_split_AandB_finaltraining.csv'\n",
    "path_lr_data_test = 'lr_clusterC.csv'\n",
    "path_hr_data_test = 'hr_clusterC.csv'\n",
    "\n",
    "# path to save the evaluation metrics to - this means that Cluster C will be used for testing\n",
    "path_eval_matrics = '10-clusterCV-split3.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ePtRVqsvz-kg"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "USE_GPU = False\n",
    "device = 'cuda' if torch.cuda.is_available() and USE_GPU else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hV_-AT1Gz-iE"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GAT(nn.Module):\n",
    "    \"\"\"\n",
    "    A basic implementation of the GAT layer.\n",
    "\n",
    "    This layer applies an attention mechanism in the graph convolution process,\n",
    "    allowing the model to focus on different parts of the neighborhood\n",
    "    of each node.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features, activation=None, layer_norm=False, p=0):\n",
    "        super(GAT, self).__init__()\n",
    "        # Initialize the weights, bias, and attention parameters as\n",
    "        # trainable parameters\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        self.bias = nn.Parameter(torch.zeros(out_features))\n",
    "        self.phi = nn.Parameter(torch.FloatTensor(2 * out_features, 1))\n",
    "        self.activation = activation\n",
    "\n",
    "        self.layer_norm = layer_norm\n",
    "        if layer_norm:\n",
    "            self.norm_layer = nn.LayerNorm(out_features)\n",
    "\n",
    "        self.dropout_layer = nn.Dropout(p=p) if p > 0.0 else nn.Identity()\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "        nn.init.xavier_uniform_(self.phi)\n",
    "\n",
    "    def forward(self, adj, input):\n",
    "        \"\"\"\n",
    "        Forward pass of the GAT layer.\n",
    "\n",
    "        Parameters:\n",
    "        input (Tensor): The input features of the nodes.\n",
    "        adj (Tensor): The adjacency matrix of the graph.\n",
    "\n",
    "        Returns:\n",
    "        Tensor: The output features of the nodes after applying the GAT layer.\n",
    "        \"\"\"\n",
    "        ############# Your code here ############\n",
    "        ## 1. Apply linear transformation and add bias\n",
    "        H_k = input @ self.weight + self.bias\n",
    "        ## 2. Compute the attention scores utilizing the previously\n",
    "        ## established mechanism.\n",
    "        ## Note: Keep in mind that employing matrix notation can\n",
    "        ## optimize this process.\n",
    "        N, D = H_k.shape\n",
    "        H_k_1 = H_k @ self.phi[:D]\n",
    "        H_k_2 = H_k @ self.phi[D:]\n",
    "        S = H_k_1 + H_k_2.transpose(0, 1)\n",
    "        ## Apply a non-linearity before masking\n",
    "        S = F.leaky_relu(S)\n",
    "        ## 3. Compute mask based on adjacency matrix\n",
    "        mask = (adj + torch.eye(adj.shape[0]).to(adj.device)) == 0\n",
    "        ## 4. Apply mask to the pre-attention matrix\n",
    "        S_masked = torch.where(mask, torch.tensor(-9e15).to(adj.device), S)\n",
    "        ## 5. Compute attention weights using softmax\n",
    "        S_normalised = F.softmax(S_masked, dim=1)\n",
    "        ## 6. Aggregate features based on attention weights\n",
    "        ## Note: name the last line as `h`\n",
    "        S_normalised = self.dropout_layer(S_normalised)\n",
    "        h = S_normalised @ H_k\n",
    "        ## (9-10 lines of code)\n",
    "        #########################################\n",
    "        h = self.norm_layer(h) if self.layer_norm else h\n",
    "        return self.activation(h) if self.activation else h\n",
    "\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    \"\"\"\n",
    "    A basic implementation of GCN layer.\n",
    "    It aggregates information from a node's neighbors\n",
    "    using mean aggregation.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features, activation=None, layer_norm=False, p=0):\n",
    "        super(GCN, self).__init__()\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        self.bias = nn.Parameter(torch.zeros(out_features))\n",
    "        self.activation = activation\n",
    "\n",
    "        self.layer_norm = layer_norm\n",
    "        if layer_norm:\n",
    "            self.norm_layer = nn.LayerNorm(out_features)\n",
    "\n",
    "        self.dropout_layer = nn.Dropout(p=p) if p > 0.0 else nn.Identity()\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "\n",
    "    def forward(self, adj, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the GCN layer.\n",
    "\n",
    "        Parameters:\n",
    "        input (Tensor): The input features of the nodes.\n",
    "        adj (Tensor): The adjacency matrix of the graph.\n",
    "\n",
    "        Returns:\n",
    "        Tensor: The output features of the nodes after applying the GCN layer.\n",
    "        \"\"\"\n",
    "        # apply dropout\n",
    "        x = self.dropout_layer(x)\n",
    "        ############# Your code here ############\n",
    "        ## Note:\n",
    "        ## 1. Apply the linear transformation\n",
    "        transformed = torch.matmul(x, self.weight)\n",
    "        ## 2. Perform the graph convolution operation\n",
    "        h = torch.matmul(adj, transformed) + self.bias\n",
    "        ## Note: rename the last line as `output`\n",
    "        ## (2 lines of code)\n",
    "        #########################################\n",
    "        h = self.norm_layer(h) if self.layer_norm else h\n",
    "        h = self.activation(h) if self.activation else h\n",
    "        return h\n",
    "\n",
    "\n",
    "class GraphUnpool(nn.Module):\n",
    "    '''\n",
    "    Graph unpool layer\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super(GraphUnpool, self).__init__()\n",
    "\n",
    "    def forward(self, A, X, idx):\n",
    "        new_X = torch.zeros([A.shape[0], X.shape[1]], device=device)\n",
    "        new_X[idx] = X\n",
    "        return A, new_X\n",
    "\n",
    "class GraphPool(nn.Module):\n",
    "    '''\n",
    "    Graph polling layer\n",
    "    '''\n",
    "    def __init__(self, k, in_dim):\n",
    "        super(GraphPool, self).__init__()\n",
    "        self.k = k\n",
    "        self.proj = nn.Linear(in_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, A, X):\n",
    "        scores = self.proj(X)\n",
    "        # scores = torch.abs(scores)\n",
    "        scores = torch.squeeze(scores)\n",
    "        scores = self.sigmoid(scores/100)\n",
    "        num_nodes = A.shape[0]\n",
    "        values, idx = torch.topk(scores, int(self.k*num_nodes))\n",
    "        new_X = X[idx, :]\n",
    "        values = torch.unsqueeze(values, -1)\n",
    "        new_X = torch.mul(new_X, values)\n",
    "        A = A[idx, :]\n",
    "        A = A[:, idx]\n",
    "        return A, new_X, idx\n",
    "\n",
    "class GraphUnet(nn.Module):\n",
    "    '''\n",
    "    an improved version os Graph U-Net base a GAT\n",
    "    '''\n",
    "    def __init__(self, ks, in_dim, out_dim, dim=320):\n",
    "        super(GraphUnet, self).__init__()\n",
    "        self.ks = ks\n",
    "        self.start_gcn = GAT(in_dim, dim, nn.ReLU(True)).to(device)\n",
    "        self.bottom_gcn = GAT(dim, dim, nn.ReLU(True)).to(device)\n",
    "        self.end_gcn = GAT(2*dim, out_dim, nn.ReLU(True)).to(device)\n",
    "        self.down_gcns = []\n",
    "        self.up_gcns = []\n",
    "        self.pools = []\n",
    "        self.unpools = []\n",
    "        self.l_n = len(ks)\n",
    "        for i in range(self.l_n):\n",
    "            self.down_gcns.append(GAT(dim, dim, nn.ReLU(True), p=0.5).to(device))\n",
    "            self.up_gcns.append(GAT(dim, dim, nn.ReLU(True), p=0.5).to(device))\n",
    "            self.pools.append(GraphPool(ks[i], dim).to(device))\n",
    "            self.unpools.append(GraphUnpool().to(device))\n",
    "\n",
    "    def forward(self, A, X):\n",
    "        adj_ms = []\n",
    "        indices_list = []\n",
    "        down_outs = []\n",
    "        X = self.start_gcn(A, X)\n",
    "        start_gcn_outs = X\n",
    "        org_X = X\n",
    "        for i in range(self.l_n):\n",
    "\n",
    "            X = self.down_gcns[i](A, X)\n",
    "            adj_ms.append(A)\n",
    "            down_outs.append(X)\n",
    "            A, X, idx = self.pools[i](A, X)\n",
    "            indices_list.append(idx)\n",
    "        X = self.bottom_gcn(A, X)\n",
    "        for i in range(self.l_n):\n",
    "            up_idx = self.l_n - i - 1\n",
    "\n",
    "            A, idx = adj_ms[up_idx], indices_list[up_idx]\n",
    "            A, X = self.unpools[i](A, X, idx)\n",
    "            X = self.up_gcns[i](A, X)\n",
    "            X = X.add(down_outs[up_idx])\n",
    "        X = torch.cat([X, org_X], 1)\n",
    "        X = self.end_gcn(A, X)\n",
    "\n",
    "        return X, start_gcn_outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WlaAPAiK0QVP"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def weight_variable_glorot(output_dim):\n",
    "    '''\n",
    "    glorot weight initialization method\n",
    "    '''\n",
    "\n",
    "    input_dim = output_dim\n",
    "    init_range = np.sqrt(6.0 / (input_dim + output_dim))\n",
    "    initial = np.random.uniform(-init_range, init_range,\n",
    "                                (input_dim, output_dim))\n",
    "\n",
    "    return initial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JEHCT85nxzK_"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class GSRLayer(nn.Module):\n",
    "    '''\n",
    "    GSRLayers which is used for predicting an HR connectome from the LR connectivity matrix and feature embeddings of the LR connectome\n",
    "    '''\n",
    "\n",
    "    def __init__(self, hr_dim):\n",
    "        super(GSRLayer, self).__init__()\n",
    "\n",
    "        self.weights = torch.from_numpy(\n",
    "            weight_variable_glorot(hr_dim)).type(torch.FloatTensor)\n",
    "        self.weights = torch.nn.Parameter(\n",
    "            data=self.weights, requires_grad=True)\n",
    "\n",
    "    def forward(self, A, X):\n",
    "        with torch.autograd.set_detect_anomaly(True):\n",
    "\n",
    "            lr = A\n",
    "            lr_dim = lr.shape[0]\n",
    "            f = X\n",
    "            eig_val_lr, U_lr = torch.linalg.eigh(lr, UPLO='U')\n",
    "\n",
    "            eye_mat = torch.eye(lr_dim).type(torch.FloatTensor).to(A.device)\n",
    "            s_d = torch.cat((eye_mat, eye_mat), 0)\n",
    "\n",
    "            a = torch.matmul(self.weights, s_d)\n",
    "            b = torch.matmul(a, torch.t(U_lr))\n",
    "            f_d = torch.matmul(b, f)\n",
    "            f_d = torch.abs(f_d)\n",
    "            f_d = f_d.fill_diagonal_(1)\n",
    "            adj = f_d\n",
    "\n",
    "            X = torch.mm(adj, adj.t())\n",
    "            X = (X + X.t())/2\n",
    "            X = X.fill_diagonal_(1)\n",
    "        return adj, torch.abs(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZuS9aTQ40HwO"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import scipy.io\n",
    "\n",
    "path = '/dgl/slim_dataset'\n",
    "roi_str = 'ROI_FC.mat'\n",
    "def pad_HR_adj(label, split):\n",
    "\n",
    "    label = np.pad(label, ((split, split), (split, split)), mode=\"constant\")\n",
    "    np.fill_diagonal(label, 1)\n",
    "    return label\n",
    "\n",
    "\n",
    "def normalize_adj_torch(mx):\n",
    "    rowsum = mx.sum(1)\n",
    "    r_inv_sqrt = torch.pow(rowsum, -0.5).flatten()\n",
    "    r_inv_sqrt[torch.isinf(r_inv_sqrt)] = 0.\n",
    "    r_mat_inv_sqrt = torch.diag(r_inv_sqrt)\n",
    "    mx = torch.matmul(mx, r_mat_inv_sqrt)\n",
    "    mx = torch.transpose(mx, 0, 1)\n",
    "    mx = torch.matmul(mx, r_mat_inv_sqrt)\n",
    "    return mx\n",
    "\n",
    "\n",
    "def unpad(data, split):\n",
    "\n",
    "    idx_0 = data.shape[0]-split\n",
    "    idx_1 = data.shape[1]-split\n",
    "    train = data[split:idx_0, split:idx_1]\n",
    "    return train\n",
    "\n",
    "\n",
    "def extract_data(subject, session_str, parcellation_str, subjects_roi):\n",
    "    folder_path = os.path.join(\n",
    "        path, str(subject), session_str, parcellation_str)\n",
    "    roi_data = scipy.io.loadmat(os.path.join(folder_path, roi_str))\n",
    "    roi = roi_data['r']\n",
    "\n",
    "    # Replacing NaN values\n",
    "    col_mean = np.nanmean(roi, axis=0)\n",
    "    inds = np.where(np.isnan(roi))\n",
    "    roi[inds] = 1\n",
    "\n",
    "    # Taking the absolute values of the matrix\n",
    "    roi = np.absolute(roi, dtype=np.float32)\n",
    "\n",
    "    if parcellation_str == 'shen_268':\n",
    "        roi = np.reshape(roi, (1, 268, 268))\n",
    "    else:\n",
    "        roi = np.reshape(roi, (1, 160, 160))\n",
    "\n",
    "    if subject == 25629:\n",
    "        subjects_roi = roi\n",
    "    else:\n",
    "        subjects_roi = np.concatenate((subjects_roi, roi), axis=0)\n",
    "\n",
    "    return subjects_roi\n",
    "\n",
    "\n",
    "def load_data(start_value, end_value):\n",
    "\n",
    "    subjects_label = np.zeros((1, 268, 268))\n",
    "    subjects_adj = np.zeros((1, 160, 160))\n",
    "\n",
    "    for subject in range(start_value, end_value):\n",
    "        subject_path = os.path.join(path, str(subject))\n",
    "\n",
    "        if 'session_1' in os.listdir(subject_path):\n",
    "\n",
    "            subjects_label = extract_data(\n",
    "                subject, 'session_1', 'shen_268', subjects_label)\n",
    "            subjects_adj = extract_data(\n",
    "                subject, 'session_1', 'Dosenbach_160', subjects_adj)\n",
    "\n",
    "    return subjects_adj, subjects_label\n",
    "\n",
    "\n",
    "def data():\n",
    "    subjects_adj, subjects_labels = load_data(25629, 25830)\n",
    "    test_adj_1, test_labels_1 = load_data(25831, 25863)\n",
    "    test_adj_2, test_labels_2 = load_data(30701, 30757)\n",
    "    test_adj = np.concatenate((test_adj_1, test_adj_2), axis=0)\n",
    "    test_labels = np.concatenate((test_labels_1, test_labels_2), axis=0)\n",
    "    return subjects_adj, subjects_labels, test_adj, test_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0HuUbr1FxzNi"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class AGSRNet(nn.Module):\n",
    "    '''\n",
    "    Original AGSRNet model class from the github repo https://github.com/basiralab/AGSR-Net\n",
    "    '''\n",
    "\n",
    "    def __init__(self, ks, args):\n",
    "        super(AGSRNet, self).__init__()\n",
    "\n",
    "        self.lr_dim = args.lr_dim\n",
    "        self.hr_dim = args.hr_dim\n",
    "        self.hidden_dim = args.hidden_dim\n",
    "        self.layer = GSRLayer(self.hr_dim).to(device)\n",
    "        self.net = GraphUnet(ks, self.lr_dim, self.hr_dim).to(device)\n",
    "        self.gc1 = GCN(self.hr_dim, self.hidden_dim, nn.ReLU(True), True, p=0.5).to(device)\n",
    "        self.gc2 = GCN(self.hidden_dim, self.hidden_dim, nn.ReLU(True), True, p=0.5).to(device)\n",
    "        self.gc3 = GCN(self.hidden_dim, self.hr_dim, nn.ReLU(True)).to(device)\n",
    "\n",
    "    def forward(self, lr, lr_dim, hr_dim):\n",
    "        with torch.autograd.set_detect_anomaly(True):\n",
    "\n",
    "            I = torch.eye(self.lr_dim).type(torch.FloatTensor).to(device)\n",
    "            A = normalize_adj_torch(lr).type(torch.FloatTensor).to(device)\n",
    "\n",
    "            # Unet\n",
    "            net_outs, start_gcn_outs = self.net(A, I)\n",
    "\n",
    "            # GSRLayer\n",
    "            adj, z = self.layer(A, net_outs)\n",
    "\n",
    "            # 3 GCN layers\n",
    "            z = self.gc1(adj, z)\n",
    "            z = self.gc2(adj, z)\n",
    "            z = self.gc3(adj, z)\n",
    "\n",
    "            z = (z + z.t())/2\n",
    "            z = z.fill_diagonal_(1)\n",
    "\n",
    "        return torch.abs(z), net_outs, start_gcn_outs, adj\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    '''\n",
    "    GCN based discriminators\n",
    "    '''\n",
    "    def __init__(self, args):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.in_layer = GCN(args.hr_dim, 2 * args.hr_dim, nn.LeakyReLU(0.2, True)).to(device)\n",
    "        self.hidden1 = GCN(2 * args.hr_dim, args.hr_dim, nn.LeakyReLU(0.2, True)).to(device)\n",
    "        self.hidden2 = GCN(args.hr_dim, args.hr_dim // 2, nn.LeakyReLU(0.2, True)).to(device)\n",
    "        self.out_layer = GCN(args.hr_dim // 2, 1, nn.Sigmoid()).to(device)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        np.random.seed(1)\n",
    "        torch.manual_seed(1)\n",
    "\n",
    "        A = normalize_adj_torch(inputs).to(device)\n",
    "        # A = A + torch.eye(A.shape[0], dtype=torch.float, device=device)\n",
    "\n",
    "        H = self.in_layer(A, inputs)\n",
    "        H = self.hidden1(A, H)\n",
    "        H = self.hidden2(A, H)\n",
    "        output = self.out_layer(A, H)\n",
    "\n",
    "        return output\n",
    "\n",
    "def add_noise_to_adjacency_matrix(adj_matrix, noise_level=0.05):\n",
    "    # Assume adj_matrix is a numpy array representing your adjacency matrix\n",
    "    noise = np.random.randn(*adj_matrix.shape) * noise_level\n",
    "    noisy_adj_matrix = adj_matrix + noise\n",
    "    # Ensure the noisy adjacency matrix is still symmetric for undirected graphs\n",
    "    noisy_adj_matrix = np.maximum(noisy_adj_matrix, 0)  # Remove negative values\n",
    "    noisy_adj_matrix = (noisy_adj_matrix + noisy_adj_matrix.T) / 2  # Make symmetric\n",
    "    np.fill_diagonal(noisy_adj_matrix, 1)\n",
    "\n",
    "    return noisy_adj_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j5nO9dCaxzSH"
   },
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import StepLR\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train(model, subjects_adj, subjects_labels, test_lr, test_hr, args):\n",
    "    optimizerG = optim.Adam(model.parameters(), lr=args.lr, betas=(0.5, 0.999))\n",
    "    schedulerG = StepLR(optimizerG, step_size=40, gamma=0.8)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    with tqdm(range(args.epochs), desc='Training') as tepoch:\n",
    "        for epoch in tepoch:\n",
    "            epoch_loss = []\n",
    "            epoch_error = []\n",
    "            with torch.autograd.set_detect_anomaly(True):\n",
    "                model.train()\n",
    "                for lr, hr in zip(subjects_adj, subjects_labels):\n",
    "                    optimizerG.zero_grad()\n",
    "\n",
    "                    # augmentation through noise injection\n",
    "                    hr = add_noise_to_adjacency_matrix(hr, 0.1)\n",
    "                    lr = add_noise_to_adjacency_matrix(lr, 0.1)\n",
    "\n",
    "                    hr = pad_HR_adj(hr, args.padding)\n",
    "                    lr = torch.from_numpy(lr).type(torch.FloatTensor).to(device)\n",
    "                    padded_hr = torch.from_numpy(hr).type(torch.FloatTensor).to(device)\n",
    "\n",
    "                    eig_val_hr, U_hr = torch.linalg.eigh(padded_hr, UPLO='U')\n",
    "\n",
    "                    model_outputs, net_outs, start_gcn_outs, layer_outs = model(\n",
    "                        lr, args.lr_dim, args.hr_dim)\n",
    "\n",
    "                    recon_loss = criterion(model_outputs, padded_hr)\n",
    "\n",
    "                    mse_loss = args.lmbda * criterion(net_outs, start_gcn_outs) + criterion(\n",
    "                        model.layer.weights, U_hr) + recon_loss\n",
    "\n",
    "                    mse_loss.backward()\n",
    "                    optimizerG.step()\n",
    "\n",
    "                    epoch_loss.append(mse_loss.item())\n",
    "                    epoch_error.append(recon_loss.item())\n",
    "\n",
    "            schedulerG.step()\n",
    "\n",
    "            if (epoch % 10 == 0) or (epoch == args.epochs - 1):\n",
    "                epoch_test_loss = []\n",
    "                epoch_test_error = []\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    for lr, hr in zip(test_lr, test_hr):\n",
    "                        hr = pad_HR_adj(hr, args.padding)\n",
    "                        lr = torch.from_numpy(lr).type(torch.FloatTensor).to(device)\n",
    "                        padded_hr = torch.from_numpy(hr).type(torch.FloatTensor).to(device)\n",
    "\n",
    "                        eig_val_hr, U_hr = torch.linalg.eigh(padded_hr, UPLO='U')\n",
    "                        model_outputs, net_outs, start_gcn_outs, layer_outs = model(\n",
    "                            lr, args.lr_dim, args.hr_dim)\n",
    "                        recon_loss = criterion(model_outputs, padded_hr)\n",
    "                        mse_loss = args.lmbda * criterion(net_outs, start_gcn_outs) + criterion(\n",
    "                            model.layer.weights, U_hr) + recon_loss\n",
    "\n",
    "                        epoch_test_loss.append(mse_loss.item())\n",
    "                        epoch_test_error.append(recon_loss.item())\n",
    "\n",
    "            tepoch.set_description(f\"Epoch {epoch}\")\n",
    "            tepoch.set_postfix(total_loss=np.mean(epoch_loss),\n",
    "                               mse_error=f'{np.mean(epoch_error) * 100:.2f}%',\n",
    "                               total_test_loss=np.mean(epoch_test_loss),\n",
    "                               mse_test_error=f'{np.mean(epoch_test_error) * 100:.2f}%'\n",
    "                               )\n",
    "            print()\n",
    "\n",
    "\n",
    "def test(model, test_adj, args):\n",
    "\n",
    "    preds_list = []\n",
    "\n",
    "    for lr in test_adj:\n",
    "        all_zeros_lr = not np.any(lr)\n",
    "        if all_zeros_lr == False :\n",
    "            lr = torch.from_numpy(lr).type(torch.FloatTensor).to(device)\n",
    "            preds, a, b, c = model(lr, args.lr_dim, args.hr_dim)\n",
    "            preds = unpad(preds, args.padding).detach().cpu().numpy()\n",
    "            preds_list.append(preds)\n",
    "\n",
    "    return np.stack(preds_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "I0cJVmyNxzXv",
    "outputId": "c386fa8a-9749-4c6b-a714-2ccb3d593aad"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "class Args:\n",
    "    '''\n",
    "    Hyperparameters for training\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.epochs = 91\n",
    "        self.lr = 0.001\n",
    "        self.lmbda = 0.1\n",
    "        self.lr_dim = 160\n",
    "        self.hr_dim = 320\n",
    "        self.hidden_dim = 320\n",
    "        self.padding = 26\n",
    "        self.mean_dense = 0.\n",
    "        self.std_dense = 0.01\n",
    "        self.mean_gaussian = 0.\n",
    "        self.std_gaussian = 0.1\n",
    "\n",
    "class AGRNet:\n",
    "    '''\n",
    "    the AGR model class which is based on the github repo https://github.com/basiralab/AGSR-Net\n",
    "    '''\n",
    "    def __init__(self, args=None):\n",
    "        # If args is None, use default arguments\n",
    "        if args is None:\n",
    "                self.args = Args()\n",
    "        else:\n",
    "                self.args = args\n",
    "\n",
    "        # ks = [0.9, 0.7, 0.6, 0.5]\n",
    "        ks = [0.9, 0.8, 0.7, 0.6, 0.5, 0.3]\n",
    "        self.model = AGSRNet(ks, self.args).to(device)\n",
    "\n",
    "        print(self.model)\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    def train(self, lr_vectors, hr_vectors, lr_test, hr_test):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "                # lr_vectors: N*d e.g.(190, 12720) 12720=(160*159)/2\n",
    "                # hr_vectors: N*d e.g.(190, 35778) 35778=(268*267)/2\n",
    "                (190, 160, 160)\n",
    "                (190, 268, 268)\n",
    "        Output:\n",
    "                for each epoch, prints out the mean training MSE error\n",
    "        \"\"\"\n",
    "\n",
    "        train(self.model, lr_vectors, hr_vectors, lr_test, hr_test, self.args)\n",
    "        model_save_path = './agrnet1.pth'\n",
    "        torch.save(self.model.state_dict(), model_save_path)\n",
    "\n",
    "    def predict(self, lr_vectors):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "                # lr_vectors: N*d e.g.(190, 12720) 12720=(160*159)/2\n",
    "                (190, 160, 160)\n",
    "        Outputs:\n",
    "                predicted_hr_vectors: (190, 268, 268)\n",
    "        \"\"\"\n",
    "        model_load_path = './agrnet1.pth'\n",
    "        model_state_dict = torch.load(model_load_path)\n",
    "        self.model.load_state_dict(model_state_dict)\n",
    "        self.model.eval()\n",
    "\n",
    "        predicted_hr_vectors = test(self.model, lr_vectors, self.args)\n",
    "\n",
    "        return predicted_hr_vectors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NDgs8DRSxz1o"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class MatrixVectorizer:\n",
    "    \"\"\"\n",
    "    A class for transforming between matrices and vector representations.\n",
    "\n",
    "    This class provides methods to convert a symmetric matrix into a vector (vectorize)\n",
    "    and to reconstruct the matrix from its vector form (anti_vectorize), focusing on\n",
    "    vertical (column-based) traversal and handling of elements.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes the MatrixVectorizer instance.\n",
    "\n",
    "        The constructor currently does not perform any actions but is included for\n",
    "        potential future extensions where initialization parameters might be required.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def vectorize(matrix, include_diagonal=False):\n",
    "        \"\"\"\n",
    "        Converts a matrix into a vector by vertically extracting elements.\n",
    "\n",
    "        This method traverses the matrix column by column, collecting elements from the\n",
    "        upper triangle, and optionally includes the diagonal elements immediately below\n",
    "        the main diagonal based on the include_diagonal flag.\n",
    "\n",
    "        Parameters:\n",
    "        - matrix (numpy.ndarray): The matrix to be vectorized.\n",
    "        - include_diagonal (bool, optional): Flag to include diagonal elements in the vectorization.\n",
    "          Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "        - numpy.ndarray: The vectorized form of the matrix.\n",
    "        \"\"\"\n",
    "        # Determine the size of the matrix based on its first dimension\n",
    "        matrix_size = matrix.shape[0]\n",
    "\n",
    "        # Initialize an empty list to accumulate vector elements\n",
    "        vector_elements = []\n",
    "\n",
    "        # Iterate over columns and then rows to collect the relevant elements\n",
    "        for col in range(matrix_size):\n",
    "            for row in range(matrix_size):\n",
    "                # Skip diagonal elements if not including them\n",
    "                if row != col:\n",
    "                    if row < col:\n",
    "                        # Collect upper triangle elements\n",
    "                        vector_elements.append(matrix[row, col])\n",
    "                    elif include_diagonal and row == col + 1:\n",
    "                        # Optionally include the diagonal elements immediately below the diagonal\n",
    "                        vector_elements.append(matrix[row, col])\n",
    "\n",
    "        return np.array(vector_elements)\n",
    "\n",
    "    @staticmethod\n",
    "    def anti_vectorize(vector, matrix_size, include_diagonal=False):\n",
    "        \"\"\"\n",
    "        Reconstructs a matrix from its vector form, filling it vertically.\n",
    "\n",
    "        The method fills the matrix by reflecting vector elements into the upper triangle\n",
    "        and optionally including the diagonal elements based on the include_diagonal flag.\n",
    "\n",
    "        Parameters:\n",
    "        - vector (numpy.ndarray): The vector to be transformed into a matrix.\n",
    "        - matrix_size (int): The size of the square matrix to be reconstructed.\n",
    "        - include_diagonal (bool, optional): Flag to include diagonal elements in the reconstruction.\n",
    "          Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "        - numpy.ndarray: The reconstructed square matrix.\n",
    "        \"\"\"\n",
    "        # Initialize a square matrix of zeros with the specified size\n",
    "        matrix = np.zeros((matrix_size, matrix_size))\n",
    "\n",
    "        # Index to keep track of the current position in the vector\n",
    "        vector_idx = 0\n",
    "\n",
    "        # Fill the matrix by iterating over columns and then rows\n",
    "        for col in range(matrix_size):\n",
    "            for row in range(matrix_size):\n",
    "                # Skip diagonal elements if not including them\n",
    "                if row != col:\n",
    "                    if row < col:\n",
    "                        # Reflect vector elements into the upper triangle and its mirror in the lower triangle\n",
    "                        matrix[row, col] = vector[vector_idx]\n",
    "                        matrix[col, row] = vector[vector_idx]\n",
    "                        vector_idx += 1\n",
    "                    elif include_diagonal and row == col + 1:\n",
    "                        # Optionally fill the diagonal elements after completing each column\n",
    "                        matrix[row, col] = vector[vector_idx]\n",
    "                        matrix[col, row] = vector[vector_idx]\n",
    "                        vector_idx += 1\n",
    "\n",
    "        return matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EKOTwsewQ1DI"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr, structural_similarity as ssim\n",
    "import community.community_louvain as community_louvain\n",
    "import os\n",
    "def calculate_centralities(adj_matrix):\n",
    "    if adj_matrix.shape[0] != adj_matrix.shape[1]:\n",
    "        raise ValueError(f\"Adjacency matrix is not square: shape={adj_matrix.shape}\")\n",
    "    print(f\"Processing adjacency matrix of shape: {adj_matrix.shape}\")\n",
    "\n",
    "    G = nx.from_numpy_array(adj_matrix)\n",
    "    partition = community_louvain.best_partition(G)\n",
    "\n",
    "    # Calculate the participation coefficient with the partition\n",
    "    pc_dict = participation_coefficient(G, partition)\n",
    "\n",
    "    # Calculate averages of centrality measures\n",
    "    pr = nx.pagerank(G, alpha=0.9)\n",
    "    ec = nx.eigenvector_centrality_numpy(G, max_iter=100)\n",
    "    bc = nx.betweenness_centrality(G, normalized=True, endpoints=False)\n",
    "    ns = np.array(list(nx.degree_centrality(G).values())) * (len(G.nodes()) - 1)\n",
    "    acc = nx.average_clustering(G, weight=None)\n",
    "\n",
    "    # Average participation coefficient\n",
    "    pc_avg = np.mean(list(pc_dict.values()))\n",
    "\n",
    "    return {\n",
    "        'pr': np.mean(list(pr.values())),\n",
    "        'ec': np.mean(list(ec.values())),\n",
    "        'bc': np.mean(list(bc.values())),\n",
    "        'ns': ns,\n",
    "        'pc': pc_avg,\n",
    "        'acc': acc\n",
    "    }\n",
    "\n",
    "def participation_coefficient(G, partition):\n",
    "    # Initialize dictionary for participation coefficients\n",
    "    pc_dict = {}\n",
    "\n",
    "    # Calculate participation coefficient for each node\n",
    "    for node in G.nodes():\n",
    "        node_degree = G.degree(node)\n",
    "        if node_degree == 0:\n",
    "            pc_dict[node] = 0.0\n",
    "        else:\n",
    "            # Count within-module connections\n",
    "            within_module_degree = sum(1 for neighbor in G[node] if partition[neighbor] == partition[node])\n",
    "            # Calculate participation coefficient\n",
    "            pc_dict[node] = 1 - (within_module_degree / node_degree) ** 2\n",
    "\n",
    "    return pc_dict\n",
    "\n",
    "def evaluate_all(true_hr_matrices, predicted_hr_matrices, output_path=path_eval_matrics):\n",
    "    print(true_hr_matrices.shape)\n",
    "    print(predicted_hr_matrices.shape)\n",
    "\n",
    "    num_subjects = true_hr_matrices.shape[0]\n",
    "    results = []\n",
    "\n",
    "    for i in range(num_subjects):\n",
    "        true_matrix = true_hr_matrices[i, :, :]\n",
    "        pred_matrix = predicted_hr_matrices[i, :, :]\n",
    "\n",
    "        print(f\"Evaluating subject {i+1} with matrix shapes: true={true_matrix.shape}, pred={pred_matrix.shape}\")\n",
    "\n",
    "        if true_matrix.shape != pred_matrix.shape or true_matrix.shape[0] != true_matrix.shape[1]:\n",
    "            print(f\"Error: Matrix shape mismatch or not square for subject {i+1}: true={true_matrix.shape}, pred={pred_matrix.shape}\")\n",
    "            continue\n",
    "\n",
    "        metrics = {\n",
    "            'ID': i + 1,\n",
    "            'MAE': mean_absolute_error(true_matrix.flatten(), pred_matrix.flatten()),\n",
    "            'PCC': pearsonr(true_matrix.flatten(), pred_matrix.flatten())[0],\n",
    "            'JSD': jensenshannon(true_matrix.flatten(), pred_matrix.flatten()),\n",
    "        }\n",
    "\n",
    "        true_metrics = calculate_centralities(true_matrix)\n",
    "        pred_metrics = calculate_centralities(pred_matrix)\n",
    "\n",
    "        for key in ['NS', 'PR', 'EC', 'BC', 'PC', 'ACC']:\n",
    "            metrics[f'MAE in {key}'] = mean_absolute_error([true_metrics[key.lower()]], [pred_metrics[key.lower()]])\n",
    "\n",
    "        results.append(metrics)\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    if not df.empty:\n",
    "        # Check if the file exists to decide whether to write headers\n",
    "        file_exists = os.path.isfile(output_path)\n",
    "\n",
    "        df.to_csv(output_path, mode='a', header=not file_exists, index=False)\n",
    "        print(f\"Results appended to {output_path}.\")\n",
    "    else:\n",
    "        print(\"No data to save.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "F5xtZ1Vnxz4C",
    "outputId": "70567195-481a-46cd-83bc-55aa08c2a670"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "import networkx as nx\n",
    "import random, torch\n",
    "\n",
    "# Set a fixed random seed for reproducibility across multiple libraries\n",
    "random_seed = 42\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "# Check for CUDA (GPU support) and set device accordingly\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"CUDA is available. Using GPU.\")\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed)  # For multi-GPU setups\n",
    "    # Additional settings for ensuring reproducibility on CUDA\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDA not available. Using CPU.\")\n",
    "\n",
    "def preprocess_data(data_path):\n",
    "    # Load data\n",
    "    data = pd.read_csv(data_path)\n",
    "\n",
    "    # Data cleansing: replace negative and NaN values with 0\n",
    "    data = np.maximum(data, 0)\n",
    "    data = np.nan_to_num(data)\n",
    "\n",
    "    return data\n",
    "\n",
    "def vectorize(data):\n",
    "    # Vectorization (if needed)\n",
    "    vectorizer = MatrixVectorizer()\n",
    "    vectors = vectorizer.vectorize(data)\n",
    "\n",
    "    return vectors\n",
    "\n",
    "def anti_vectorize(data, size, diagonal=False):\n",
    "    # Reverse the vectorization (if needed)\n",
    "    graphs = np.zeros((data.shape[0], size, size))\n",
    "    for idx, graph in enumerate(data):\n",
    "        vectorizer = MatrixVectorizer()\n",
    "        graphs[idx] = vectorizer.anti_vectorize(graph, size, diagonal)\n",
    "\n",
    "    return graphs\n",
    "\n",
    "if option == 1:\n",
    "    lr_data_path = path_lr_data\n",
    "    hr_data_path = path_hr_data\n",
    "\n",
    "    lr_data_path_test = path_lr_data_test\n",
    "    hr_data_path_test = path_hr_data_test\n",
    "\n",
    "if option == 0:\n",
    "    lr_data_path = path_lr_data_tr\n",
    "    hr_data_path = path_hr_data_tr\n",
    "\n",
    "    lr_data_path_test = path_lr_data_valid\n",
    "    hr_data_path_test = path_hr_data_valid\n",
    "\n",
    "# lr_matrix: N * 12720, hr_matrix: N * 35778\n",
    "lr_matrix = preprocess_data(lr_data_path)\n",
    "hr_matrix = preprocess_data(hr_data_path)\n",
    "\n",
    "lr_matrix_test = preprocess_data(lr_data_path_test)\n",
    "hr_matrix_test = preprocess_data(hr_data_path_test)\n",
    "\n",
    "# Define a function to calculate statistics and return them in a dictionary\n",
    "def calculate_statistics(data):\n",
    "    statistics = {\n",
    "        'Mean': np.mean(data),\n",
    "        'Median': np.median(data),\n",
    "        'Standard Deviation': np.std(data),\n",
    "        'Min': np.min(data),\n",
    "        'Max': np.max(data)\n",
    "    }\n",
    "    return statistics\n",
    "\n",
    "# Calculate statistics for LR and HR data\n",
    "lr_stats = calculate_statistics(lr_matrix)\n",
    "hr_stats = calculate_statistics(hr_matrix)\n",
    "\n",
    "# Create a DataFrame to hold the statistics for comparison\n",
    "df_stats = pd.DataFrame({'LR Data': lr_stats, 'HR Data': hr_stats})\n",
    "\n",
    "# Round the numbers to four decimal places for better readability\n",
    "df_stats = df_stats.round(4)\n",
    "\n",
    "def plot_evaluation_metrics(fold_results):\n",
    "    metrics = np.array(fold_results)\n",
    "\n",
    "    # Calculate mean and standard deviation across folds for each metric\n",
    "    metrics_mean = metrics.mean(axis=0)\n",
    "    metrics_std = metrics.std(axis=0)\n",
    "\n",
    "    # Define metric names\n",
    "    metric_names = ['MAE', 'PCC', 'JSD', 'MAE-BC', 'MAE-EC', 'MAE-PR']\n",
    "\n",
    "    # Determine the number of subplot rows and columns (up to 2 plots per line)\n",
    "    n_folds = len(fold_results)\n",
    "    n_rows = 2\n",
    "    n_cols = (n_folds + 1) // n_rows + ((n_folds + 1) % n_rows > 0)\n",
    "\n",
    "    fig, axs = plt.subplots(n_rows, n_cols, figsize=(20, 10), sharey=True)\n",
    "\n",
    "    # Flatten the axs array for easy indexing\n",
    "    axs = axs.flatten()\n",
    "\n",
    "    # Plot each fold's metrics\n",
    "    for i in range(n_folds):\n",
    "        axs[i].bar(metric_names, metrics[i], color='skyblue')\n",
    "        axs[i].set_title(f'Fold {i + 1}')\n",
    "\n",
    "    # Adjust subplot index for the average metrics based on the number of folds\n",
    "    avg_metrics_index = n_folds\n",
    "\n",
    "    # Plot the average metrics with error bars on the next subplot\n",
    "    axs[avg_metrics_index].bar(metric_names, metrics_mean, color='orange', yerr=metrics_std, capsize=5)\n",
    "    axs[avg_metrics_index].set_title('Avg. Across Folds')\n",
    "\n",
    "    # Adding error bars to the average metrics\n",
    "    for i, (mean, std) in enumerate(zip(metrics_mean, metrics_std)):\n",
    "        axs[avg_metrics_index].errorbar(metric_names[i], mean, yerr=std, fmt='k_', ecolor='black', capsize=5)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('evaluation_metrics.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "def create_graph_from_vector(vectorized_matrix, matrix_size):\n",
    "  adjacency_matrix = MatrixVectorizer.anti_vectorize(vectorized_matrix, matrix_size)\n",
    "  G = nx.from_numpy_array(adjacency_matrix)\n",
    "  return G\n",
    "\n",
    "# N * 12720\n",
    "X_tr = np.array(lr_matrix)\n",
    "# N * 35778\n",
    "Y_tr = np.array(hr_matrix)\n",
    "#print(X.shape, y.shape)\n",
    "X_test = np.array(lr_matrix_test)\n",
    "# N * 35778\n",
    "Y_test = np.array(hr_matrix_test)\n",
    "\n",
    "\n",
    "#fold_results = []\n",
    "\n",
    "X_train_tmp, X_test_tmp = X_tr, X_test\n",
    "y_train_tmp, y_test = Y_tr, Y_test\n",
    "\n",
    "# Initialize your GNN model here\n",
    "model = AGRNet()\n",
    "X_train = anti_vectorize(X_train_tmp, 160)\n",
    "X_test = anti_vectorize(X_test_tmp, 160)\n",
    "y_train = anti_vectorize(y_train_tmp, 268)\n",
    "y_test_graph = anti_vectorize(y_test, 268)\n",
    "# ################################################################################\n",
    "# # Train final model using all data\n",
    "# N * 12720\n",
    "#print('Now training using all the data')\n",
    "#X = np.array(lr_matrix)\n",
    "# N * 35778\n",
    "#y = np.array(hr_matrix)\n",
    "#print(X.shape, y.shape)\n",
    "\n",
    "model = AGRNet()\n",
    "#X_train = anti_vectorize(X, 160)\n",
    "#y_train = anti_vectorize(y, 268)\n",
    "\n",
    "model.train(X_train, y_train, X_train, y_train)\n",
    "################################################################################\n",
    "\n",
    "predictions_tmp = model.predict(X_test)\n",
    "\n",
    "metrics = evaluate_all(\n",
    "    y_test_graph, predictions_tmp\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
