{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VlrL5h0B3BAk"
   },
   "source": [
    "# GenGSR-NET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b6vnGFAuWmbj"
   },
   "source": [
    "## Things to change before running the Jupyter Notebook\n",
    "1. Ensure that the following Python packages are installed:\n",
    "    - `numpy`\n",
    "    - `random`\n",
    "    - `torch`\n",
    "    - `pandas`\n",
    "    - `sklearn`\n",
    "    - `seaborn`\n",
    "    - `matplotlib`\n",
    "    - `scipy`\n",
    "    - `networkx`\n",
    "2. In the third cell change the filepath to the directory where the data is stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "import networkx as nx\n",
    "import community as community_louvain\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Ensure correct installation of community-louvain library\n",
    "!pip uninstall -y community\n",
    "!pip uninstall -y python-louvain\n",
    "!pip install python-louvain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KOgP5CphWmbk"
   },
   "source": [
    "## Set the seed for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3439,
     "status": "ok",
     "timestamp": 1719224286212,
     "user": {
      "displayName": "yiying guan",
      "userId": "06690427779299886783"
     },
     "user_tz": -60
    },
    "id": "PglhF4v6Wmbk",
    "outputId": "36a5fbbc-8f94-4ccd-e3b8-84fa771fa36d"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Set a fixed random seed for reproducibility across multiple libraries\n",
    "random_seed = 24\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "# Check for CUDA (GPU support) and set device accordingly\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"CUDA is available. Using GPU.\")\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed)  # For multi-GPU setups\n",
    "    # Additional settings for ensuring reproducibility on CUDA\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDA not available. Using CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hQ12g4lxduJs"
   },
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_3SKJU0jdtG1"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class MatrixVectorizer:\n",
    "    \"\"\"\n",
    "    A class for transforming between matrices and vector representations.\n",
    "\n",
    "    This class provides methods to convert a symmetric matrix into a vector (vectorize)\n",
    "    and to reconstruct the matrix from its vector form (anti_vectorize), focusing on\n",
    "    vertical (column-based) traversal and handling of elements.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes the MatrixVectorizer instance.\n",
    "\n",
    "        The constructor currently does not perform any actions but is included for\n",
    "        potential future extensions where initialization parameters might be required.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def vectorize(matrix, include_diagonal=False):\n",
    "        \"\"\"\n",
    "        Converts a matrix into a vector by vertically extracting elements.\n",
    "\n",
    "        This method traverses the matrix column by column, collecting elements from the\n",
    "        upper triangle, and optionally includes the diagonal elements immediately below\n",
    "        the main diagonal based on the include_diagonal flag.\n",
    "\n",
    "        Parameters:\n",
    "        - matrix (numpy.ndarray): The matrix to be vectorized.\n",
    "        - include_diagonal (bool, optional): Flag to include diagonal elements in the vectorization.\n",
    "          Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "        - numpy.ndarray: The vectorized form of the matrix.\n",
    "        \"\"\"\n",
    "        # Determine the size of the matrix based on its first dimension\n",
    "        matrix_size = matrix.shape[0]\n",
    "\n",
    "        # Initialize an empty list to accumulate vector elements\n",
    "        vector_elements = []\n",
    "\n",
    "        # Iterate over columns and then rows to collect the relevant elements\n",
    "        for col in range(matrix_size):\n",
    "            for row in range(matrix_size):\n",
    "                # Skip diagonal elements if not including them\n",
    "                if row != col:\n",
    "                    if row < col:\n",
    "                        # Collect upper triangle elements\n",
    "                        vector_elements.append(matrix[row, col])\n",
    "                    elif include_diagonal and row == col + 1:\n",
    "                        # Optionally include the diagonal elements immediately below the diagonal\n",
    "                        vector_elements.append(matrix[row, col])\n",
    "\n",
    "        return np.array(vector_elements)\n",
    "\n",
    "    @staticmethod\n",
    "    def anti_vectorize(vector, matrix_size, include_diagonal=False):\n",
    "        \"\"\"\n",
    "        Reconstructs a matrix from its vector form, filling it vertically.\n",
    "\n",
    "        The method fills the matrix by reflecting vector elements into the upper triangle\n",
    "        and optionally including the diagonal elements based on the include_diagonal flag.\n",
    "\n",
    "        Parameters:\n",
    "        - vector (numpy.ndarray): The vector to be transformed into a matrix.\n",
    "        - matrix_size (int): The size of the square matrix to be reconstructed.\n",
    "        - include_diagonal (bool, optional): Flag to include diagonal elements in the reconstruction.\n",
    "          Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "        - numpy.ndarray: The reconstructed square matrix.\n",
    "        \"\"\"\n",
    "        # Initialize a square matrix of zeros with the specified size\n",
    "        matrix = np.zeros((matrix_size, matrix_size))\n",
    "\n",
    "        # Index to keep track of the current position in the vector\n",
    "        vector_idx = 0\n",
    "\n",
    "        # Fill the matrix by iterating over columns and then rows\n",
    "        for col in range(matrix_size):\n",
    "            for row in range(matrix_size):\n",
    "                # Skip diagonal elements if not including them\n",
    "                if row != col:\n",
    "                    if row < col:\n",
    "                        # Reflect vector elements into the upper triangle and its mirror in the lower triangle\n",
    "                        matrix[row, col] = vector[vector_idx]\n",
    "                        matrix[col, row] = vector[vector_idx]\n",
    "                        vector_idx += 1\n",
    "                    elif include_diagonal and row == col + 1:\n",
    "                        # Optionally fill the diagonal elements after completing each column\n",
    "                        matrix[row, col] = vector[vector_idx]\n",
    "                        matrix[col, row] = vector[vector_idx]\n",
    "                        vector_idx += 1\n",
    "\n",
    "        return matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iIr1YgECWmbl"
   },
   "source": [
    "## Load Data\n",
    "Change the file path to the path of the data on your machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 59191,
     "status": "ok",
     "timestamp": 1719224345400,
     "user": {
      "displayName": "yiying guan",
      "userId": "06690427779299886783"
     },
     "user_tz": -60
    },
    "id": "0-uTsDeQxSQT",
    "outputId": "2832c0db-eba9-49b2-8686-0872d822bf94"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "!cp -r /content/drive/MyDrive/DGL_kaggle/Cluster-CV ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fulhcDKpWmbl"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Change the file path to the location of the data\n",
    "\n",
    "data_hr_split_1 = pd.read_csv('Cluster-CV/Fold1/hr_clusterA_modified.csv')\n",
    "data_hr_split_1_np = data_hr_split_1.to_numpy()\n",
    "\n",
    "data_lr_split_1 = pd.read_csv('Cluster-CV/Fold1/lr_clusterA.csv')\n",
    "data_lr_split_1_np = data_lr_split_1.to_numpy()\n",
    "\n",
    "data_hr_split_2 = pd.read_csv('Cluster-CV/Fold2/hr_clusterB_modified.csv')\n",
    "data_hr_split_2_np = data_hr_split_2.to_numpy()\n",
    "\n",
    "data_lr_split_2 = pd.read_csv('Cluster-CV/Fold2/lr_clusterB.csv')\n",
    "data_lr_split_2_np = data_lr_split_2.to_numpy()\n",
    "\n",
    "data_hr_split_3 = pd.read_csv('Cluster-CV/Fold3/hr_clusterC_modified.csv')\n",
    "data_hr_split_3_np = data_hr_split_3.to_numpy()\n",
    "\n",
    "data_lr_split_3 = pd.read_csv('Cluster-CV/Fold3/lr_clusterC.csv')\n",
    "data_lr_split_3_np = data_lr_split_3.to_numpy()\n",
    "\n",
    "test_data = pd.read_csv('Cluster-CV/Fold1/hr_clusterA_modified.csv')\n",
    "test_data_np = test_data.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KvfY5WTE295g"
   },
   "outputs": [],
   "source": [
    "def weight_variable_glorot(output_dim):\n",
    "\n",
    "    input_dim = output_dim\n",
    "    init_range = np.sqrt(6.0 / (input_dim + output_dim))\n",
    "    initial = np.random.uniform(-init_range, init_range, (input_dim, output_dim))\n",
    "\n",
    "    return initial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EC7arxiR295h"
   },
   "outputs": [],
   "source": [
    "def pad_HR_adj(label, split):\n",
    "\n",
    "    label = np.pad(label, ((split, split), (split, split)), mode=\"constant\")\n",
    "    np.fill_diagonal(label, 1)\n",
    "    return torch.from_numpy(label).type(torch.FloatTensor)\n",
    "\n",
    "def normalize_adj_torch(mx):\n",
    "    rowsum = mx.sum(1)\n",
    "    r_inv_sqrt = torch.pow(rowsum, -0.5).flatten()\n",
    "    r_inv_sqrt[torch.isinf(r_inv_sqrt)] = 0.0\n",
    "    r_mat_inv_sqrt = torch.diag(r_inv_sqrt)\n",
    "    mx = torch.matmul(mx, r_mat_inv_sqrt)\n",
    "    mx = torch.transpose(mx, 0, 1)\n",
    "    mx = torch.matmul(mx, r_mat_inv_sqrt)\n",
    "    return mx\n",
    "\n",
    "def random_zero_rows_columns(adj_matrix, fraction=0.1):\n",
    "    \"\"\"\n",
    "    Randomly zero out a fraction of rows and the corresponding columns in an adjacency matrix.\n",
    "\n",
    "    Parameters:\n",
    "    - adj_matrix: PyTorch tensor representing the adjacency matrix.\n",
    "    - fraction: Fraction of rows/columns to zero out.\n",
    "\n",
    "    Returns:\n",
    "    - Modified adjacency matrix with randomly selected rows and columns zeroed out.\n",
    "    \"\"\"\n",
    "    n = adj_matrix.size(0)\n",
    "    num_to_zero = int(n * fraction)\n",
    "    indices = np.random.choice(n, num_to_zero, replace=False)\n",
    "\n",
    "    modified_adj_matrix = adj_matrix.clone()\n",
    "    modified_adj_matrix[indices] = 0\n",
    "    modified_adj_matrix[:, indices] = 0\n",
    "\n",
    "    return modified_adj_matrix\n",
    "\n",
    "def unpad(data, split):\n",
    "\n",
    "    idx_0 = data.shape[0] - split\n",
    "    idx_1 = data.shape[1] - split\n",
    "    train = data[split:idx_0, split:idx_1]\n",
    "    return train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cqxlDzWk295i"
   },
   "outputs": [],
   "source": [
    "class GSRLayer(nn.Module):\n",
    "    def __init__(self, hr_dim):\n",
    "        super(GSRLayer, self).__init__()\n",
    "        self.hr_dim = hr_dim\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # Initialize weights on the correct device\n",
    "        initial_weights = weight_variable_glorot(hr_dim)\n",
    "        self.weights = torch.nn.Parameter(torch.FloatTensor(initial_weights).to(self.device), requires_grad=True)\n",
    "\n",
    "    def forward(self, A, X):\n",
    "        lr = A.to(self.device)\n",
    "        f = X.to(self.device)\n",
    "\n",
    "        eig_val_lr, U_lr = torch.linalg.eigh(lr, UPLO='U')\n",
    "\n",
    "        eye_mat = torch.eye(lr.shape[0], device=self.device)\n",
    "        s_d = torch.cat((eye_mat, eye_mat), 0)[:self.hr_dim]\n",
    "\n",
    "        a = torch.matmul(self.weights, s_d)\n",
    "        b = torch.matmul(a, U_lr.t())\n",
    "        f_d = torch.abs(torch.matmul(b, f))\n",
    "        f_d.fill_diagonal_(1)\n",
    "\n",
    "        adj = normalize_adj_torch(f_d)\n",
    "        X = torch.mm(adj, adj.t())\n",
    "        X = (X + X.t()) / 2\n",
    "        idx = torch.eye(self.hr_dim, dtype=torch.bool, device=self.device)\n",
    "        X[idx] = 1\n",
    "\n",
    "        return adj, torch.abs(X)\n",
    "\n",
    "\n",
    "class GraphConvolution(nn.Module):\n",
    "    def __init__(self, in_features, out_features, dropout=0.5, act=F.relu):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.dropout = dropout\n",
    "        self.act = act\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # Initialize weights on the correct device\n",
    "        self.weight = torch.nn.Parameter(torch.FloatTensor(in_features, out_features).to(self.device))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        torch.nn.init.xavier_uniform_(self.weight)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        input = input.to(self.device)\n",
    "        adj = adj.to(self.device)\n",
    "\n",
    "        support = torch.mm(input, self.weight)\n",
    "        output = torch.mm(adj, support)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SJwTixVFWmbm"
   },
   "outputs": [],
   "source": [
    "class GAT(nn.Module):\n",
    "    \"\"\"\n",
    "    A basic implementation of the GAT layer.\n",
    "\n",
    "    This layer applies an attention mechanism in the graph convolution process,\n",
    "    allowing the model to focus on different parts of the neighborhood\n",
    "    of each node.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features, activation):\n",
    "        super(GAT, self).__init__()\n",
    "        # Initialize the weights, bias, and attention parameters as\n",
    "        # trainable parameters\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        self.bias = nn.Parameter(torch.zeros(out_features))\n",
    "        self.phi = nn.Parameter(torch.FloatTensor(2 * out_features, 1))\n",
    "        self.activation = activation\n",
    "        self.reset_parameters()\n",
    "\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / np.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "\n",
    "        stdv = 1. / np.sqrt(self.phi.size(1))\n",
    "        self.phi.data.uniform_(-stdv, stdv)\n",
    "\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        # Apply linear transformation and add bias\n",
    "        h = torch.mm(input, self.weight) + self.bias\n",
    "\n",
    "\n",
    "        h_expanded = h.unsqueeze(1)\n",
    "        h_repeated = h.unsqueeze(0)\n",
    "\n",
    "        concatenated = torch.cat((h_expanded.repeat(1, adj.size(0), 1),\n",
    "                                  h_repeated.repeat(adj.size(0), 1, 1)), dim=-1)\n",
    "\n",
    "        attention_scores = torch.matmul(concatenated, self.phi).squeeze(-1)  # Shape: (N, N)\n",
    "\n",
    "        mask = adj + torch.eye(adj.size(0), device=adj.device)\n",
    "\n",
    "        S_masked = torch.where(mask > 0, attention_scores, torch.tensor(float('-inf'), device=attention_scores.device))\n",
    "\n",
    "        attention_weights = F.softmax(S_masked, dim=1)\n",
    "\n",
    "        h_prime = torch.mm(attention_weights, h)\n",
    "\n",
    "        if self.activation is not None:\n",
    "            h = self.activation(h_prime)\n",
    "        else:\n",
    "            h = h_prime\n",
    "\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MLfczqqZ295i"
   },
   "outputs": [],
   "source": [
    "class GraphUnpool(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(GraphUnpool, self).__init__()\n",
    "        self.gat = GAT(in_features=in_dim, out_features=out_dim, activation=F.relu)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.gat.to(self.device)\n",
    "\n",
    "    def forward(self, A, X, idx):\n",
    "        new_X = torch.zeros([A.shape[0], X.shape[1]], device=self.device)\n",
    "        new_X[idx] = X\n",
    "        new_X = self.gat(new_X, A)  # Apply GAT after unpooling to the features\n",
    "        return A, new_X\n",
    "\n",
    "class GraphPool(nn.Module):\n",
    "    def __init__(self, k, in_dim, out_dim):\n",
    "        super(GraphPool, self).__init__()\n",
    "        self.k = k\n",
    "        self.gat = GAT(in_features=in_dim, out_features=out_dim, activation=F.relu)\n",
    "        self.proj = nn.Linear(out_dim, 1)  # Note: Using out_dim from GAT output\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.gat.to(self.device)\n",
    "        self.proj.to(self.device)\n",
    "\n",
    "    def forward(self, A, X):\n",
    "        X = X.to(self.device)\n",
    "        X = self.gat(X, A)  # Process X through GAT before pooling\n",
    "        scores = self.proj(X)\n",
    "        scores = torch.squeeze(scores)\n",
    "        scores = self.sigmoid(scores)\n",
    "        num_nodes = A.shape[0]\n",
    "        values, idx = torch.topk(scores, int(self.k * num_nodes))\n",
    "        new_X = X[idx, :]\n",
    "        values = torch.unsqueeze(values, -1)\n",
    "        new_X = torch.mul(new_X, values)\n",
    "        A = A[idx, :]\n",
    "        A = A[:, idx]\n",
    "        return A, new_X, idx\n",
    "\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, activation=F.relu):\n",
    "        super(GCN, self).__init__()\n",
    "        self.gat = GAT(in_features=in_dim, out_features=out_dim, activation=activation)\n",
    "        self.proj = nn.Linear(out_dim, out_dim)\n",
    "        self.drop = nn.Dropout(p=0.5)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, A, X):\n",
    "        X = X.to(self.device)\n",
    "        X = self.drop(X)\n",
    "        X = self.gat(X, A)\n",
    "        X = self.proj(X)\n",
    "        return X\n",
    "\n",
    "\n",
    "class GraphUnet(nn.Module):\n",
    "    def __init__(self, ks, in_dim, out_dim, dim=268):\n",
    "        super(GraphUnet, self).__init__()\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.start_gcn = GCN(in_dim, dim).to(self.device)\n",
    "        self.bottom_gcn = GCN(dim, dim).to(self.device)\n",
    "        self.end_gcn = GCN(2 * dim, out_dim).to(self.device)\n",
    "        self.down_gcns = nn.ModuleList([GCN(dim, dim).to(self.device) for _ in range(len(ks))])\n",
    "        self.up_gcns = nn.ModuleList([GCN(dim, dim).to(self.device) for _ in range(len(ks))])\n",
    "        self.pools = nn.ModuleList([GraphPool(k, dim, dim).to(self.device) for k in ks])\n",
    "        self.unpools = nn.ModuleList([GraphUnpool(dim, dim).to(self.device) for _ in range(len(ks))])\n",
    "        self.l_n = len(ks)\n",
    "\n",
    "    def forward(self, A, X):\n",
    "        A = A.to(self.device)\n",
    "        X = X.to(self.device)\n",
    "        adj_ms = []\n",
    "        indices_list = []\n",
    "        down_outs = []\n",
    "\n",
    "        X = self.start_gcn(A, X)\n",
    "        start_gcn_outs = X\n",
    "        org_X = X\n",
    "\n",
    "        for i in range(self.l_n):\n",
    "            X = self.down_gcns[i](A, X)\n",
    "            adj_ms.append(A)\n",
    "            down_outs.append(X)\n",
    "            A, X, idx = self.pools[i](A, X)\n",
    "            indices_list.append(idx)\n",
    "\n",
    "        X = self.bottom_gcn(A, X)\n",
    "\n",
    "        for i in range(self.l_n):\n",
    "            up_idx = self.l_n - i - 1\n",
    "            A, idx = adj_ms[up_idx], indices_list[up_idx]\n",
    "            A, X = self.unpools[i](A, X, idx)\n",
    "            X = self.up_gcns[i](A, X)\n",
    "            X = X.add(down_outs[up_idx])\n",
    "\n",
    "        X = torch.cat([X, org_X], 1)\n",
    "        X = self.end_gcn(A, X)\n",
    "\n",
    "        return X, start_gcn_outs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b5lNdpwE295i"
   },
   "outputs": [],
   "source": [
    "class GSRNet(nn.Module):\n",
    "    def __init__(self, ks, args):\n",
    "        super(GSRNet, self).__init__()\n",
    "\n",
    "\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.lr_dim = args.lr_dim\n",
    "        self.hr_dim = args.hr_dim\n",
    "        self.hidden_dim = args.hidden_dim\n",
    "\n",
    "\n",
    "        self.layer = GSRLayer(self.hr_dim).to(self.device)\n",
    "        self.net = GraphUnet(ks, self.lr_dim, self.hr_dim).to(self.device)\n",
    "        self.gc1 = GraphConvolution(self.hr_dim, self.hidden_dim, 0, act=F.relu).to(self.device)\n",
    "        self.gc2 = GraphConvolution(self.hidden_dim, self.hr_dim, 0, act=F.relu).to(self.device)\n",
    "\n",
    "    def forward(self, lr):\n",
    "        lr = lr.to(self.device)\n",
    "        I = torch.eye(self.lr_dim, device=self.device)\n",
    "        A = normalize_adj_torch(lr)\n",
    "        A = random_zero_rows_columns(A, fraction=0.4)\n",
    "\n",
    "        self.net_outs, self.start_gcn_outs = self.net(A, I)\n",
    "\n",
    "        self.outputs, self.Z = self.layer(A, self.net_outs)\n",
    "\n",
    "        self.hidden1 = self.gc1(self.Z, self.outputs)\n",
    "        self.hidden2 = self.gc2(self.hidden1, self.outputs)\n",
    "\n",
    "        z = self.hidden2\n",
    "        z = (z + z.transpose(0, 1)) / 2\n",
    "\n",
    "        idx = torch.eye(self.hr_dim, dtype=torch.bool, device=self.device)\n",
    "        z[idx] = 1\n",
    "\n",
    "        return torch.abs(z), self.net_outs, self.start_gcn_outs, self.outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3RHITLhl295i"
   },
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "\n",
    "def train(model, optimizer, subjects_adj, subjects_labels, args, val_adj, val_ground_truth):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    i = 0\n",
    "    all_epochs_loss = []\n",
    "    val_epochs_loss = []\n",
    "    no_epochs = args.epochs\n",
    "\n",
    "    # Early stopping parameters\n",
    "    early_stopping_patience = 10\n",
    "    early_stopping_min_delta = 0.0001\n",
    "    min_training_epochs = 100\n",
    "\n",
    "    # Early stopping variables\n",
    "    best_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    early_stop = False\n",
    "    epoch = 0\n",
    "\n",
    "    while epoch < min_training_epochs or not early_stop:\n",
    "        epoch_loss = []\n",
    "        epoch_error = []\n",
    "\n",
    "        for lr, hr in zip(subjects_adj, subjects_labels):\n",
    "\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            lr = torch.from_numpy(lr).type(torch.FloatTensor).to(device)\n",
    "            hr = torch.from_numpy(hr).type(torch.FloatTensor).to(device)\n",
    "\n",
    "            model_outputs, net_outs, start_gcn_outs, layer_outs = model(lr)\n",
    "            model_outputs = unpad(model_outputs, args.padding)\n",
    "\n",
    "            padded_hr = pad_HR_adj(hr.cpu(), args.padding).to(device)\n",
    "            eig_val_hr, U_hr = torch.linalg.eigh(hr, UPLO='U')\n",
    "\n",
    "\n",
    "            loss = (\n",
    "                args.lmbda * criterion(net_outs, start_gcn_outs)\n",
    "                + criterion(model.layer.weights, U_hr)\n",
    "                + criterion(model_outputs, hr)\n",
    "            )\n",
    "\n",
    "            error = criterion(model_outputs, hr)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss.append(loss.item())\n",
    "            epoch_error.append(error.item())\n",
    "\n",
    "        mean_epoch_loss = np.mean(epoch_loss)\n",
    "        all_epochs_loss.append(np.mean(epoch_loss))\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = []\n",
    "        with torch.no_grad():\n",
    "            for lr, hr in zip(val_adj, val_ground_truth):\n",
    "                lr = torch.from_numpy(lr).type(torch.FloatTensor).to(device)\n",
    "                hr = torch.from_numpy(hr).type(torch.FloatTensor).to(device)\n",
    "\n",
    "                model_outputs, net_outs, start_gcn_outs, layer_outs = model(lr)\n",
    "                model_outputs = unpad(model_outputs, args.padding)\n",
    "\n",
    "                padded_hr = pad_HR_adj(hr.cpu(), args.padding).to(device)\n",
    "                eig_val_hr, U_hr = torch.linalg.eigh(hr, UPLO='U')\n",
    "\n",
    "                loss = (\n",
    "                    args.lmbda * criterion(net_outs, start_gcn_outs)\n",
    "                    + criterion(model.layer.weights, U_hr)\n",
    "                    + criterion(model_outputs, hr)\n",
    "                )\n",
    "\n",
    "                val_loss.append(loss.item())\n",
    "\n",
    "        mean_val_loss = np.mean(val_loss)\n",
    "        val_epochs_loss.append(mean_val_loss)\n",
    "\n",
    "        if mean_val_loss < best_loss - early_stopping_min_delta:\n",
    "            best_loss = mean_val_loss\n",
    "            epochs_no_improve = 0\n",
    "            torch.save(model, f'./Result/model_fold_{fold_num}.pth')\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        if epochs_no_improve >= early_stopping_patience and epoch >= min_training_epochs:\n",
    "            early_stop = True\n",
    "\n",
    "        epoch += 1\n",
    "\n",
    "        i += 1\n",
    "        print(f\"Epoch: {epoch}/{no_epochs}, Training Loss: {mean_epoch_loss}, Error: {np.mean(epoch_error):.4f}, Val Loss: {mean_val_loss}\")\n",
    "\n",
    "    return all_epochs_loss, val_epochs_loss\n",
    "\n",
    "\n",
    "def test(model, test_adj, test_labels, args):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    test_error = []\n",
    "    preds_list = []\n",
    "    g_t = []\n",
    "\n",
    "    i = 0\n",
    "    # TESTING\n",
    "    for lr, hr in zip(test_adj, test_labels):\n",
    "\n",
    "        all_zeros_lr = not np.any(lr)\n",
    "        all_zeros_hr = not np.any(hr)\n",
    "\n",
    "        if (\n",
    "            all_zeros_lr == False and all_zeros_hr == False\n",
    "        ):  # choose representative subject\n",
    "            lr = torch.from_numpy(lr).type(torch.FloatTensor).to(device)\n",
    "            np.fill_diagonal(hr, 1)\n",
    "            hr = torch.from_numpy(hr).type(torch.FloatTensor).to(device)\n",
    "            preds, a, b, c = model(lr)\n",
    "            preds = unpad(preds, args.padding)\n",
    "            preds_flattened = preds.flatten().cpu().detach().numpy()\n",
    "            preds_matrix = MatrixVectorizer.anti_vectorize(preds_flattened, args.hr_dim)\n",
    "            preds_list.append(preds_matrix)\n",
    "\n",
    "            error = criterion(preds, hr)\n",
    "            g_t.append(hr.flatten().cpu().numpy())\n",
    "            #print(error.item())\n",
    "            test_error.append(error.item())\n",
    "\n",
    "            i += 1\n",
    "\n",
    "    print(\"Test error MSE: \", np.mean(test_error))\n",
    "    return np.array(preds_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9LoC6cLS295k"
   },
   "outputs": [],
   "source": [
    "matrix_vec = MatrixVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e51m3VFt295k"
   },
   "outputs": [],
   "source": [
    "def convert_dataset(data, matrix_dim):\n",
    "    num_samples = data.shape[0]\n",
    "    matrices = np.zeros((num_samples, matrix_dim, matrix_dim))\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        vector = data[i]\n",
    "        matrix = matrix_vec.anti_vectorize(vector, matrix_dim)\n",
    "        matrices[i] = matrix\n",
    "\n",
    "    return matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lg2lJM8N295k"
   },
   "outputs": [],
   "source": [
    "data_hr_split_1_reshaped = convert_dataset(data_hr_split_1_np, 268)\n",
    "data_lr_split_1_reshaped = convert_dataset(data_lr_split_1_np, 160)\n",
    "\n",
    "data_hr_split_2_reshaped = convert_dataset(data_hr_split_2_np, 268)\n",
    "data_lr_split_2_reshaped = convert_dataset(data_lr_split_2_np, 160)\n",
    "\n",
    "data_hr_split_3_reshaped = convert_dataset(data_hr_split_3_np, 268)\n",
    "data_lr_split_3_reshaped = convert_dataset(data_lr_split_3_np, 160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n6EjfjDK295k"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def data(data_lr_reshaped, data_hr_reshaped):\n",
    "    # Determine the number of samples for training based on an 80-20 split\n",
    "    num_samples = data_lr_reshaped.shape[0]\n",
    "    train_size = int(num_samples * 0.8)\n",
    "\n",
    "    # Split the data into training and testing datasets\n",
    "    subjects_adj = data_lr_reshaped[:train_size]\n",
    "    subjects_labels = data_hr_reshaped[:train_size]\n",
    "\n",
    "    test_adj = data_lr_reshaped[train_size:]\n",
    "    test_labels = data_hr_reshaped[train_size:]\n",
    "\n",
    "    return subjects_adj, subjects_labels, test_adj, test_labels\n",
    "\n",
    "def data_es(data_lr_reshaped, data_hr_reshaped):\n",
    "    # use the last 20 samples for early stopping\n",
    "    num_samples = data_lr_reshaped.shape[0]\n",
    "    train_size = int(num_samples - 20)\n",
    "\n",
    "    subjects_adj = data_lr_reshaped[:train_size]\n",
    "    subjects_labels = data_hr_reshaped[:train_size]\n",
    "\n",
    "    test_adj = data_lr_reshaped[train_size:]\n",
    "    test_labels = data_hr_reshaped[train_size:]\n",
    "\n",
    "    return subjects_adj, subjects_labels, test_adj, test_labels\n",
    "\n",
    "\n",
    "subjects_adj_split_1, subjects_labels_split_1, test_adj_split_1, test_labels_split_1 = data_es(\n",
    "    data_lr_split_1_reshaped, data_hr_split_1_reshaped\n",
    ")\n",
    "subjects_adj_split_2, subjects_labels_split_2, test_adj_split_2, test_labels_split_2 = data_es(\n",
    "    data_lr_split_2_reshaped, data_hr_split_2_reshaped\n",
    ")\n",
    "subjects_adj_split_3, subjects_labels_split_3, test_adj_split_3, test_labels_split_3 = data_es(\n",
    "    data_lr_split_3_reshaped, data_hr_split_3_reshaped\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yJNRtva2aNZV"
   },
   "outputs": [],
   "source": [
    "def evaluation_metrics(num_test_samples, pred_matrices, gt_matrices):\n",
    "    # post-processing\n",
    "    pred_matrices[pred_matrices < 0] = 0\n",
    "\n",
    "    gt_matrices[gt_matrices < 0] = 0\n",
    "\n",
    "    # Initialize lists to store MAEs for each centrality measure\n",
    "    mae_bc = []\n",
    "    mae_ec = []\n",
    "    mae_pc = []\n",
    "\n",
    "    # Initialize lists to store flattened matrices\n",
    "    pred_1d_list = []\n",
    "    gt_1d_list = []\n",
    "\n",
    "    # Iterate over each test sample\n",
    "    for i in range(num_test_samples):\n",
    "        # Convert adjacency matrices to NetworkX graphs\n",
    "        pred_graph = nx.from_numpy_array(pred_matrices[i], edge_attr=\"weight\")\n",
    "        gt_graph = nx.from_numpy_array(gt_matrices[i], edge_attr=\"weight\")\n",
    "\n",
    "        # Compute centrality measures\n",
    "        pred_bc = nx.betweenness_centrality(pred_graph, weight=\"weight\")\n",
    "        pred_ec = nx.eigenvector_centrality(pred_graph, weight=\"weight\")\n",
    "        pred_pc = nx.pagerank(pred_graph, weight=\"weight\")\n",
    "\n",
    "        gt_bc = nx.betweenness_centrality(gt_graph, weight=\"weight\")\n",
    "        gt_ec = nx.eigenvector_centrality(gt_graph, weight=\"weight\")\n",
    "        gt_pc = nx.pagerank(gt_graph, weight=\"weight\")\n",
    "\n",
    "        # Convert centrality dictionaries to lists\n",
    "        pred_bc_values = list(pred_bc.values())\n",
    "        pred_ec_values = list(pred_ec.values())\n",
    "        pred_pc_values = list(pred_pc.values())\n",
    "\n",
    "        gt_bc_values = list(gt_bc.values())\n",
    "        gt_ec_values = list(gt_ec.values())\n",
    "        gt_pc_values = list(gt_pc.values())\n",
    "\n",
    "        # Compute MAEs\n",
    "        mae_bc.append(mean_absolute_error(pred_bc_values, gt_bc_values))\n",
    "        mae_ec.append(mean_absolute_error(pred_ec_values, gt_ec_values))\n",
    "        mae_pc.append(mean_absolute_error(pred_pc_values, gt_pc_values))\n",
    "\n",
    "        # Vectorize matrices\n",
    "        pred_1d_list.append(MatrixVectorizer.vectorize(pred_matrices[i]))\n",
    "        gt_1d_list.append(MatrixVectorizer.vectorize(gt_matrices[i]))\n",
    "\n",
    "    # Compute average MAEs\n",
    "    avg_mae_bc = sum(mae_bc) / len(mae_bc)\n",
    "    avg_mae_ec = sum(mae_ec) / len(mae_ec)\n",
    "    avg_mae_pc = sum(mae_pc) / len(mae_pc)\n",
    "\n",
    "    # Concatenate flattened matrices\n",
    "    pred_1d = np.concatenate(pred_1d_list)\n",
    "    gt_1d = np.concatenate(gt_1d_list)\n",
    "\n",
    "    # Compute metrics\n",
    "    mae = mean_absolute_error(pred_1d, gt_1d)\n",
    "    pcc = pearsonr(pred_1d, gt_1d)[0]\n",
    "    js_dis = jensenshannon(pred_1d, gt_1d)\n",
    "\n",
    "    print(\"MAE: \", mae)\n",
    "    print(\"PCC: \", pcc)\n",
    "    print(\"Jensen-Shannon Distance: \", js_dis)\n",
    "    print(\"Average MAE betweenness centrality:\", avg_mae_bc)\n",
    "    print(\"Average MAE eigenvector centrality:\", avg_mae_ec)\n",
    "    print(\"Average MAE PageRank centrality:\", avg_mae_pc)\n",
    "    return mae, pcc, js_dis, avg_mae_bc, avg_mae_ec, avg_mae_pc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sBZ5TekzyroN"
   },
   "source": [
    "Kaggle evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "executionInfo": {
     "elapsed": 6734,
     "status": "ok",
     "timestamp": 1719226864147,
     "user": {
      "displayName": "yiying guan",
      "userId": "06690427779299886783"
     },
     "user_tz": -60
    },
    "id": "29ZWDG_oyq5A",
    "outputId": "d07ad0e1-1746-4c47-b406-20af87aac8f4"
   },
   "outputs": [],
   "source": [
    "def calculate_centralities(adj_matrix):\n",
    "    if adj_matrix.shape[0] != adj_matrix.shape[1]:\n",
    "        raise ValueError(f\"Adjacency matrix is not square: shape={adj_matrix.shape}\")\n",
    "    print(f\"Processing adjacency matrix of shape: {adj_matrix.shape}\")\n",
    "\n",
    "    G = nx.from_numpy_array(adj_matrix)\n",
    "    partition = community_louvain.best_partition(G)\n",
    "\n",
    "    # Calculate the participation coefficient with the partition\n",
    "    pc_dict = participation_coefficient(G, partition)\n",
    "\n",
    "    # Calculate averages of centrality measures\n",
    "    pr = nx.pagerank(G, alpha=0.9)\n",
    "    ec = nx.eigenvector_centrality_numpy(G, max_iter=100)\n",
    "    bc = nx.betweenness_centrality(G, normalized=True, endpoints=False)\n",
    "    ns = np.array(list(nx.degree_centrality(G).values())) * (len(G.nodes()) - 1)\n",
    "    acc = nx.average_clustering(G, weight=None)\n",
    "\n",
    "    # Average participation coefficient\n",
    "    pc_avg = np.mean(list(pc_dict.values()))\n",
    "\n",
    "    return {\n",
    "        'pr': np.mean(list(pr.values())),\n",
    "        'ec': np.mean(list(ec.values())),\n",
    "        'bc': np.mean(list(bc.values())),\n",
    "        'ns': ns,\n",
    "        'pc': pc_avg,\n",
    "        'acc': acc\n",
    "    }\n",
    "\n",
    "def participation_coefficient(G, partition):\n",
    "    # Initialize dictionary for participation coefficients\n",
    "    pc_dict = {}\n",
    "\n",
    "    # Calculate participation coefficient for each node\n",
    "    for node in G.nodes():\n",
    "        node_degree = G.degree(node)\n",
    "        if node_degree == 0:\n",
    "            pc_dict[node] = 0.0\n",
    "        else:\n",
    "            # Count within-module connections\n",
    "            within_module_degree = sum(1 for neighbor in G[node] if partition[neighbor] == partition[node])\n",
    "            # Calculate participation coefficient\n",
    "            pc_dict[node] = 1 - (within_module_degree / node_degree) ** 2\n",
    "\n",
    "    return pc_dict\n",
    "\n",
    "\n",
    "def evaluate_all(true_hr_matrices, predicted_hr_matrices, output_path='11-randomCV.csv'):\n",
    "    print(true_hr_matrices.shape)\n",
    "    print(predicted_hr_matrices.shape)\n",
    "\n",
    "    num_subjects = true_hr_matrices.shape[0]\n",
    "    results = []\n",
    "\n",
    "    for i in range(num_subjects):\n",
    "        true_matrix = true_hr_matrices[i, :, :]\n",
    "        pred_matrix = predicted_hr_matrices[i, :, :]\n",
    "\n",
    "        print(f\"Evaluating subject {i+1} with matrix shapes: true={true_matrix.shape}, pred={pred_matrix.shape}\")\n",
    "\n",
    "        if true_matrix.shape != pred_matrix.shape or true_matrix.shape[0] != true_matrix.shape[1]:\n",
    "            print(f\"Error: Matrix shape mismatch or not square for subject {i+1}: true={true_matrix.shape}, pred={pred_matrix.shape}\")\n",
    "            continue\n",
    "\n",
    "        metrics = {\n",
    "            'ID': i + 1,\n",
    "            'MAE': mean_absolute_error(true_matrix.flatten(), pred_matrix.flatten()),\n",
    "            'PCC': pearsonr(true_matrix.flatten(), pred_matrix.flatten())[0],\n",
    "            'JSD': jensenshannon(true_matrix.flatten(), pred_matrix.flatten()),\n",
    "        }\n",
    "\n",
    "        true_metrics = calculate_centralities(true_matrix)\n",
    "        pred_metrics = calculate_centralities(pred_matrix)\n",
    "\n",
    "        for key in ['NS', 'PR', 'EC', 'BC', 'PC', 'ACC']:\n",
    "            metrics[f'MAE in {key}'] = mean_absolute_error([true_metrics[key.lower()]], [pred_metrics[key.lower()]])\n",
    "\n",
    "        results.append(metrics)\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    if not df.empty:\n",
    "        # Check if the file exists to decide whether to write headers\n",
    "        file_exists = os.path.isfile(output_path)\n",
    "\n",
    "        df.to_csv(output_path, mode='a', header=not file_exists, index=False)\n",
    "        print(f\"Results appended to {output_path}.\")\n",
    "    else:\n",
    "        print(\"No data to save.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "PCnSyaBp295l"
   },
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self, dictionary):\n",
    "        for key, value in dictionary.items():\n",
    "            setattr(self, key, value)\n",
    "\n",
    "args_dict = {\n",
    "    \"padding\": 0,\n",
    "    \"hidden_dim\": 350,\n",
    "    \"hr_dim\": 268,\n",
    "    \"lr_dim\": 160,\n",
    "    \"lmbda\": 16,\n",
    "    \"splits\": 3,\n",
    "    \"lr\": 1e-4,\n",
    "    \"epochs\": 200\n",
    "}\n",
    "\n",
    "args = Args(args_dict)\n",
    "\n",
    "if not os.path.exists('./Result'):\n",
    "    os.mkdir('./Result/')\n",
    "\n",
    "if not os.path.exists('./images'):\n",
    "    os.mkdir('./images/')\n",
    "\n",
    "splits = [\n",
    "    (0, 1, 2),\n",
    "    (0, 2, 1),\n",
    "    (1, 2, 0)\n",
    "]\n",
    "\n",
    "ks = [0.9, 0.7, 0.6, 0.5]\n",
    "fold_num = 0\n",
    "\n",
    "for train_indices in splits:\n",
    "    model = GSRNet(ks, args)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "\n",
    "    fold_num += 1\n",
    "    print(\"=====================================================\")\n",
    "    print(f\"Fold {fold_num}/3 started.\")\n",
    "\n",
    "    if fold_num == 1:\n",
    "      subjects_adj, test_adj, subjects_ground_truth, test_ground_truth, val_adj, val_ground_truth = (\n",
    "          np.concatenate((subjects_adj_split_1, subjects_adj_split_2), axis=0),\n",
    "          data_lr_split_3_reshaped,\n",
    "          np.concatenate((subjects_labels_split_1, subjects_labels_split_2), axis=0),\n",
    "          data_hr_split_3_reshaped,\n",
    "          np.concatenate((test_adj_split_1, test_adj_split_2), axis=0),\n",
    "          np.concatenate((test_labels_split_1, test_labels_split_2), axis=0),\n",
    "      )\n",
    "    elif fold_num == 2:\n",
    "      subjects_adj, test_adj, subjects_ground_truth, test_ground_truth, val_adj, val_ground_truth = (\n",
    "          np.concatenate((subjects_adj_split_1, subjects_adj_split_3), axis=0),\n",
    "          data_lr_split_2_reshaped,\n",
    "          np.concatenate((subjects_labels_split_1, subjects_labels_split_3), axis=0),\n",
    "          data_hr_split_2_reshaped,\n",
    "          np.concatenate((test_adj_split_1, test_adj_split_3), axis=0),\n",
    "          np.concatenate((test_labels_split_1, test_labels_split_3), axis=0),\n",
    "      )\n",
    "    else:\n",
    "      subjects_adj, test_adj, subjects_ground_truth, test_ground_truth, val_adj, val_ground_truth = (\n",
    "          np.concatenate((subjects_adj_split_2, subjects_adj_split_3), axis=0),\n",
    "          data_lr_split_1_reshaped,\n",
    "          np.concatenate((subjects_labels_split_2, subjects_labels_split_3), axis=0),\n",
    "          data_hr_split_1_reshaped,\n",
    "          np.concatenate((test_adj_split_2, test_adj_split_3), axis=0),\n",
    "          np.concatenate((test_labels_split_2, test_labels_split_3), axis=0),\n",
    "      )\n",
    "\n",
    "    epochs_loss, val_epochs_loss = train(model, optimizer, subjects_adj, subjects_ground_truth, args, val_adj, val_ground_truth)\n",
    "    preds = test(model, test_adj, test_ground_truth, args)\n",
    "    print(\"Evaluating...\")\n",
    "    mae, pcc, js_dis, avg_mae_bc, avg_mae_ec, avg_mae_pc = evaluation_metrics(len(test_ground_truth), preds, test_ground_truth)\n",
    "\n",
    "    epoch_loss_df = pd.DataFrame({'epoch': list(range(1, len(epochs_loss) + 1)), 'loss': epochs_loss})\n",
    "    epoch_loss_df.to_csv(f'11_fold_{fold_num}_loss_curve.csv', index=False)\n",
    "\n",
    "    val_epoch_loss_df = pd.DataFrame({'epoch': list(range(1, len(val_epochs_loss) + 1)), 'loss': val_epochs_loss})\n",
    "    val_epoch_loss_df.to_csv(f'11_fold_{fold_num}_eval_loss_curve.csv', index=False)\n",
    "\n",
    "    # Plot training and validation loss for the current fold\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(epoch_loss_df['epoch'], epoch_loss_df['loss'], label='Training Loss')\n",
    "    plt.plot(val_epoch_loss_df['epoch'], val_epoch_loss_df['loss'], label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f'Loss Curves for Fold {fold_num}')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'images/loss_curve_fold_{fold_num}.png')\n",
    "    plt.close()\n",
    "\n",
    "    # Plot metrics for the current fold\n",
    "    metric_names = ['MAE', 'PCC', 'JS Distance', 'Avg MAE BC', 'Avg MAE EC', 'Avg MAE PC']\n",
    "    values = [mae, pcc, js_dis, avg_mae_bc, avg_mae_ec, avg_mae_pc]\n",
    "    plot_data = pd.DataFrame({\n",
    "    'Metric': metric_names,\n",
    "    'Value': values\n",
    "    })\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x='Metric', y='Value', data=plot_data, palette='viridis')\n",
    "    plt.title(f'Metrics for Fold {fold_num}')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"images/bar_plot_fold_{fold_num}.png\")\n",
    "    plt.show()\n",
    "    print(f\"Fold {fold_num} bar plot saved!\")\n",
    "\n",
    "    evaluate_all(test_ground_truth, preds, output_path=f'11_fold_{fold_num}_evaluation.csv')\n",
    "\n",
    "    with open(f'predictions_fold_{fold_num}.csv', 'w') as submission_file:\n",
    "        submission_file.write('ID,Predicted\\n')\n",
    "        model.eval().to(device)\n",
    "        for i in range(test_data_np.shape[0]):\n",
    "            vector = test_data_np[i]\n",
    "            matrix = matrix_vec.anti_vectorize(vector, 160)\n",
    "            matrix = torch.from_numpy(matrix).type(torch.FloatTensor).to(device)\n",
    "            preds, a, b, c = model(matrix)\n",
    "            preds = unpad(preds, args.padding)\n",
    "            preds = preds.cpu().detach().numpy()\n",
    "            vector = matrix_vec.vectorize(preds)\n",
    "            for j, val in enumerate(vector, start=1):\n",
    "                # Set val to 0 if it is NaN or negative\n",
    "                val = 0 if np.isnan(val) or val < 0 else val\n",
    "                submission_file.write(f'{i * len(vector) + j},{val}\\n')\n",
    "    print(f\"Fold {fold_num} predictions written to file.\")\n",
    "    print(f\"Fold {fold_num}/3 completed.\")\n",
    "    print(\"=====================================================\")\n",
    "\n",
    "print(\"All folds completed.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
