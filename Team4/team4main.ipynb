{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "oGIxoKm78xPR"
   },
   "outputs": [],
   "source": [
    "import colorsys\n",
    "import matplotlib.colors as mc\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from karateclub.estimator import Estimator\n",
    "from karateclub.utils.walker import BiasedRandomWalker\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "from tqdm.notebook import tqdm\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "On_NsFrwtFBe"
   },
   "source": [
    "## Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Elul1_sD8xPZ"
   },
   "outputs": [],
   "source": [
    "# Set a fixed random seed for reproducibility across multiple libraries\n",
    "random_seed = 42\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "# Check for CUDA (GPU support) and set device accordingly\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"CUDA is available. Using GPU.\")\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDA not available. Using CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gPc1NktytFBf"
   },
   "source": [
    "## Define Node2Vec class for Node Embedding Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "kbxhcMEIkwBE"
   },
   "outputs": [],
   "source": [
    "class Node2Vec(Estimator):\n",
    "    \"\"\"Implementation of \"Node2Vec\" <https://cs.stanford.edu/~jure/pubs/node2vec-kdd16.pdf> from the\n",
    "       KDD '16 paper \"node2vec: Scalable Feature Learning for Networks\". The procedure uses biased\n",
    "       second order random walks to approximate the pointwise mutual information matrix obtained\n",
    "       by pooling normalized adjacency matrix powers.\n",
    "\n",
    "    Args:\n",
    "        walk_number (int): Number of random walks.\n",
    "        walk_length (int): Length of random walks.\n",
    "        p (float): Return parameter (1/p transition probability) to move towards from previous node.\n",
    "        q (float): In-out parameter (1/q transition probability) to move away from previous node.\n",
    "        dimensions (int): Dimensionality of embedding.\n",
    "        workers (int): Number of cores.\n",
    "        window_size (int): Matrix power order.\n",
    "        epochs (int): Number of epochs.\n",
    "        learning_rate (float): Learning rate.\n",
    "        min_count (int): Minimal count of node occurrences.\n",
    "        seed (int): Random seed value.\n",
    "    \"\"\"\n",
    "    _embedding: List[np.ndarray]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        walk_number: int = 2,\n",
    "        walk_length: int = 10,\n",
    "        p: float = 0.8,\n",
    "        q: float = 1.5,\n",
    "        dimensions: int = 268,\n",
    "        workers: int = 4,\n",
    "        window_size: int = 5,\n",
    "        epochs: int = 1,\n",
    "        learning_rate: float = 0.05,\n",
    "        min_count: int = 1,\n",
    "        seed: int = random_seed,\n",
    "    ):\n",
    "        super(Node2Vec, self).__init__()\n",
    "\n",
    "        self.walk_number = walk_number\n",
    "        self.walk_length = walk_length\n",
    "        self.p = p\n",
    "        self.q = q\n",
    "        self.dimensions = dimensions\n",
    "        self.workers = workers\n",
    "        self.window_size = window_size\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.min_count = min_count\n",
    "        self.seed = seed\n",
    "\n",
    "    def fit(self, graph):\n",
    "            self._set_seed()\n",
    "            # Checking if input graph is in expected format\n",
    "            graph = self._check_graph(graph)\n",
    "            # Initialize random walker\n",
    "            walker = BiasedRandomWalker(self.walk_length, self.walk_number, self.p, self.q)\n",
    "            # Perform random walks on G\n",
    "            walker.do_walks(graph)\n",
    "\n",
    "            # Train a Word2Vec model on the random walks\n",
    "            model = Word2Vec(\n",
    "                walker.walks,\n",
    "                hs=1,\n",
    "                alpha=self.learning_rate,\n",
    "                epochs=self.epochs,\n",
    "                vector_size=self.dimensions,\n",
    "                window=self.window_size,\n",
    "                min_count=self.min_count,\n",
    "                workers=self.workers,\n",
    "                seed=self.seed,\n",
    "                negative=0\n",
    "            )\n",
    "\n",
    "            # Retrieve node embeddings from the trained model\n",
    "            n_nodes = graph.number_of_nodes()\n",
    "            self._embedding = [model.wv[str(n)] for n in range(n_nodes)]\n",
    "\n",
    "\n",
    "    def get_embedding(self) -> np.array:\n",
    "        return np.array(self._embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "80TIUXSAtFBg"
   },
   "source": [
    "## Define GSR (Graph Super Resolution) layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "9K-AMw7ItFBg"
   },
   "outputs": [],
   "source": [
    "# Helper method for weight initialization in GSRLayer\n",
    "def weight_variable_glorot(output_dim):\n",
    "    input_dim = output_dim\n",
    "    init_range = np.sqrt(6.0 / (input_dim + output_dim))\n",
    "\n",
    "     # Sample from uniform distribution within Glorot's range\n",
    "    initial = np.random.uniform(-init_range, init_range,\n",
    "                                (input_dim, output_dim))\n",
    "\n",
    "    return initial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Ff2zGMddtFBg"
   },
   "outputs": [],
   "source": [
    "class GSRLayer(nn.Module):\n",
    "    def __init__(self, hr_dim):\n",
    "        super(GSRLayer, self).__init__()\n",
    "\n",
    "        self.weights = torch.from_numpy(\n",
    "            weight_variable_glorot(hr_dim)).type(torch.FloatTensor).to(device)\n",
    "        self.weights = torch.nn.Parameter(\n",
    "            data=self.weights, requires_grad=True)\n",
    "\n",
    "    def forward(self, A_l, Z_l):\n",
    "        \"\"\"\n",
    "        Takes in low resolution graph's connectivity A_l and node embeddings Z_l found from U-Net\n",
    "        and returns high resolutioni connectivity A_h and node embeddings X_h.\n",
    "        \"\"\"\n",
    "        lr_dim = A_l.shape[0]\n",
    "        hr_dim = Z_l.shape[1]\n",
    "\n",
    "        _, U_l = torch.linalg.eigh(A_l, UPLO='U') # Compute the eigenvectors of A_l\n",
    "\n",
    "        I = torch.eye(lr_dim, lr_dim).to(device) # Identity matrix\n",
    "        S_d = torch.cat((I, I), dim=0) # Concatenation of identity matrices for upsampling\n",
    "        S_d = S_d[:hr_dim] # Only keep hr_dim rows of S_d\n",
    "\n",
    "        # Super-resolution of adjacency matrix\n",
    "        A_h = torch.matmul(torch.matmul(torch.matmul(self.weights, S_d), torch.t(U_l)), Z_l) # Apply propagation rule A_h = W * S_d * U_l * Z_l\n",
    "        A_h = torch.abs(A_h)\n",
    "        A_h = A_h.fill_diagonal_(1) # Add self-loops to the high resolution graph\n",
    "\n",
    "        # Super-resolution of node embeddings\n",
    "        X_h = torch.matmul(A_h, torch.t(A_h))\n",
    "        X_h = (X_h + torch.t(X_h)) / 2\n",
    "        X_h = X_h.fill_diagonal_(1)\n",
    "        X_h = torch.abs(X_h)\n",
    "\n",
    "        return A_h, X_h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xrb50OwL8xPb"
   },
   "source": [
    "## Define the GCN (Graph Convolutional Network) layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "BrEF-wWF8xPe"
   },
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple GCN layer, similar to implementation done in Kipf et al.'s paper\n",
    "    \"Semi-Supervised Classification with Graph Convolutional Networks\"\n",
    "    (https://arxiv.org/abs/1609.02907)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features, dropout, act=F.relu):\n",
    "        super(GCN, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.dropout = dropout\n",
    "        self.act = act\n",
    "        self.weight = torch.nn.Parameter(\n",
    "            torch.FloatTensor(in_features, out_features))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "         # Apply Xavier uniform initialization to the weights\n",
    "        init.xavier_uniform_(self.weight)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        # Apply dropout to input features\n",
    "        input = F.dropout(input, self.dropout, self.training)\n",
    "        # Message update rule: Hk+1 = act(A*Hk*W)\n",
    "        support = torch.mm(input, self.weight)\n",
    "        output = torch.mm(adj, support)\n",
    "        output = self.act(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jgiuBOl78xPi"
   },
   "source": [
    "## Define the AGSR-Vec (Adversarial Graph Super Resolution-Vec) model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PwnIVlyx8xPj"
   },
   "source": [
    "#### Helper function(s) for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "6MBwCf-m8xPk"
   },
   "outputs": [],
   "source": [
    "def normalize_adj_torch(A):\n",
    "    # Calculate inverse square root of each node's degree\n",
    "    r_inv_sqrt = torch.pow(A.sum(1), -0.5).flatten()\n",
    "    # Mask to zero to avoid division by zero issues\n",
    "    r_inv_sqrt[torch.isinf(r_inv_sqrt)] = 0.\n",
    "    r_mat_inv_sqrt = torch.diag(r_inv_sqrt)\n",
    "\n",
    "    # Apply normalization steps on the adjacency matrix A.\n",
    "    A = torch.matmul(A, r_mat_inv_sqrt)\n",
    "    A = torch.transpose(A, 0, 1)\n",
    "    A = torch.matmul(A, r_mat_inv_sqrt)\n",
    "    return A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wKuCZy4atFBh"
   },
   "source": [
    "### Define the AGSR-Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "rgRHC-VI8xPl"
   },
   "outputs": [],
   "source": [
    "\n",
    "class AGSRVec(nn.Module):\n",
    "    '''\n",
    "    AGSRVec model for super-resolving low-resolution (LR) input data with an upsampling layer and graph convolutional networks (GCNs).\n",
    "\n",
    "    Args:\n",
    "        hr_dim (int): Dimensionality of the high-resolution (HR) embeddings.\n",
    "        hidden_dim (int): Dimensionality of the hidden layers in GCNs.\n",
    "\n",
    "    Attributes:\n",
    "        gsr_layer (GSRLayer): Upsampling layer.\n",
    "        gcn_1 (GCN): First graph convolutional network.\n",
    "        gcn_2 (GCN): Second graph convolutional network.\n",
    "\n",
    "    For detailed explanation refer to README.md.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, hr_dim, hidden_dim):\n",
    "        super(AGSRVec, self).__init__()\n",
    "        self.gsr_layer = GSRLayer(hr_dim)\n",
    "        self.gcn_1 = GCN(hr_dim, hidden_dim, 0, act=F.relu)\n",
    "        self.gcn_2 = GCN(hidden_dim, hr_dim, 0, act=F.relu)\n",
    "\n",
    "    def forward(self, A_l, X_initial):\n",
    "        '''\n",
    "        Forward pass performing the following steps:\n",
    "            (1) Upscaling initial LR node embeddings\n",
    "            (2) Passing HR node embeddings through GCN_1\n",
    "            (3) Passing HR hidden_dim embeddings through GCN_2\n",
    "            (4) Symmetrizing output by averaging with transpose and addding self-connections\n",
    "\n",
    "        Args:\n",
    "            A_l: LR Input adj matrix\n",
    "            X_initial: Initial LR node embeddings (from Node2Vec)\n",
    "        '''\n",
    "        # X_initial.shape = (lr_dim, hr_dim), A_l.shape = (lr_dim, lr_dim)\n",
    "        A_l = normalize_adj_torch(A_l)\n",
    "        A_h, Z_h = self.gsr_layer(A_l, X_initial)\n",
    "\n",
    "        # Refine HR embeddings, Z_h through GCN Layer(s)\n",
    "        # Z_h.shape = A_h.shape = (hr_dim, hr_dim)\n",
    "        Z_h = self.gcn_1(Z_h, A_h)\n",
    "        Z_h = self.gcn_2(Z_h, A_h)\n",
    "\n",
    "        # Symmetrize the output by averaging with its transpose and ensure self-connections\n",
    "        Z_h = (Z_h + torch.t(Z_h)) / 2\n",
    "        Z_h = Z_h.fill_diagonal_(1)\n",
    "        return torch.abs(Z_h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cfixF9298xPm"
   },
   "source": [
    "## Define the discriminator model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "PLcjt1Ts8xPn"
   },
   "outputs": [],
   "source": [
    "class Dense(nn.Module):\n",
    "    '''\n",
    "    Vanilla densely-connected NN layer.\n",
    "\n",
    "    Args:\n",
    "        n1 (int): Number of input features.\n",
    "        n2 (int): Number of output features.\n",
    "        mean_dense (float): Mean of the normal distribution for weight initialization.\n",
    "        std_dense (float): Standard deviation of the normal distribution for weight initialization.\n",
    "    '''\n",
    "    def __init__(self, n1, n2, mean_dense, std_dense):\n",
    "        super(Dense, self).__init__()\n",
    "        self.weights = torch.nn.Parameter(\n",
    "            torch.FloatTensor(n1, n2), requires_grad=True)\n",
    "        init.normal_(self.weights, mean=mean_dense, std=std_dense)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.mm(x, self.weights)\n",
    "        return out\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    '''\n",
    "    Discriminator model used as part of an adversarial model to distinguish between whether a\n",
    "    high-resolution connectome is from a prior ground-truth high-resolution distribution or generated.\n",
    "\n",
    "    Args:\n",
    "        hr_dim (int): Dimensionality of the input features.\n",
    "        mean_dense (float): Mean of the normal distribution for weight initialization.\n",
    "        std_dense (float): Standard deviation of the normal distribution for weight initialization.\n",
    "    '''\n",
    "    def __init__(self, hr_dim, mean_dense, std_dense):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.dense_1 = Dense(hr_dim, hr_dim, mean_dense, std_dense)\n",
    "        self.relu_1 = nn.ReLU(inplace=False)\n",
    "\n",
    "        self.dense_2 = Dense(hr_dim, hr_dim, mean_dense, std_dense)\n",
    "        self.relu_2 = nn.ReLU(inplace=False)\n",
    "\n",
    "        self.dense_3 = Dense(hr_dim, 1, mean_dense, std_dense)\n",
    "        self.sigmoid = nn.Sigmoid() # Apply\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        dc_den1 = self.relu_1(self.dense_1(inputs))\n",
    "        dc_den2 = self.relu_2(self.dense_2(dc_den1))\n",
    "        output = self.sigmoid(self.dense_3(dc_den2))\n",
    "        return torch.abs(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pUt_sYah8xPn"
   },
   "source": [
    "## Matrix Vectorising and Anti-Vectorising functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "lx7DFqVU8xPn"
   },
   "outputs": [],
   "source": [
    "class MatrixVectorizer:\n",
    "    \"\"\"\n",
    "    A class for transforming between matrices and vector representations.\n",
    "\n",
    "    Provides methods to convert a symmetric matrix into a vector (vectorize)\n",
    "    and to reconstruct the matrix from its vector form (anti_vectorize), focusing on\n",
    "    vertical (column-based) traversal and handling of elements.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def vectorize(matrix, include_diagonal=False):\n",
    "        \"\"\"\n",
    "        Converts a matrix into a vector by vertically extracting elements.\n",
    "\n",
    "        This method traverses the matrix column by column, collecting elements from the\n",
    "        upper triangle, and optionally includes the diagonal elements immediately below\n",
    "        the main diagonal based on the include_diagonal flag.\n",
    "\n",
    "        Arguments:\n",
    "        - matrix (numpy.ndarray): The matrix to be vectorized.\n",
    "        - include_diagonal (bool, optional): Flag to include diagonal elements in the vectorization.\n",
    "          Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "        - numpy.ndarray: The vectorized form of the matrix.\n",
    "        \"\"\"\n",
    "        # Determine the size of the matrix based on its first dimension\n",
    "        matrix_size = matrix.shape[0]\n",
    "\n",
    "        # Initialize an empty list to accumulate vector elements\n",
    "        vector_elements = []\n",
    "\n",
    "        # Iterate over columns and then rows to collect the relevant elements\n",
    "        for col in range(matrix_size):\n",
    "            for row in range(matrix_size):\n",
    "                # Skip diagonal elements if not including them\n",
    "                if row != col:\n",
    "                    if row < col:\n",
    "                        # Collect upper triangle elements\n",
    "                        vector_elements.append(matrix[row, col])\n",
    "                    elif include_diagonal and row == col + 1:\n",
    "                        # Optionally include the diagonal elements immediately below the diagonal\n",
    "                        vector_elements.append(matrix[row, col])\n",
    "\n",
    "        return np.array(vector_elements)\n",
    "\n",
    "    @staticmethod\n",
    "    def anti_vectorize(vector, matrix_size, include_diagonal=False):\n",
    "        \"\"\"\n",
    "        Reconstructs a matrix from its vector form, filling it vertically.\n",
    "\n",
    "        The method fills the matrix by reflecting vector elements into the upper triangle\n",
    "        and optionally including the diagonal elements based on the include_diagonal flag.\n",
    "\n",
    "        Arguments:\n",
    "        - vector (numpy.ndarray): The vector to be transformed into a matrix.\n",
    "        - matrix_size (int): The size of the square matrix to be reconstructed.\n",
    "        - include_diagonal (bool, optional): Flag to include diagonal elements in the reconstruction.\n",
    "          Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "        - numpy.ndarray: The reconstructed square matrix.\n",
    "        \"\"\"\n",
    "        # Initialize a square matrix of zeros with the specified size\n",
    "        matrix = np.zeros((matrix_size, matrix_size))\n",
    "\n",
    "        # Index to keep track of the current position in the vector\n",
    "        vector_idx = 0\n",
    "\n",
    "        # Fill the matrix by iterating over columns and then rows\n",
    "        for col in range(matrix_size):\n",
    "            for row in range(matrix_size):\n",
    "                # Skip diagonal elements if not including them\n",
    "                if row != col:\n",
    "                    if row < col:\n",
    "                        # Reflect vector elements into the upper triangle and its mirror in the lower triangle\n",
    "                        matrix[row, col] = vector[vector_idx]\n",
    "                        matrix[col, row] = vector[vector_idx]\n",
    "                        vector_idx += 1\n",
    "                    elif include_diagonal and row == col + 1:\n",
    "                        # Optionally fill the diagonal elements after completing each column\n",
    "                        matrix[row, col] = vector[vector_idx]\n",
    "                        matrix[col, row] = vector[vector_idx]\n",
    "                        vector_idx += 1\n",
    "\n",
    "        return matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u5B9CW7H8xPo"
   },
   "source": [
    "## Define Train, Predict and Test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "-AunUIcs8xPo"
   },
   "outputs": [],
   "source": [
    "def gaussian_noise_layer(input_layer, mean_gaus, std_gaus):\n",
    "    '''\n",
    "    Adds Gaussian noise to the input layer.\n",
    "    '''\n",
    "    z = torch.empty_like(input_layer)\n",
    "    noise = z.normal_(mean=mean_gaus, std=std_gaus)\n",
    "    z = torch.abs(input_layer + noise)\n",
    "\n",
    "    z = (z + z.t())/2\n",
    "    z = z.fill_diagonal_(1)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Ynq9bCQu8xPr"
   },
   "outputs": [],
   "source": [
    "def predict(model, A, X_initial):\n",
    "    '''\n",
    "    Predicts using the given model.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The graph nn model (AGSRVec) to use for prediction.\n",
    "        A (list): List of adjacency matrices.\n",
    "        X_initial (numpy.ndarray or torch.Tensor): Initial LR node embeddings.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Contains predicted adjacency matrices and correspondin vectorized forms.\n",
    "    '''\n",
    "    model.eval()\n",
    "\n",
    "    # Convert X_initial to torch.Tensor if needed\n",
    "    if not isinstance(X_initial, torch.Tensor):\n",
    "        X_initial = torch.from_numpy(X_initial).type(torch.FloatTensor).to(device)\n",
    "\n",
    "    preds_list_adjacency = []\n",
    "    preds_list_vector = []\n",
    "    for lr, X in zip(A, X_initial):\n",
    "        if not np.any(lr) == False:\n",
    "            lr = torch.from_numpy(lr).type(torch.FloatTensor).to(device)\n",
    "            X = X.float().to(device)\n",
    "\n",
    "            # Generate predictions using the model\n",
    "            preds = model(lr, X)\n",
    "            adjacency = preds.cpu().detach().numpy()\n",
    "            preds_list_adjacency.append(adjacency)\n",
    "            preds_list_vector.append(MatrixVectorizer.vectorize(adjacency))\n",
    "\n",
    "    preds_list_adjacency = np.array(preds_list_adjacency)\n",
    "    preds_list_vector = np.array(preds_list_vector)\n",
    "    return preds_list_adjacency, preds_list_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "3oCZwr_E8xPr"
   },
   "outputs": [],
   "source": [
    "def evaluate(preds, truths, full_eval=True):\n",
    "    '''\n",
    "    Evaluate metrics comparing predicted results against ground truth data.\n",
    "\n",
    "    Args:\n",
    "        preds (numpy.ndarray): Predicted HR output\n",
    "        truths (numpy.ndarray): Ground truth HR output\n",
    "        full_eval (bool, optional): Flag to perform full evaluation including computing centrality measures\n",
    "\n",
    "    Returns:\n",
    "        tuple: Tuple containing evaluation metrics.\n",
    "            If full_eval is True:\n",
    "                - Mean absolute error (MAE)\n",
    "                - Pearson correlation coefficient (PCC)\n",
    "                - Jensen-Shannon divergence (JS_DIS)\n",
    "                - Average MAE for betweenness centrality (avg_mae_bc)\n",
    "                - Average MAE for eigenvector centrality (avg_mae_ec)\n",
    "                - Average MAE for pagerank centrality (avg_mae_pc)\n",
    "                - Average MAE for communicability betweenness entrality (avg_mae_cbc)\n",
    "                - Average MAE for degree centrality (avg_mae_dc)\n",
    "            If full_eval is False:\n",
    "                - Mean absolute error (MAE)\n",
    "                - Pearson correlation coefficient (PCC)\n",
    "                - Jensen-Shannon divergence (JS_DIS)\n",
    "    '''\n",
    "    # Initialize lists to store MAEs for each centrality measure\n",
    "    mae_bc = []\n",
    "    mae_ec = []\n",
    "    mae_pc = []\n",
    "    mae_cbc = []\n",
    "    mae_dc = []\n",
    "\n",
    "    pred_1d_list = []\n",
    "    gt_1d_list = []\n",
    "    count = 0\n",
    "    # Iterate over each test sample\n",
    "    for pred, truth in zip(preds, truths):\n",
    "        if full_eval:\n",
    "            print(count)\n",
    "        count += 1\n",
    "        # Convert adjacency matrices to NetworkX graph\n",
    "        pred_graph = nx.from_numpy_array(pred)\n",
    "        gt_graph = nx.from_numpy_array(truth)\n",
    "\n",
    "        if full_eval and nx.is_connected(pred_graph) and nx.is_connected(gt_graph):\n",
    "            # Compute predicted centrality measures\n",
    "            pred_bc = nx.betweenness_centrality(pred_graph)\n",
    "            pred_ec = nx.eigenvector_centrality(pred_graph)\n",
    "            pred_pc = nx.pagerank(pred_graph)\n",
    "            pred_cbc = nx.communicability_betweenness_centrality(pred_graph)\n",
    "            pred_dc = nx.degree_centrality(pred_graph)\n",
    "\n",
    "            # Compute ground-truth centrality measures\n",
    "            gt_bc = nx.betweenness_centrality(gt_graph)\n",
    "            gt_ec = nx.eigenvector_centrality(gt_graph)\n",
    "            gt_pc = nx.pagerank(gt_graph)\n",
    "            gt_cbc = nx.current_flow_betweenness_centrality(gt_graph)\n",
    "            gt_dc = nx.degree_centrality(gt_graph)\n",
    "\n",
    "            # Convert centrality dictionaries to lists\n",
    "            pred_bc_values = list(pred_bc.values())\n",
    "            pred_ec_values = list(pred_ec.values())\n",
    "            pred_pc_values = list(pred_pc.values())\n",
    "            pred_cbc_values = list(pred_cbc.values())\n",
    "            pred_dc_values = list(pred_dc.values())\n",
    "\n",
    "            gt_bc_values = list(gt_bc.values())\n",
    "            gt_ec_values = list(gt_ec.values())\n",
    "            gt_pc_values = list(gt_pc.values())\n",
    "            gt_cbc_values = list(gt_cbc.values())\n",
    "            gt_dc_values = list(gt_dc.values())\n",
    "\n",
    "            # Compute MAEs for centrality measures\n",
    "            mae_bc.append(mean_absolute_error(pred_bc_values, gt_bc_values))\n",
    "            mae_ec.append(mean_absolute_error(pred_ec_values, gt_ec_values))\n",
    "            mae_pc.append(mean_absolute_error(pred_pc_values, gt_pc_values))\n",
    "            mae_cbc.append(mean_absolute_error(pred_cbc_values, gt_cbc_values))\n",
    "            mae_dc.append(mean_absolute_error(pred_dc_values, gt_dc_values))\n",
    "\n",
    "        # Vectorize matrices\n",
    "        pred_1d_list.append(MatrixVectorizer.vectorize(pred))\n",
    "        gt_1d_list.append(MatrixVectorizer.vectorize(truth))\n",
    "\n",
    "    if full_eval:\n",
    "        # Compute average MAEs\n",
    "        avg_mae_bc = sum(mae_bc) / len(mae_bc)\n",
    "        avg_mae_ec = sum(mae_ec) / len(mae_ec)\n",
    "        avg_mae_pc = sum(mae_pc) / len(mae_pc)\n",
    "        avg_mae_cbc = sum(mae_cbc) / len(mae_cbc)\n",
    "        avg_mae_dc = sum(mae_dc) / len(mae_dc)\n",
    "\n",
    "    # Concatenate flattened matrices\n",
    "    pred_1d = np.concatenate(pred_1d_list)\n",
    "    gt_1d = np.concatenate(gt_1d_list)\n",
    "\n",
    "    mae = mean_absolute_error(pred_1d, gt_1d)\n",
    "    pcc = pearsonr(pred_1d, gt_1d)[0]\n",
    "    js_dis = jensenshannon(pred_1d, gt_1d)\n",
    "\n",
    "    if full_eval:\n",
    "        return mae, pcc, js_dis, avg_mae_bc, avg_mae_ec, avg_mae_pc, avg_mae_cbc, avg_mae_dc\n",
    "    else:\n",
    "        return mae, pcc, js_dis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "8x2U82HSsk_w"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr, structural_similarity as ssim\n",
    "from community import community_louvain\n",
    "import os\n",
    "\n",
    "def calculate_centralities(adj_matrix):\n",
    "    if adj_matrix.shape[0] != adj_matrix.shape[1]:\n",
    "        raise ValueError(f\"Adjacency matrix is not square: shape={adj_matrix.shape}\")\n",
    "    print(f\"Processing adjacency matrix of shape: {adj_matrix.shape}\")\n",
    "\n",
    "    G = nx.from_numpy_array(adj_matrix)\n",
    "    partition = community_louvain.best_partition(G)\n",
    "\n",
    "    # Calculate the participation coefficient with the partition\n",
    "    pc_dict = participation_coefficient(G, partition)\n",
    "\n",
    "    # Calculate averages of centrality measures\n",
    "    pr = nx.pagerank(G, alpha=0.9)\n",
    "    ec = nx.eigenvector_centrality_numpy(G, max_iter=100)\n",
    "    bc = nx.betweenness_centrality(G, normalized=True, endpoints=False)\n",
    "    ns = np.array(list(nx.degree_centrality(G).values())) * (len(G.nodes()) - 1)\n",
    "    acc = nx.average_clustering(G, weight=None)\n",
    "\n",
    "    # Average participation coefficient\n",
    "    pc_avg = np.mean(list(pc_dict.values()))\n",
    "\n",
    "    return {\n",
    "        'pr': np.mean(list(pr.values())),\n",
    "        'ec': np.mean(list(ec.values())),\n",
    "        'bc': np.mean(list(bc.values())),\n",
    "        'ns': ns,\n",
    "        'pc': pc_avg,\n",
    "        'acc': acc\n",
    "    }\n",
    "\n",
    "def participation_coefficient(G, partition):\n",
    "    # Initialize dictionary for participation coefficients\n",
    "    pc_dict = {}\n",
    "\n",
    "    # Calculate participation coefficient for each node\n",
    "    for node in G.nodes():\n",
    "        node_degree = G.degree(node)\n",
    "        if node_degree == 0:\n",
    "            pc_dict[node] = 0.0\n",
    "        else:\n",
    "            # Count within-module connections\n",
    "            within_module_degree = sum(1 for neighbor in G[node] if partition[neighbor] == partition[node])\n",
    "            # Calculate participation coefficient\n",
    "            pc_dict[node] = 1 - (within_module_degree / node_degree) ** 2\n",
    "\n",
    "    return pc_dict\n",
    "\n",
    "\n",
    "def evaluate_all(true_hr_matrices, predicted_hr_matrices, output_path='ID-randomCV.csv'):\n",
    "    print(true_hr_matrices.shape)\n",
    "    print(predicted_hr_matrices.shape)\n",
    "\n",
    "    num_subjects = true_hr_matrices.shape[0]\n",
    "    results = []\n",
    "\n",
    "    for i in range(num_subjects):\n",
    "        true_matrix = true_hr_matrices[i, :, :]\n",
    "        pred_matrix = predicted_hr_matrices[i, :, :]\n",
    "\n",
    "        print(f\"Evaluating subject {i+1} with matrix shapes: true={true_matrix.shape}, pred={pred_matrix.shape}\")\n",
    "\n",
    "        if true_matrix.shape != pred_matrix.shape or true_matrix.shape[0] != true_matrix.shape[1]:\n",
    "            print(f\"Error: Matrix shape mismatch or not square for subject {i+1}: true={true_matrix.shape}, pred={pred_matrix.shape}\")\n",
    "            continue\n",
    "\n",
    "        metrics = {\n",
    "            'ID': i + 1,\n",
    "            'MAE': mean_absolute_error(true_matrix.flatten(), pred_matrix.flatten()),\n",
    "            'PCC': pearsonr(true_matrix.flatten(), pred_matrix.flatten())[0],\n",
    "            'JSD': jensenshannon(true_matrix.flatten(), pred_matrix.flatten()),\n",
    "        }\n",
    "\n",
    "        true_metrics = calculate_centralities(true_matrix)\n",
    "        pred_metrics = calculate_centralities(pred_matrix)\n",
    "\n",
    "        for key in ['NS', 'PR', 'EC', 'BC', 'PC', 'ACC']:\n",
    "            metrics[f'MAE in {key}'] = mean_absolute_error([true_metrics[key.lower()]], [pred_metrics[key.lower()]])\n",
    "\n",
    "        results.append(metrics)\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    if not df.empty:\n",
    "        # Check if the file exists to decide whether to write headers\n",
    "        file_exists = os.path.isfile(output_path)\n",
    "\n",
    "        df.to_csv(output_path, mode='a', header=not file_exists, index=False)\n",
    "        print(f\"Results appended to {output_path}.\")\n",
    "    else:\n",
    "        print(\"No data to save.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Urgm2S7Ci8qH"
   },
   "source": [
    "## Define Jensen-Shannon Divergence and Weights Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "vuV5U2zkjKiB"
   },
   "outputs": [],
   "source": [
    "def js_divergence(p, q):\n",
    "    \"\"\"\n",
    "    Calculate the Jensen-Shannon Divergence between two probability distributions\n",
    "    \"\"\"\n",
    "    m = 0.5 * (p + q)\n",
    "    # Use torch.distributions.kl_divergence for stability and avoid manual log\n",
    "    p_dist = torch.distributions.Categorical(probs=p)\n",
    "    q_dist = torch.distributions.Categorical(probs=q)\n",
    "    m_dist = torch.distributions.Categorical(probs=m)\n",
    "    return 0.5 * (torch.distributions.kl_divergence(p_dist, m_dist) + torch.distributions.kl_divergence(q_dist, m_dist))\n",
    "\n",
    "def compute_histogram(tensor, bins=10, min=0, max=1, eps=1e-10):\n",
    "    # Create a histogram of tensor values, assuming tensor values are in [min, max]\n",
    "    histogram = torch.histc(tensor, bins=bins, min=min, max=max)\n",
    "    # Add a small epsilon to each bin count to avoid division by zero or log(0)\n",
    "    histogram += eps\n",
    "    # Normalize the histogram to get a probability distribution\n",
    "    prob_distribution = histogram / torch.sum(histogram)\n",
    "    return prob_distribution\n",
    "\n",
    "def jsd_loss(Z_h, A_h):\n",
    "    '''\n",
    "    Compute the Jensen-Shannon Divergence (JSD) loss between predicted and ground truth distributions\n",
    "\n",
    "    Args:\n",
    "        Z_h (torch.Tensor): Predicted distribution tensor\n",
    "        A_h (torch.Tensor): Ground truth distribution tensor\n",
    "    '''\n",
    "    Z_h_flat = Z_h.flatten()\n",
    "    A_h_flat = A_h.flatten()\n",
    "    pred_dist = compute_histogram(Z_h_flat, bins=10, min=0, max=1)\n",
    "    true_dist = compute_histogram(A_h_flat, bins=10, min=0, max=1)\n",
    "    epsilon = 1e-10  # To prevent log(0)\n",
    "    jsd_loss = js_divergence(torch.log(pred_dist + epsilon), torch.log(true_dist + epsilon))\n",
    "    return jsd_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "83231GPyjOEg"
   },
   "source": [
    "## Main Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "SItDAtrqPOHA"
   },
   "outputs": [],
   "source": [
    "def train(model, A_train, GT_train, X_train_initial, num_epochs, lr, hr_dim, mean_dense, std_dense, mean_gaus, std_gaus, lmbda, mu=1, A_test=None, GT_test=None, X_test_initial=None):\n",
    "    '''\n",
    "    Train the model using the given arguments.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The graph-based leanring model (AGSRVec) to train.\n",
    "        A_train (list): List of training LR adjacency matrices.\n",
    "        GT_train (list): List of ground truth HR training output.\n",
    "        X_train_initial (numpy.ndarray or list): Initial LR data for training.\n",
    "        num_epochs (int): Number of training epochs.\n",
    "        lr (float): Learning rate for optimization.\n",
    "        hr_dim (int): Dimensionality of the high-resolution data.\n",
    "        mean_dense (float): Mean of the normal distribution for weight initialization in Discrimnator dense layers.\n",
    "        std_dense (float): Standard deviation of the normal distribution for weight initialization in Discriminator dense layers.\n",
    "        mean_gaus (float): Mean of the Gaussian noise added to genearted samples.\n",
    "        std_gaus (float): Standard deviation of the Gaussian noise added to generated samples.\n",
    "        lmbda (float): Weight for the Jensen-Shannon Divergence loss.\n",
    "        mu (float, optional): Additional weight for the MSE loss. Default is 1.\n",
    "        A_test (list, optional): List of test adjacency matrices. Default is None.\n",
    "        GT_test (list, optional): List of ground truth test data. Default is None.\n",
    "        X_test_initial (numpy.ndarray or list, optional): Initial input data for testing. Default is None.\n",
    "    '''\n",
    "\n",
    "    # Define loss criterions\n",
    "    bce_loss = nn.BCELoss()\n",
    "    criterion_mse = nn.MSELoss()\n",
    "    criterion_mae = nn.L1Loss()\n",
    "\n",
    "    # Initialize Discriminator model\n",
    "    modelD = Discriminator(hr_dim, mean_dense, std_dense).to(device)\n",
    "\n",
    "    # Initialize optimizers for generator and discriminator\n",
    "    optimizerG = optim.Adam(model.parameters(), lr=lr)\n",
    "    optimizerD = optim.Adam(modelD.parameters(), lr=lr)\n",
    "\n",
    "    best_loss = 100\n",
    "    counter = 0\n",
    "\n",
    "    # Main training loop\n",
    "    for epoch in tqdm(range(1, num_epochs+1), desc='Epochs', file=sys.stdout, mininterval=1):\n",
    "        epoch_loss = []\n",
    "        epoch_mse_error = []\n",
    "        epoch_mae_error = []\n",
    "        epoch_jsd_error = []\n",
    "        epoch_eigen_error = []\n",
    "\n",
    "        # Set models to training mode\n",
    "        model.train()\n",
    "        modelD.train()\n",
    "\n",
    "        # Itearte over training data (paired with corresponding initial node embeddings)\n",
    "        for A_l, A_h, X_initial in zip(A_train, GT_train, X_train_initial):\n",
    "            optimizerG.zero_grad()\n",
    "            optimizerD.zero_grad()\n",
    "\n",
    "            # Convert data to tensors and move to appropriate device\n",
    "            A_l = torch.from_numpy(A_l).float().to(device)\n",
    "            A_h = torch.from_numpy(A_h).float().to(device)\n",
    "            X_initial = torch.from_numpy(X_initial).float().to(device)\n",
    "\n",
    "            # Compute eigenvectors of the ground truth adjacency matrix\n",
    "            _, U_h = torch.linalg.eigh(A_h, UPLO='U')\n",
    "\n",
    "            # Pass input through model\n",
    "            Z_h = model(A_l, X_initial)\n",
    "\n",
    "            # Compute Jensen-Shannon Divergence loss\n",
    "            jsd_epoch_loss = jsd_loss(Z_h, A_h)\n",
    "            jsd_error = jsd_loss(Z_h, A_h)\n",
    "\n",
    "            # Compute total AGSRVec loss\n",
    "            mse_loss = criterion_mse(Z_h, A_h) + mu * criterion_mse(model.gsr_layer.weights, U_h) + lmbda * jsd_epoch_loss\n",
    "\n",
    "            # Compute individual errors for monitoring\n",
    "            mse_error = criterion_mse(Z_h, A_h)\n",
    "            mae_error = criterion_mae(Z_h, A_h)\n",
    "            eigen_error = criterion_mse(model.gsr_layer.weights, U_h)\n",
    "\n",
    "            # Generate fake data using Gaussian noise\n",
    "            real_data = Z_h.detach()\n",
    "            fake_data = gaussian_noise_layer(A_h, mean_gaus, std_gaus)\n",
    "\n",
    "            # Train-step for discriminator model\n",
    "            d_real = modelD(real_data)\n",
    "            d_fake = modelD(fake_data)\n",
    "            d_loss = bce_loss(d_real, torch.ones_like(d_real)) + bce_loss(d_fake, torch.zeros_like(d_fake))\n",
    "            d_loss.backward()\n",
    "            optimizerD.step()\n",
    "\n",
    "            d_fake = modelD(gaussian_noise_layer(A_h, mean_gaus, std_gaus))\n",
    "\n",
    "            # Compute generator loss\n",
    "            gen_loss = bce_loss(d_fake, torch.ones_like(d_fake)) + mse_loss\n",
    "            gen_loss.backward()\n",
    "            optimizerG.step()\n",
    "\n",
    "            epoch_loss.append(gen_loss.item())\n",
    "            epoch_mse_error.append(mse_error.item())\n",
    "            epoch_mae_error.append(mae_error.item())\n",
    "            epoch_jsd_error.append(jsd_error.item())\n",
    "            epoch_eigen_error.append(eigen_error.item())\n",
    "\n",
    "        tqdm.write(\"Epoch: {}, Loss: {:.4f}, MSE_Error: {:.4f}%, JSD_Error: {:.4f}, Eigen_Error: {:.4f},  MAE_Error: {:.4f}%\".format(epoch, np.mean(epoch_loss), np.mean(epoch_mse_error)*100, lmbda * np.mean(epoch_jsd_error), np.mean(epoch_eigen_error), np.mean(epoch_mae_error)*100))\n",
    "\n",
    "        # Evaluate on test set if available\n",
    "        if A_test is not None and GT_test is not None and X_test_initial is not None:\n",
    "            preds_adjacencies, _ = predict(model, A_test, X_test_initial)\n",
    "            mae, pcc, js_dis = evaluate(preds_adjacencies, GT_test, full_eval=False)\n",
    "            tqdm.write(\"Evaluation  mae: {:.4f}%, pcc: {:.4f}, js_dis: {:.4f}\".format(mae*100, pcc, js_dis))\n",
    "\n",
    "            if mae >= best_loss:\n",
    "              counter += 1\n",
    "              if counter >= 10 and epoch >= 110:\n",
    "                break\n",
    "            else:\n",
    "              best_loss = mae\n",
    "              counter = 0\n",
    "              tqdm.write(f'----best model epoch: {epoch}----')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YJ1naFdZtFBj"
   },
   "source": [
    "#### Now that all functions for model initialization and tarining are defined, we define main scripts for loading in data, k-fold cross-validation, and full-model training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iBNegwGK8xPt"
   },
   "source": [
    "## Load in {lr_train, hr_train, lr_test}\n",
    "#### We standardize data with 0.5 mean and clip for values < 0 post-standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZgBr8Kba__dT"
   },
   "outputs": [],
   "source": [
    "# Initialize StandardScalers for HR and LR separately\n",
    "scaler_hr = StandardScaler()\n",
    "scaler_lr = StandardScaler()\n",
    "\n",
    "# Load the training data\n",
    "training_1 = pd.read_csv('RandomCV/Fold1/lr_split_1.csv').to_numpy()\n",
    "training_2 = pd.read_csv('RandomCV/Fold2/lr_split_2.csv').to_numpy()\n",
    "training_3 = pd.read_csv('RandomCV/Fold3/lr_split_3.csv').to_numpy()\n",
    "print(training_1.shape)\n",
    "print(training_2.shape)\n",
    "print(training_3.shape)\n",
    "training_A = np.concatenate((training_1, training_2, training_3), axis=0)\n",
    "A_all = []\n",
    "for i in range(len(training_A)):\n",
    "    A = MatrixVectorizer.anti_vectorize(training_A[i], 160, include_diagonal=False)\n",
    "    A_scaled = scaler_lr.fit_transform(A)  # Scale the LR data\n",
    "    A_scaled = A_scaled + 0.5\n",
    "    # Clip negative values to zero\n",
    "    A_scaled[A_scaled < 0.0] = 0.0\n",
    "    A_all.append(A_scaled)\n",
    "training_A = np.array(A_all)\n",
    "\n",
    "# Load the training truths\n",
    "training_truths_1 = pd.read_csv('RandomCV/Fold1/hr_split_1.csv').to_numpy()\n",
    "training_truths_2 = pd.read_csv('RandomCV/Fold2/hr_split_2.csv').to_numpy()\n",
    "training_truths_3 = pd.read_csv('RandomCV/Fold3/hr_split_3.csv').to_numpy()\n",
    "print(training_truths_1.shape)\n",
    "print(training_truths_2.shape)\n",
    "print(training_truths_3.shape)\n",
    "training_truths = np.concatenate((training_truths_1, training_truths_2, training_truths_3), axis=0)\n",
    "A_all = []\n",
    "for i in range(len(training_truths)):\n",
    "    A = MatrixVectorizer.anti_vectorize(training_truths[i], 268, include_diagonal=False)\n",
    "    A_all.append(A)\n",
    "training_truths = np.array(A_all)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BFuUijvlxmdq"
   },
   "source": [
    "## (Pre-) Compute Node2Vec Embeddings for lr_{train, test}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w9yajSkdxaGf"
   },
   "outputs": [],
   "source": [
    "# Initialize Node2Vec model\n",
    "node2vec = Node2Vec()\n",
    "training_initial_embeddings = []\n",
    "\n",
    "for idx, A_l in enumerate(training_A):\n",
    "    print(f\"Computing Node2Vec initial embeddings for training sample {idx}\")\n",
    "    # Fit Node2Vec model to current adjacency matrix (converted to a nx graph)\n",
    "    node2vec.fit(nx.from_numpy_array(A_l))\n",
    "    X_initial = torch.tensor(np.array(node2vec._embedding))\n",
    "    training_initial_embeddings.append(X_initial.float())\n",
    "\n",
    "training_initial_embeddings = torch.stack(training_initial_embeddings).numpy()\n",
    "\n",
    "# Save to file using numpy.save for efficiency\n",
    "np.save('training_embeddings.npy', training_initial_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gb7DHZF9tFBj"
   },
   "source": [
    "### Load the initial embeddings produced by Node2Vec\n",
    "To avoid re-computations, run this cell to load in pre-computed embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "Z2Ky76PReGd4"
   },
   "outputs": [],
   "source": [
    "training_initial_embeddings = np.load('training_embeddings.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QdFZxZMlx1HM"
   },
   "source": [
    "### Perform K-Fold Cross Validation\n",
    "For K = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "E4WXi2L5tFBj"
   },
   "outputs": [],
   "source": [
    "# Tuned (using Bayesian inference) hyperaprameters used in training\n",
    "lmbda_best = 0.2\n",
    "mu_best = 5\n",
    "lr_best = 0.00015"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V6B_uvEd6C-L"
   },
   "source": [
    "#### Random dataset\n",
    "##### If train_all is true, train the model with all the training data. Otherwise, split the train set into train set and validation set and train the model only with train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ieh6LHvVOirw"
   },
   "outputs": [],
   "source": [
    "# fold1\n",
    "\n",
    "# Initialize lists for storing model predictions and ground truths for each fold\n",
    "\n",
    "train_all = True\n",
    "fold_predictions = []\n",
    "fold_truths = []\n",
    "\n",
    "print(\"Fold number:\", 1)\n",
    "\n",
    "if train_all:\n",
    "  # Get the training and testing data for the current fold\n",
    "  A_train, A_test = training_A[93: ], training_A[: 93]\n",
    "  print(A_train.shape)\n",
    "  print(A_test.shape)\n",
    "  # Get the initial embeddings for the training and testing data\n",
    "  X_train_initial, X_test_initial = training_initial_embeddings[93: ], training_initial_embeddings[: 93]\n",
    "  print(X_train_initial.shape)\n",
    "  print(X_test_initial.shape)\n",
    "  # Get the truths for the current fold\n",
    "  GT_train, GT_test = training_truths[93: ], training_truths[: 93]\n",
    "  print(GT_train.shape)\n",
    "  print(GT_test.shape)\n",
    "\n",
    "  # Initialize and train the model\n",
    "  model = AGSRVec(hr_dim=268, hidden_dim=268*2).to(device)\n",
    "\n",
    "  train(model, A_train, GT_train, X_train_initial, num_epochs=60, lr=lr_best, hr_dim=268, mean_dense=0., std_dense=0.01, mean_gaus=0., std_gaus=0.1, lmbda=lmbda_best, mu=mu_best)\n",
    "  torch.save(model.state_dict(), 'checkpoint_fold1_random_all.pt')\n",
    "\n",
    "else:\n",
    "  # Get the training and testing data for the current fold\n",
    "  A_train = np.concatenate((training_A[93: 166], training_A[186: 259]), axis=0)\n",
    "  A_val = np.concatenate((training_A[166: 186], training_A[259: ]), axis=0)\n",
    "  A_test = training_A[: 93]\n",
    "  print(A_train.shape)\n",
    "  print(A_val.shape)\n",
    "  print(A_test.shape)\n",
    "  # Get the initial embeddings for the training and testing data\n",
    "  X_train_initial = np.concatenate((training_initial_embeddings[93: 166], training_initial_embeddings[186: 259]), axis=0)\n",
    "  X_val_initial = np.concatenate((training_initial_embeddings[166: 186], training_initial_embeddings[259: ]), axis=0)\n",
    "  X_test_initial = training_initial_embeddings[: 93]\n",
    "  print(X_train_initial.shape)\n",
    "  print(X_val_initial.shape)\n",
    "  print(X_test_initial.shape)\n",
    "  # Get the truths for the current fold\n",
    "  GT_train = np.concatenate((training_truths[93: 166], training_truths[186: 259]), axis=0)\n",
    "  GT_val = np.concatenate((training_truths[166: 186], training_truths[259: ]), axis=0)\n",
    "  GT_test = training_truths[: 93]\n",
    "  print(GT_train.shape)\n",
    "  print(GT_val.shape)\n",
    "  print(GT_test.shape)\n",
    "\n",
    "  # Initialize and train the model\n",
    "  model = AGSRVec(hr_dim=268, hidden_dim=268*2).to(device)\n",
    "\n",
    "  train(model, A_train, GT_train, X_train_initial, num_epochs=500, lr=lr_best, hr_dim=268, mean_dense=0., std_dense=0.01, mean_gaus=0., std_gaus=0.1, lmbda=lmbda_best, mu=mu_best, A_test=A_val, GT_test=GT_val, X_test_initial=X_val_initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5LBlFrlx6l0p"
   },
   "outputs": [],
   "source": [
    "# fold2\n",
    "\n",
    "# Initialize lists for storing model predictions and ground truths for each fold\n",
    "\n",
    "train_all = True\n",
    "fold_predictions = []\n",
    "fold_truths = []\n",
    "\n",
    "print(\"Fold number:\", 2)\n",
    "\n",
    "if train_all:\n",
    "  # Get the training and testing data for the current fold\n",
    "  A_train = np.concatenate((training_A[: 93], training_A[186: ]), axis=0)\n",
    "  A_test = training_A[93: 186]\n",
    "  print(A_train.shape)\n",
    "  print(A_test.shape)\n",
    "  # Get the initial embeddings for the training and testing data\n",
    "  X_train_initial = np.concatenate((training_initial_embeddings[: 93], training_initial_embeddings[186: ]), axis=0)\n",
    "  X_test_initial = training_initial_embeddings[93: 186]\n",
    "  print(X_train_initial.shape)\n",
    "  print(X_test_initial.shape)\n",
    "  # Get the truths for the current fold\n",
    "  GT_train = np.concatenate((training_truths[: 93], training_truths[186: ]), axis=0)\n",
    "  GT_test = training_truths[93: 186]\n",
    "  print(GT_train.shape)\n",
    "  print(GT_test.shape)\n",
    "\n",
    "  # Initialize and train the model\n",
    "  model = AGSRVec(hr_dim=268, hidden_dim=268*2).to(device)\n",
    "\n",
    "  train(model, A_train, GT_train, X_train_initial, num_epochs=54, lr=lr_best, hr_dim=268, mean_dense=0., std_dense=0.01, mean_gaus=0., std_gaus=0.1, lmbda=lmbda_best, mu=mu_best)\n",
    "  torch.save(model.state_dict(), 'checkpoint_fold2_random_all.pt')\n",
    "\n",
    "else:\n",
    "  # Get the training and testing data for the current fold\n",
    "  A_train = np.concatenate((training_A[: 73], training_A[186: 259]), axis=0)\n",
    "  A_val = np.concatenate((training_A[73: 93], training_A[259: ]), axis=0)\n",
    "  A_test = training_A[93: 186]\n",
    "  print(A_train.shape)\n",
    "  print(A_val.shape)\n",
    "  print(A_test.shape)\n",
    "  # Get the initial embeddings for the training and testing data\n",
    "  X_train_initial = np.concatenate((training_initial_embeddings[: 73], training_initial_embeddings[186: 259]), axis=0)\n",
    "  X_val_initial = np.concatenate((training_initial_embeddings[73: 93], training_initial_embeddings[259: ]), axis=0)\n",
    "  X_test_initial = training_initial_embeddings[93: 186]\n",
    "  print(X_train_initial.shape)\n",
    "  print(X_val_initial.shape)\n",
    "  print(X_test_initial.shape)\n",
    "  # Get the truths for the current fold\n",
    "  GT_train = np.concatenate((training_truths[: 73], training_truths[186: 259]), axis=0)\n",
    "  GT_val = np.concatenate((training_truths[73: 93], training_truths[259: ]), axis=0)\n",
    "  GT_test = training_truths[93: 186]\n",
    "  print(GT_train.shape)\n",
    "  print(GT_val.shape)\n",
    "  print(GT_test.shape)\n",
    "\n",
    "  # Initialize and train the model\n",
    "  model = AGSRVec(hr_dim=268, hidden_dim=268*2).to(device)\n",
    "\n",
    "  train(model, A_train, GT_train, X_train_initial, num_epochs=500, lr=lr_best, hr_dim=268, mean_dense=0., std_dense=0.01, mean_gaus=0., std_gaus=0.1, lmbda=lmbda_best, mu=mu_best, A_test=A_val, GT_test=GT_val, X_test_initial=X_val_initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yNP8pzR-6oz3"
   },
   "outputs": [],
   "source": [
    "# fold3\n",
    "\n",
    "# Initialize lists for storing model predictions and ground truths for each fold\n",
    "\n",
    "train_all = True\n",
    "fold_predictions = []\n",
    "fold_truths = []\n",
    "\n",
    "print(\"Fold number:\", 3)\n",
    "\n",
    "if train_all:\n",
    "  # Get the training and testing data for the current fold\n",
    "  A_train, A_test = training_A[: 186], training_A[186: ]\n",
    "  print(A_train.shape)\n",
    "  print(A_test.shape)\n",
    "  # Get the initial embeddings for the training and testing data\n",
    "  X_train_initial, X_test_initial = training_initial_embeddings[: 186], training_initial_embeddings[186: ]\n",
    "  print(X_train_initial.shape)\n",
    "  print(X_test_initial.shape)\n",
    "  # Get the truths for the current fold\n",
    "  GT_train, GT_test = training_truths[: 186], training_truths[186: ]\n",
    "  print(GT_train.shape)\n",
    "  print(GT_test.shape)\n",
    "\n",
    "  # Initialize and train the model\n",
    "  model = AGSRVec(hr_dim=268, hidden_dim=268*2).to(device)\n",
    "\n",
    "  train(model, A_train, GT_train, X_train_initial, num_epochs=63, lr=lr_best, hr_dim=268, mean_dense=0., std_dense=0.01, mean_gaus=0., std_gaus=0.1, lmbda=lmbda_best, mu=mu_best)\n",
    "\n",
    "  torch.save(model.state_dict(), 'checkpoint_fold3_random_all.pt')\n",
    "\n",
    "else:\n",
    "  # Get the training and testing data for the current fold\n",
    "  A_train = np.concatenate((training_A[: 73], training_A[93: 166]), axis=0)\n",
    "  A_val = np.concatenate((training_A[73: 93], training_A[166: 186]), axis=0)\n",
    "  A_test = training_A[186: ]\n",
    "  print(A_train.shape)\n",
    "  print(A_val.shape)\n",
    "  print(A_test.shape)\n",
    "  # Get the initial embeddings for the training and testing data\n",
    "  X_train_initial = np.concatenate((training_initial_embeddings[: 73], training_initial_embeddings[93: 166]), axis=0)\n",
    "  X_val_initial = np.concatenate((training_initial_embeddings[73: 93], training_initial_embeddings[166: 186]), axis=0)\n",
    "  X_test_initial = training_initial_embeddings[186: ]\n",
    "  print(X_train_initial.shape)\n",
    "  print(X_val_initial.shape)\n",
    "  print(X_test_initial.shape)\n",
    "  # Get the truths for the current fold\n",
    "  GT_train = np.concatenate((training_truths[: 73], training_truths[93: 166]), axis=0)\n",
    "  GT_val = np.concatenate((training_truths[73: 93], training_truths[166: 186]), axis=0)\n",
    "  GT_test = training_truths[186: ]\n",
    "  print(GT_train.shape)\n",
    "  print(GT_val.shape)\n",
    "  print(GT_test.shape)\n",
    "\n",
    "  # Initialize and train the model\n",
    "  model = AGSRVec(hr_dim=268, hidden_dim=268*2).to(device)\n",
    "\n",
    "  train(model, A_train, GT_train, X_train_initial, num_epochs=500, lr=lr_best, hr_dim=268, mean_dense=0., std_dense=0.01, mean_gaus=0., std_gaus=0.1, lmbda=lmbda_best, mu=mu_best, A_test=A_val, GT_test=GT_val, X_test_initial=X_val_initial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y6o8PkP86eZY"
   },
   "source": [
    "#### Cluster dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OL821YZR5kjw"
   },
   "outputs": [],
   "source": [
    "# fold1\n",
    "\n",
    "# Initialize lists for storing model predictions and ground truths for each fold\n",
    "\n",
    "train_all = True\n",
    "fold_predictions = []\n",
    "fold_truths = []\n",
    "\n",
    "print(\"Fold number:\", 1)\n",
    "\n",
    "if train_all:\n",
    "  # Get the training and testing data for the current fold\n",
    "  A_train, A_test = training_A[102: ], training_A[: 102]\n",
    "  print(A_train.shape)\n",
    "  print(A_test.shape)\n",
    "  # Get the initial embeddings for the training and testing data\n",
    "  X_train_initial, X_test_initial = training_initial_embeddings[102: ], training_initial_embeddings[: 102]\n",
    "  print(X_train_initial.shape)\n",
    "  print(X_test_initial.shape)\n",
    "  # Get the truths for the current fold\n",
    "  GT_train, GT_test = training_truths[102: ], training_truths[: 102]\n",
    "  print(GT_train.shape)\n",
    "  print(GT_test.shape)\n",
    "\n",
    "  # Initialize and train the model\n",
    "  model = AGSRVec(hr_dim=268, hidden_dim=268*2).to(device)\n",
    "\n",
    "  train(model, A_train, GT_train, X_train_initial, num_epochs=43, lr=lr_best, hr_dim=268, mean_dense=0., std_dense=0.01, mean_gaus=0., std_gaus=0.1, lmbda=lmbda_best, mu=mu_best)\n",
    "  torch.save(model.state_dict(), 'checkpoint_fold1_cluster_all.pt')\n",
    "\n",
    "else:\n",
    "  # Get the training and testing data for the current fold\n",
    "  A_train = np.concatenate((training_A[102: 184], training_A[204: 259]), axis=0)\n",
    "  A_val = np.concatenate((training_A[184: 204], training_A[259: ]), axis=0)\n",
    "  A_test = training_A[: 102]\n",
    "  print(A_train.shape)\n",
    "  print(A_val.shape)\n",
    "  print(A_test.shape)\n",
    "  # Get the initial embeddings for the training and testing data\n",
    "  X_train_initial = np.concatenate((training_initial_embeddings[102: 184], training_initial_embeddings[204: 259]), axis=0)\n",
    "  X_val_initial = np.concatenate((training_initial_embeddings[184: 204], training_initial_embeddings[259: ]), axis=0)\n",
    "  X_test_initial = training_initial_embeddings[: 102]\n",
    "  print(X_train_initial.shape)\n",
    "  print(X_val_initial.shape)\n",
    "  print(X_test_initial.shape)\n",
    "  # Get the truths for the current fold\n",
    "  GT_train = np.concatenate((training_truths[102: 184], training_truths[204: 259]), axis=0)\n",
    "  GT_val = np.concatenate((training_truths[184: 204], training_truths[259: ]), axis=0)\n",
    "  GT_test = training_truths[: 102]\n",
    "  print(GT_train.shape)\n",
    "  print(GT_val.shape)\n",
    "  print(GT_test.shape)\n",
    "\n",
    "  # Initialize and train the model\n",
    "  model = AGSRVec(hr_dim=268, hidden_dim=268*2).to(device)\n",
    "\n",
    "  train(model, A_train, GT_train, X_train_initial, num_epochs=500, lr=lr_best, hr_dim=268, mean_dense=0., std_dense=0.01, mean_gaus=0., std_gaus=0.1, lmbda=lmbda_best, mu=mu_best, A_test=A_val, GT_test=GT_val, X_test_initial=X_val_initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uzfXJ29l7DWs"
   },
   "outputs": [],
   "source": [
    "# fold2\n",
    "\n",
    "# Initialize lists for storing model predictions and ground truths for each fold\n",
    "\n",
    "train_all = True\n",
    "fold_predictions = []\n",
    "fold_truths = []\n",
    "\n",
    "print(\"Fold number:\", 2)\n",
    "\n",
    "if train_all:\n",
    "  # Get the training and testing data for the current fold\n",
    "  A_train = np.concatenate((training_A[: 102], training_A[204: ]), axis=0)\n",
    "  A_test = training_A[102: 204]\n",
    "  print(A_train.shape)\n",
    "  print(A_test.shape)\n",
    "  # Get the initial embeddings for the training and testing data\n",
    "  X_train_initial = np.concatenate((training_initial_embeddings[: 102], training_initial_embeddings[204: ]), axis=0)\n",
    "  X_test_initial = training_initial_embeddings[102: 204]\n",
    "  print(X_train_initial.shape)\n",
    "  print(X_test_initial.shape)\n",
    "  # Get the truths for the current fold\n",
    "  GT_train = np.concatenate((training_truths[: 102], training_truths[204: ]), axis=0)\n",
    "  GT_test = training_truths[102: 204]\n",
    "  print(GT_train.shape)\n",
    "  print(GT_test.shape)\n",
    "\n",
    "  # Initialize and train the model\n",
    "  model = AGSRVec(hr_dim=268, hidden_dim=268*2).to(device)\n",
    "\n",
    "  train(model, A_train, GT_train, X_train_initial, num_epochs=51, lr=lr_best, hr_dim=268, mean_dense=0., std_dense=0.01, mean_gaus=0., std_gaus=0.1, lmbda=lmbda_best, mu=mu_best)\n",
    "  torch.save(model.state_dict(), 'checkpoint_fold2_cluster_all.pt')\n",
    "\n",
    "else:\n",
    "  # Get the training and testing data for the current fold\n",
    "  A_train = np.concatenate((training_A[: 82], training_A[204: 259]), axis=0)\n",
    "  A_val = np.concatenate((training_A[82: 102], training_A[259: ]), axis=0)\n",
    "  A_test = training_A[102: 204]\n",
    "  print(A_train.shape)\n",
    "  print(A_val.shape)\n",
    "  print(A_test.shape)\n",
    "  # Get the initial embeddings for the training and testing data\n",
    "  X_train_initial = np.concatenate((training_initial_embeddings[: 82], training_initial_embeddings[204: 259]), axis=0)\n",
    "  X_val_initial = np.concatenate((training_initial_embeddings[82: 102], training_initial_embeddings[259: ]), axis=0)\n",
    "  X_test_initial = training_initial_embeddings[102: 204]\n",
    "  print(X_train_initial.shape)\n",
    "  print(X_val_initial.shape)\n",
    "  print(X_test_initial.shape)\n",
    "  # Get the truths for the current fold\n",
    "  GT_train = np.concatenate((training_truths[: 82], training_truths[204: 259]), axis=0)\n",
    "  GT_val = np.concatenate((training_truths[82: 102], training_truths[259: ]), axis=0)\n",
    "  GT_test = training_truths[102: 204]\n",
    "  print(GT_train.shape)\n",
    "  print(GT_val.shape)\n",
    "  print(GT_test.shape)\n",
    "\n",
    "  # Initialize and train the model\n",
    "  model = AGSRVec(hr_dim=268, hidden_dim=268*2).to(device)\n",
    "\n",
    "  train(model, A_train, GT_train, X_train_initial, num_epochs=500, lr=lr_best, hr_dim=268, mean_dense=0., std_dense=0.01, mean_gaus=0., std_gaus=0.1, lmbda=lmbda_best, mu=mu_best, A_test=A_val, GT_test=GT_val, X_test_initial=X_val_initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ITtIhvZ47VPB"
   },
   "outputs": [],
   "source": [
    "# fold3\n",
    "\n",
    "# Initialize lists for storing model predictions and ground truths for each fold\n",
    "\n",
    "train_all = True\n",
    "fold_predictions = []\n",
    "fold_truths = []\n",
    "\n",
    "print(\"Fold number:\", 3)\n",
    "\n",
    "if train_all:\n",
    "  # Get the training and testing data for the current fold\n",
    "  A_train, A_test = training_A[: 204], training_A[204: ]\n",
    "  print(A_train.shape)\n",
    "  print(A_test.shape)\n",
    "  # Get the initial embeddings for the training and testing data\n",
    "  X_train_initial, X_test_initial = training_initial_embeddings[: 204], training_initial_embeddings[204: ]\n",
    "  print(X_train_initial.shape)\n",
    "  print(X_test_initial.shape)\n",
    "  # Get the truths for the current fold\n",
    "  GT_train, GT_test = training_truths[: 204], training_truths[204: ]\n",
    "  print(GT_train.shape)\n",
    "  print(GT_test.shape)\n",
    "\n",
    "  # Initialize and train the model\n",
    "  model = AGSRVec(hr_dim=268, hidden_dim=268*2).to(device)\n",
    "\n",
    "  train(model, A_train, GT_train, X_train_initial, num_epochs=158, lr=lr_best, hr_dim=268, mean_dense=0., std_dense=0.01, mean_gaus=0., std_gaus=0.1, lmbda=lmbda_best, mu=mu_best)\n",
    "  torch.save(model.state_dict(), 'checkpoint_fold3_cluster_all.pt')\n",
    "\n",
    "else:\n",
    "  # Get the training and testing data for the current fold\n",
    "  A_train = np.concatenate((training_A[: 82], training_A[102: 184]), axis=0)\n",
    "  A_val = np.concatenate((training_A[82: 102], training_A[184: 204]), axis=0)\n",
    "  A_test = training_A[204: ]\n",
    "  print(A_train.shape)\n",
    "  print(A_val.shape)\n",
    "  print(A_test.shape)\n",
    "  # Get the initial embeddings for the training and testing data\n",
    "  X_train_initial = np.concatenate((training_initial_embeddings[: 82], training_initial_embeddings[102: 184]), axis=0)\n",
    "  X_val_initial = np.concatenate((training_initial_embeddings[82: 102], training_initial_embeddings[184: 204]), axis=0)\n",
    "  X_test_initial = training_initial_embeddings[204: ]\n",
    "  print(X_train_initial.shape)\n",
    "  print(X_val_initial.shape)\n",
    "  print(X_test_initial.shape)\n",
    "  # Get the truths for the current fold\n",
    "  GT_train = np.concatenate((training_truths[: 82], training_truths[102: 184]), axis=0)\n",
    "  GT_val = np.concatenate((training_truths[82: 102], training_truths[184: 204]), axis=0)\n",
    "  GT_test = training_truths[204: ]\n",
    "  print(GT_train.shape)\n",
    "  print(GT_val.shape)\n",
    "  print(GT_test.shape)\n",
    "\n",
    "  # Initialize and train the model\n",
    "  model = AGSRVec(hr_dim=268, hidden_dim=268*2).to(device)\n",
    "\n",
    "  train(model, A_train, GT_train, X_train_initial, num_epochs=500, lr=lr_best, hr_dim=268, mean_dense=0., std_dense=0.01, mean_gaus=0., std_gaus=0.1, lmbda=lmbda_best, mu=mu_best, A_test=A_val, GT_test=GT_val, X_test_initial=X_val_initial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9-lfIYlEtFBp"
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pFtzPfZZn_7I"
   },
   "source": [
    "### random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h-eiU3jMtQhy"
   },
   "outputs": [],
   "source": [
    "# fold1\n",
    "\n",
    "# Get the training and testing data for the current fold\n",
    "A_train, A_test = training_A[: 186], training_A[186: ]\n",
    "print(A_train.shape)\n",
    "print(A_test.shape)\n",
    "# Get the initial embeddings for the training and testing data\n",
    "X_train_initial, X_test_initial = training_initial_embeddings[: 186], training_initial_embeddings[186: ]\n",
    "print(X_train_initial.shape)\n",
    "print(X_test_initial.shape)\n",
    "# Get the truths for the current fold\n",
    "GT_train, GT_test = training_truths[: 186], training_truths[186: ]\n",
    "print(GT_train.shape)\n",
    "print(GT_test.shape)\n",
    "\n",
    "model = AGSRVec(hr_dim=268, hidden_dim=268*2).to(device)\n",
    "state_dict_path = 'checkpoint_fold1_random_all.pt'\n",
    "model.load_state_dict(torch.load(state_dict_path))\n",
    "\n",
    "preds_adjacencies, preds_vectors = predict(model, A_test, X_test_initial)\n",
    "# mae, pcc, js_dis, avg_mae_bc, avg_mae_ec, avg_mae_pc, avg_mae_cbc, avg_mae_dc = evaluate(preds_adjacencies, GT_test, full_eval=True)\n",
    "# print(f\"Fold 1 MAE: {mae}, PCC: {pcc}, JS Distance: {js_dis}, Avg MAE BC: {avg_mae_bc}, Avg MAE EC: {avg_mae_ec}, Avg MAE PC: {avg_mae_pc}, Avg MAE CBC: {avg_mae_cbc}, Avg MAE DC: {avg_mae_dc}\")\n",
    "\n",
    "evaluate_all(GT_test, preds_adjacencies, output_path='4-randomCV.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xSK_Kt5Xb1w_"
   },
   "outputs": [],
   "source": [
    "# fold2\n",
    "\n",
    "# Get the training and testing data for the current fold\n",
    "A_train = np.concatenate((training_A[: 93], training_A[186: ]), axis=0)\n",
    "A_test = training_A[93: 186]\n",
    "print(A_train.shape)\n",
    "print(A_test.shape)\n",
    "# Get the initial embeddings for the training and testing data\n",
    "X_train_initial = np.concatenate((training_initial_embeddings[: 93], training_initial_embeddings[186: ]), axis=0)\n",
    "X_test_initial = training_initial_embeddings[93: 186]\n",
    "print(X_train_initial.shape)\n",
    "print(X_test_initial.shape)\n",
    "# Get the truths for the current fold\n",
    "GT_train = np.concatenate((training_truths[: 93], training_truths[186: ]), axis=0)\n",
    "GT_test = training_truths[93: 186]\n",
    "print(GT_train.shape)\n",
    "print(GT_test.shape)\n",
    "\n",
    "model = AGSRVec(hr_dim=268, hidden_dim=268*2).to(device)\n",
    "state_dict_path = 'checkpoint_fold2_random_all.pt'\n",
    "model.load_state_dict(torch.load(state_dict_path))\n",
    "\n",
    "preds_adjacencies, preds_vectors = predict(model, A_test, X_test_initial)\n",
    "# mae, pcc, js_dis, avg_mae_bc, avg_mae_ec, avg_mae_pc, avg_mae_cbc, avg_mae_dc = evaluate(preds_adjacencies, GT_test, full_eval=True)\n",
    "# print(f\"Fold 1 MAE: {mae}, PCC: {pcc}, JS Distance: {js_dis}, Avg MAE BC: {avg_mae_bc}, Avg MAE EC: {avg_mae_ec}, Avg MAE PC: {avg_mae_pc}, Avg MAE CBC: {avg_mae_cbc}, Avg MAE DC: {avg_mae_dc}\")\n",
    "\n",
    "evaluate_all(GT_test, preds_adjacencies, output_path='4-randomCV.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uLeUnxjVfFKY"
   },
   "outputs": [],
   "source": [
    "# fold3\n",
    "\n",
    "# Get the training and testing data for the current fold\n",
    "A_train, A_test = training_A[: 186], training_A[186: ]\n",
    "print(A_train.shape)\n",
    "print(A_test.shape)\n",
    "# Get the initial embeddings for the training and testing data\n",
    "X_train_initial, X_test_initial = training_initial_embeddings[: 186], training_initial_embeddings[186: ]\n",
    "print(X_train_initial.shape)\n",
    "print(X_test_initial.shape)\n",
    "# Get the truths for the current fold\n",
    "GT_train, GT_test = training_truths[: 186], training_truths[186: ]\n",
    "print(GT_train.shape)\n",
    "print(GT_test.shape)\n",
    "\n",
    "model = AGSRVec(hr_dim=268, hidden_dim=268*2).to(device)\n",
    "state_dict_path = 'checkpoint_fold3_random_all.pt'\n",
    "model.load_state_dict(torch.load(state_dict_path))\n",
    "\n",
    "preds_adjacencies, preds_vectors = predict(model, A_test, X_test_initial)\n",
    "# mae, pcc, js_dis, avg_mae_bc, avg_mae_ec, avg_mae_pc, avg_mae_cbc, avg_mae_dc = evaluate(preds_adjacencies, GT_test, full_eval=True)\n",
    "# print(f\"Fold 1 MAE: {mae}, PCC: {pcc}, JS Distance: {js_dis}, Avg MAE BC: {avg_mae_bc}, Avg MAE EC: {avg_mae_ec}, Avg MAE PC: {avg_mae_pc}, Avg MAE CBC: {avg_mae_cbc}, Avg MAE DC: {avg_mae_dc}\")\n",
    "\n",
    "evaluate_all(GT_test, preds_adjacencies, output_path='4-randomCV.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y1gDCHaz1HiV"
   },
   "source": [
    "### cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rZizFSe4J_ns"
   },
   "outputs": [],
   "source": [
    "# fold 1\n",
    "\n",
    "# Get the training and testing data for the current fold\n",
    "A_train, A_test = training_A[102: ], training_A[: 102]\n",
    "print(A_train.shape)\n",
    "print(A_test.shape)\n",
    "# Get the initial embeddings for the training and testing data\n",
    "X_train_initial, X_test_initial = training_initial_embeddings[102: ], training_initial_embeddings[: 102]\n",
    "print(X_train_initial.shape)\n",
    "print(X_test_initial.shape)\n",
    "# Get the truths for the current fold\n",
    "GT_train, GT_test = training_truths[102: ], training_truths[: 102]\n",
    "print(GT_train.shape)\n",
    "print(GT_test.shape)\n",
    "\n",
    "# Initialize and train the model\n",
    "model = AGSRVec(hr_dim=268, hidden_dim=268*2).to(device)\n",
    "state_dict_path = 'checkpoint_fold1_cluster_all.pt'\n",
    "model.load_state_dict(torch.load(state_dict_path))\n",
    "\n",
    "preds_adjacencies, preds_vectors = predict(model, A_test, X_test_initial)\n",
    "# mae, pcc, js_dis, avg_mae_bc, avg_mae_ec, avg_mae_pc, avg_mae_cbc, avg_mae_dc = evaluate(preds_adjacencies, GT_test, full_eval=True)\n",
    "# print(f\"Fold 1 MAE: {mae}, PCC: {pcc}, JS Distance: {js_dis}, Avg MAE BC: {avg_mae_bc}, Avg MAE EC: {avg_mae_ec}, Avg MAE PC: {avg_mae_pc}, Avg MAE CBC: {avg_mae_cbc}, Avg MAE DC: {avg_mae_dc}\")\n",
    "\n",
    "evaluate_all(GT_test, preds_adjacencies, output_path='4-clusterCV.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YzwT2zZ8NMfR"
   },
   "outputs": [],
   "source": [
    "# fold2\n",
    "\n",
    "# Get the training and testing data for the current fold\n",
    "A_train = np.concatenate((training_A[: 102], training_A[204: ]), axis=0)\n",
    "A_test = training_A[102: 204]\n",
    "print(A_train.shape)\n",
    "print(A_test.shape)\n",
    "# Get the initial embeddings for the training and testing data\n",
    "X_train_initial = np.concatenate((training_initial_embeddings[: 102], training_initial_embeddings[204: ]), axis=0)\n",
    "X_test_initial = training_initial_embeddings[102: 204]\n",
    "print(X_train_initial.shape)\n",
    "print(X_test_initial.shape)\n",
    "# Get the truths for the current fold\n",
    "GT_train = np.concatenate((training_truths[: 102], training_truths[204: ]), axis=0)\n",
    "GT_test = training_truths[102: 204]\n",
    "print(GT_train.shape)\n",
    "print(GT_test.shape)\n",
    "\n",
    "# Initialize and train the model\n",
    "model = AGSRVec(hr_dim=268, hidden_dim=268*2).to(device)\n",
    "state_dict_path = 'checkpoint_fold2_cluster_all.pt'\n",
    "model.load_state_dict(torch.load(state_dict_path))\n",
    "\n",
    "preds_adjacencies, preds_vectors = predict(model, A_test, X_test_initial)\n",
    "# mae, pcc, js_dis, avg_mae_bc, avg_mae_ec, avg_mae_pc, avg_mae_cbc, avg_mae_dc = evaluate(preds_adjacencies, GT_test, full_eval=True)\n",
    "# print(f\"Fold 1 MAE: {mae}, PCC: {pcc}, JS Distance: {js_dis}, Avg MAE BC: {avg_mae_bc}, Avg MAE EC: {avg_mae_ec}, Avg MAE PC: {avg_mae_pc}, Avg MAE CBC: {avg_mae_cbc}, Avg MAE DC: {avg_mae_dc}\")\n",
    "\n",
    "evaluate_all(GT_test, preds_adjacencies, output_path='4-clusterCV.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "arjA_LR1NUrn"
   },
   "outputs": [],
   "source": [
    "# fold3\n",
    "\n",
    "# Get the training and testing data for the current fold\n",
    "A_train, A_test = training_A[: 204], training_A[204: ]\n",
    "print(A_train.shape)\n",
    "print(A_test.shape)\n",
    "# Get the initial embeddings for the training and testing data\n",
    "X_train_initial, X_test_initial = training_initial_embeddings[: 204], training_initial_embeddings[204: ]\n",
    "print(X_train_initial.shape)\n",
    "print(X_test_initial.shape)\n",
    "# Get the truths for the current fold\n",
    "GT_train, GT_test = training_truths[: 204], training_truths[204: ]\n",
    "print(GT_train.shape)\n",
    "print(GT_test.shape)\n",
    "\n",
    "# Initialize and train the model\n",
    "model = AGSRVec(hr_dim=268, hidden_dim=268*2).to(device)\n",
    "state_dict_path = 'checkpoint_fold3_cluster_all.pt'\n",
    "model.load_state_dict(torch.load(state_dict_path))\n",
    "\n",
    "preds_adjacencies, preds_vectors = predict(model, A_test, X_test_initial)\n",
    "# mae, pcc, js_dis, avg_mae_bc, avg_mae_ec, avg_mae_pc, avg_mae_cbc, avg_mae_dc = evaluate(preds_adjacencies, GT_test, full_eval=True)\n",
    "# print(f\"Fold 1 MAE: {mae}, PCC: {pcc}, JS Distance: {js_dis}, Avg MAE BC: {avg_mae_bc}, Avg MAE EC: {avg_mae_ec}, Avg MAE PC: {avg_mae_pc}, Avg MAE CBC: {avg_mae_cbc}, Avg MAE DC: {avg_mae_dc}\")\n",
    "\n",
    "evaluate_all(GT_test, preds_adjacencies, output_path='4-clusterCV.csv')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
