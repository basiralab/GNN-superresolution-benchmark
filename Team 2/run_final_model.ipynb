{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Library\n",
    "import argparse\n",
    "import random\n",
    "import pickle\n",
    "from typing import Union\n",
    "\n",
    "# Third-Party Libraries\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "# Custom Modules\n",
    "from MatrixVectorizer import *\n",
    "from dataloaders import NoisyDataset\n",
    "from model import *\n",
    "from preprocessing import *\n",
    "from train import *\n",
    "from evaluation import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a fixed random seed for reproducibility across multiple libraries\n",
    "random_seed = 42\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_632366/3687639064.py:14: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:275.)\n",
      "  lr_train_data_vectorized = torch.tensor([MatrixVectorizer.anti_vectorize(row, 160) for row in lr_train_data],\n"
     ]
    }
   ],
   "source": [
    "# load csvs as numpy\n",
    "lr_data_path = './data/lr_train.csv'\n",
    "hr_data_path = './data/hr_train.csv'\n",
    "\n",
    "lr_train_data = pd.read_csv(lr_data_path, delimiter=',').to_numpy()\n",
    "hr_train_data = pd.read_csv(hr_data_path, delimiter=',').to_numpy()\n",
    "lr_train_data[lr_train_data < 0] = 0\n",
    "np.nan_to_num(lr_train_data, copy=False)\n",
    "\n",
    "hr_train_data[hr_train_data < 0] = 0\n",
    "np.nan_to_num(hr_train_data, copy=False)\n",
    "\n",
    "# map the anti-vectorize function to each row of the lr_train_data\n",
    "lr_train_data_vectorized = torch.tensor([MatrixVectorizer.anti_vectorize(row, 160) for row in lr_train_data],\n",
    "                                        dtype=torch.float32)\n",
    "hr_train_data_vectorized = torch.tensor([MatrixVectorizer.anti_vectorize(row, 268) for row in hr_train_data],\n",
    "                                        dtype=torch.float32)\n",
    "\n",
    "splits, (lr_test_data,hr_test_data) = load_random_files(return_matrix=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = NoisyDataset(lr_train_data_vectorized, hr_train_data_vectorized, noise_level=0.5)\n",
    "train_data_loader = torch.utils.data.DataLoader(train_data, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with comments showing the values for our grid search\n",
    "num_splt = 3\n",
    "epochs = 200\n",
    "lr = 0.00005 # try [0.0001, 0.0005, 0.00001, 0.00005]\n",
    "lmbda = 17 # should be around 15-20\n",
    "lamdba_topo = 0.0005 # should be around 0.0001-0.001\n",
    "lr_dim = 160\n",
    "hr_dim = 320\n",
    "hidden_dim = 320 # try smaller and larger - [160-512]\n",
    "padding = 26\n",
    "dropout = 0.2 # try [0., 0.1, 0.2, 0.3]\n",
    "\n",
    "\n",
    "args = argparse.Namespace()\n",
    "args.epochs = epochs\n",
    "args.lr = lr\n",
    "args.lmbda = lmbda\n",
    "args.lamdba_topo = lamdba_topo\n",
    "args.lr_dim = lr_dim\n",
    "args.hr_dim = hr_dim\n",
    "args.hidden_dim = hidden_dim\n",
    "args.padding = padding\n",
    "args.p = dropout\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Model & Kaggle Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1 Loss:  0.45927190227422887 Error:  0.20969157554432302 Topo:  36.410682860962645\n",
      "Epoch:  2 Loss:  0.3102445595278711 Error:  0.18255760373469598 Topo:  20.425317592963488\n",
      "Epoch:  3 Loss:  0.3022257846272634 Error:  0.17880466553622376 Topo:  19.412191602284324\n",
      "Epoch:  4 Loss:  0.29694903450097865 Error:  0.17759281218408823 Topo:  19.119499400704207\n",
      "Epoch:  5 Loss:  0.29097256385637615 Error:  0.17600396784122832 Topo:  18.684219988520272\n",
      "Epoch:  6 Loss:  0.28645330560421517 Error:  0.17544859995742043 Topo:  18.958074090009678\n",
      "Epoch:  7 Loss:  0.2823514826818854 Error:  0.17473038504580538 Topo:  18.991527511688048\n",
      "Epoch:  8 Loss:  0.27717077509014904 Error:  0.17393922654097665 Topo:  18.707939056579225\n",
      "Epoch:  9 Loss:  0.2734783270401869 Error:  0.17317594730568503 Topo:  18.943735282578153\n",
      "Epoch:  10 Loss:  0.2698545825338649 Error:  0.17259178529242555 Topo:  18.894377245874463\n",
      "Epoch:  11 Loss:  0.26590690150589286 Error:  0.17149784384730332 Topo:  18.638007775038304\n",
      "Epoch:  12 Loss:  0.26265180708762415 Error:  0.1705990599657961 Topo:  18.846258860148357\n",
      "Epoch:  13 Loss:  0.2590246369203408 Error:  0.16939080778710142 Topo:  18.49984507075327\n",
      "Epoch:  14 Loss:  0.2567969997842869 Error:  0.16874451551608696 Topo:  18.72156100929854\n",
      "Epoch:  15 Loss:  0.25389175322241414 Error:  0.16771206250804627 Topo:  18.732354004226046\n",
      "Epoch:  16 Loss:  0.2510267497001294 Error:  0.16651777994489955 Topo:  18.6758561048679\n",
      "Epoch:  17 Loss:  0.24884271710932612 Error:  0.165496689741483 Topo:  18.63664206773221\n",
      "Epoch:  18 Loss:  0.24643084906532378 Error:  0.16434535036186973 Topo:  18.615833442368192\n",
      "Epoch:  19 Loss:  0.2447077768469999 Error:  0.163555779678379 Topo:  18.656868797576355\n",
      "Epoch:  20 Loss:  0.2427647642925114 Error:  0.16256561919957577 Topo:  18.650084758233167\n",
      "Epoch:  21 Loss:  0.24120120124188726 Error:  0.16173229942064798 Topo:  18.628748385492198\n",
      "Epoch:  22 Loss:  0.23999853057418755 Error:  0.16087551468503689 Topo:  18.743510942973064\n",
      "Epoch:  23 Loss:  0.23850119417299054 Error:  0.160068123729643 Topo:  18.5844832894331\n",
      "Epoch:  24 Loss:  0.2370924387506382 Error:  0.15925874164004525 Topo:  18.60588717888929\n",
      "Epoch:  25 Loss:  0.2359244878777487 Error:  0.1584732860862138 Topo:  18.475844046312893\n",
      "Epoch:  26 Loss:  0.23513184693045244 Error:  0.1580683620211607 Topo:  18.66745596445963\n",
      "Epoch:  27 Loss:  0.2339854299308297 Error:  0.15721367834927794 Topo:  18.505092889248967\n",
      "Epoch:  28 Loss:  0.23331000000059962 Error:  0.1569726385042339 Topo:  18.499789106631706\n",
      "Epoch:  29 Loss:  0.2324562705384043 Error:  0.15643342972515586 Topo:  18.56774219353042\n",
      "Epoch:  30 Loss:  0.23177523322091131 Error:  0.15596494988766973 Topo:  18.527214307271077\n",
      "Epoch:  31 Loss:  0.231313446413971 Error:  0.15566120443943732 Topo:  18.599925332440588\n",
      "Epoch:  32 Loss:  0.23027789637357174 Error:  0.15505951866061388 Topo:  18.47286610974523\n",
      "Epoch:  33 Loss:  0.22968289978846818 Error:  0.15468057847308542 Topo:  18.46994121345931\n",
      "Epoch:  34 Loss:  0.22906121620518005 Error:  0.15451416801549717 Topo:  18.515794565577707\n",
      "Epoch:  35 Loss:  0.2284970284400586 Error:  0.15399865636568583 Topo:  18.53540169264742\n",
      "Epoch:  36 Loss:  0.22835008532344225 Error:  0.15397191832879345 Topo:  18.582621071866885\n",
      "Epoch:  37 Loss:  0.22760335985058083 Error:  0.15364786652390827 Topo:  18.4871997147977\n",
      "Epoch:  38 Loss:  0.2268751929263155 Error:  0.1531688805647239 Topo:  18.417446262108353\n",
      "Epoch:  39 Loss:  0.2268529300561208 Error:  0.15326332778273943 Topo:  18.58013733275636\n",
      "Epoch:  40 Loss:  0.22636012732982635 Error:  0.1529245609027183 Topo:  18.55160246637767\n",
      "Epoch:  41 Loss:  0.2257410826797257 Error:  0.15266777066413514 Topo:  18.454796014431707\n",
      "Epoch:  42 Loss:  0.22502299429413802 Error:  0.15225155892486344 Topo:  18.45393332178721\n",
      "Epoch:  43 Loss:  0.22483017353597515 Error:  0.15228425037718105 Topo:  18.419067399944375\n",
      "Epoch:  44 Loss:  0.22445285534430406 Error:  0.15218549770509412 Topo:  18.575952912519078\n",
      "Epoch:  45 Loss:  0.22383518668705832 Error:  0.1520055270569767 Topo:  18.452001154779673\n",
      "Epoch:  46 Loss:  0.22326637411902764 Error:  0.15155659062776736 Topo:  18.393321671171815\n",
      "Epoch:  47 Loss:  0.22314279593393474 Error:  0.15164339207782002 Topo:  18.440254177162032\n",
      "Epoch:  48 Loss:  0.22279910047254162 Error:  0.15147630914956509 Topo:  18.45302328235375\n",
      "Epoch:  49 Loss:  0.22230768034201182 Error:  0.15126917988597277 Topo:  18.36977432730669\n",
      "Epoch:  50 Loss:  0.2220965730037518 Error:  0.15125019754656774 Topo:  18.513803225077556\n",
      "Epoch:  51 Loss:  0.22172084477490295 Error:  0.1511463651846269 Topo:  18.424034872454797\n",
      "Epoch:  52 Loss:  0.22132398250574123 Error:  0.15088950061869477 Topo:  18.334963181775485\n",
      "Epoch:  53 Loss:  0.22124291795813394 Error:  0.15099520669012012 Topo:  18.434681127171316\n",
      "Epoch:  54 Loss:  0.2207191562402748 Error:  0.15081294425233396 Topo:  18.419331442096276\n",
      "Epoch:  55 Loss:  0.220388465834235 Error:  0.15074311299416834 Topo:  18.445217520890836\n",
      "Epoch:  56 Loss:  0.22015196174204707 Error:  0.1506154957021068 Topo:  18.43502172024664\n",
      "Epoch:  57 Loss:  0.2194676468829195 Error:  0.15031045046216712 Topo:  18.352796571697304\n",
      "Epoch:  58 Loss:  0.21943944763994502 Error:  0.1504694568718265 Topo:  18.446532043868196\n",
      "Epoch:  59 Loss:  0.21923486754565896 Error:  0.15040409297286395 Topo:  18.416825562893987\n",
      "Epoch:  60 Loss:  0.21901181179606272 Error:  0.15035940925339739 Topo:  18.43905134258156\n",
      "Epoch:  61 Loss:  0.21875287235497 Error:  0.15034954642464302 Topo:  18.418829523874614\n",
      "Epoch:  62 Loss:  0.21830313898132234 Error:  0.15011327472810973 Topo:  18.41808682858587\n",
      "Epoch:  63 Loss:  0.2182946284552534 Error:  0.150168231147492 Topo:  18.449361487063104\n",
      "Epoch:  64 Loss:  0.21797103135885593 Error:  0.15013736633661978 Topo:  18.44051197189057\n",
      "Epoch:  65 Loss:  0.2175313526998737 Error:  0.1500139906110164 Topo:  18.425472225257735\n",
      "Epoch:  66 Loss:  0.2171179771780254 Error:  0.1498180192833889 Topo:  18.358153200435066\n",
      "Epoch:  67 Loss:  0.2170249783171865 Error:  0.1500031142356153 Topo:  18.560106871370785\n",
      "Epoch:  68 Loss:  0.21648353150861707 Error:  0.14965738382881988 Topo:  18.27605209807436\n",
      "Epoch:  69 Loss:  0.21641043194396767 Error:  0.14963957662889343 Topo:  18.36333628900037\n",
      "Epoch:  70 Loss:  0.21620804999402898 Error:  0.14973778482861147 Topo:  18.375731274039445\n",
      "Epoch:  71 Loss:  0.21590579020049044 Error:  0.14962104248429486 Topo:  18.40601180413526\n",
      "Epoch:  72 Loss:  0.21595372482688127 Error:  0.14978914065453822 Topo:  18.478091713911045\n",
      "Epoch:  73 Loss:  0.2153015928175635 Error:  0.1494835355056974 Topo:  18.288630502666543\n",
      "Epoch:  74 Loss:  0.21559372544288635 Error:  0.1497632129463607 Topo:  18.548362994622327\n",
      "Epoch:  75 Loss:  0.21496705585967996 Error:  0.14939874785389015 Topo:  18.40096165891179\n",
      "Epoch:  76 Loss:  0.2145318372877772 Error:  0.1492171972811579 Topo:  18.334335458492806\n",
      "Epoch:  77 Loss:  0.2146428319329987 Error:  0.14942360022467768 Topo:  18.36572163404819\n",
      "Epoch:  78 Loss:  0.21415355248365572 Error:  0.14926653283977223 Topo:  18.248502502898255\n",
      "Epoch:  79 Loss:  0.21411867975117918 Error:  0.14925563348802978 Topo:  18.3791292156288\n",
      "Epoch:  80 Loss:  0.21375806701040553 Error:  0.14915883532184326 Topo:  18.34293065099659\n",
      "Epoch:  81 Loss:  0.21392559639351097 Error:  0.14937304205701737 Topo:  18.393491436621385\n",
      "Epoch:  82 Loss:  0.21334810340832808 Error:  0.1491059237076137 Topo:  18.279713270906917\n",
      "Epoch:  83 Loss:  0.21312248180369417 Error:  0.1491162970126746 Topo:  18.31981741739604\n",
      "Epoch:  84 Loss:  0.21300776204663122 Error:  0.1491183859859398 Topo:  18.310822972280537\n",
      "Epoch:  85 Loss:  0.21271097570836187 Error:  0.14898968858276299 Topo:  18.242879096619383\n",
      "Epoch:  86 Loss:  0.21283263735428543 Error:  0.14923694645037908 Topo:  18.34548555591149\n",
      "Epoch:  87 Loss:  0.21233651800426895 Error:  0.14889836315801758 Topo:  18.307152239862315\n",
      "Epoch:  88 Loss:  0.21211409194026878 Error:  0.14879712355351019 Topo:  18.29213136684395\n",
      "Epoch:  89 Loss:  0.21205508173582796 Error:  0.14882857066963961 Topo:  18.31309580660152\n",
      "Epoch:  90 Loss:  0.21200925513298927 Error:  0.14897741252433755 Topo:  18.325529081378868\n",
      "Epoch:  91 Loss:  0.21184887250740372 Error:  0.14897260141229915 Topo:  18.37986425296989\n",
      "Epoch:  92 Loss:  0.2115240487866773 Error:  0.14882314807462121 Topo:  18.377944974842187\n",
      "Epoch:  93 Loss:  0.2111946237301398 Error:  0.14867309815512447 Topo:  18.227299170579737\n",
      "Epoch:  94 Loss:  0.2113170547042778 Error:  0.1488818351826268 Topo:  18.40039160722744\n",
      "Epoch:  95 Loss:  0.21087030807655013 Error:  0.1486373840424115 Topo:  18.267120081507517\n",
      "Epoch:  96 Loss:  0.21091284446730585 Error:  0.14875275940595273 Topo:  18.3354223673929\n",
      "Epoch:  97 Loss:  0.21058838872495525 Error:  0.14859524244319894 Topo:  18.27962688628785\n",
      "Epoch:  98 Loss:  0.2107373869526172 Error:  0.14879391324555802 Topo:  18.391897527043692\n",
      "Epoch:  99 Loss:  0.21018109394761617 Error:  0.14853650639335553 Topo:  18.29022152837879\n",
      "Epoch:  100 Loss:  0.21029710867804682 Error:  0.14868501689798103 Topo:  18.256749164558457\n",
      "Epoch:  101 Loss:  0.21026553481281873 Error:  0.14879550949899023 Topo:  18.385473205657775\n",
      "Epoch:  102 Loss:  0.20983455709354606 Error:  0.14849333143876697 Topo:  18.301742770714675\n",
      "Epoch:  103 Loss:  0.2097713054118756 Error:  0.14853121701650276 Topo:  18.31781955536254\n",
      "Epoch:  104 Loss:  0.20967335294106762 Error:  0.1485331059840625 Topo:  18.270154045013612\n",
      "Epoch:  105 Loss:  0.20985535277934844 Error:  0.14873573358008962 Topo:  18.373047788699942\n",
      "Epoch:  106 Loss:  0.20928974574554465 Error:  0.14850590864341415 Topo:  18.224160382847586\n",
      "Epoch:  107 Loss:  0.2092985177111483 Error:  0.14855199775652972 Topo:  18.294165074468374\n",
      "Epoch:  108 Loss:  0.20908672358104569 Error:  0.14847810419198282 Topo:  18.30411385062212\n",
      "Epoch:  109 Loss:  0.20869432657421705 Error:  0.14827036826375 Topo:  18.1899475451715\n",
      "Epoch:  110 Loss:  0.2085182664101709 Error:  0.14827532887815714 Topo:  18.19508188213417\n",
      "Epoch:  111 Loss:  0.2087172832853066 Error:  0.14845641912100557 Topo:  18.325043021561857\n",
      "Epoch:  112 Loss:  0.20833614513188778 Error:  0.14824599808383132 Topo:  18.243892441252747\n",
      "Epoch:  113 Loss:  0.20847830916950089 Error:  0.14837589824271058 Topo:  18.375992866333373\n",
      "Epoch:  114 Loss:  0.2082860609014591 Error:  0.14835021952669064 Topo:  18.271328749057062\n",
      "Epoch:  115 Loss:  0.20832433538165634 Error:  0.14842325680983995 Topo:  18.272797356108704\n",
      "Epoch:  116 Loss:  0.2079596485563381 Error:  0.1483130123979317 Topo:  18.267384403480026\n",
      "Epoch:  117 Loss:  0.20811388342680331 Error:  0.14840121741244894 Topo:  18.334772663915942\n",
      "Epoch:  118 Loss:  0.2077523269160779 Error:  0.14819296150507327 Topo:  18.288006148652403\n",
      "Epoch:  119 Loss:  0.2077127516626598 Error:  0.14825135201751116 Topo:  18.299754936538058\n",
      "Epoch:  120 Loss:  0.2075624055491236 Error:  0.1483535324028152 Topo:  18.309896925966182\n",
      "Epoch:  121 Loss:  0.20748892230188062 Error:  0.14821648829711412 Topo:  18.237435951918187\n",
      "Epoch:  122 Loss:  0.20718871521021792 Error:  0.14816814727947383 Topo:  18.235580415782813\n",
      "Epoch:  123 Loss:  0.20732118662245974 Error:  0.14825243261938323 Topo:  18.326768738067077\n",
      "Epoch:  124 Loss:  0.2070843376620801 Error:  0.1481561673258593 Topo:  18.26403640130323\n",
      "Epoch:  125 Loss:  0.20698346938201767 Error:  0.14824330726426518 Topo:  18.247337386993593\n",
      "Epoch:  126 Loss:  0.20717774474335288 Error:  0.14841739712896462 Topo:  18.3493176477398\n",
      "Epoch:  127 Loss:  0.20703086524666425 Error:  0.14834291983150436 Topo:  18.330020293504177\n",
      "Epoch:  128 Loss:  0.2063345016833551 Error:  0.14786472680147536 Topo:  18.20912939060234\n",
      "Epoch:  129 Loss:  0.20678589301194975 Error:  0.14835647324066675 Topo:  18.40061152338268\n",
      "Epoch:  130 Loss:  0.20664305101611657 Error:  0.14817331552862406 Topo:  18.27646616404642\n",
      "Epoch:  131 Loss:  0.20633372155849092 Error:  0.14801178189987194 Topo:  18.141038866100196\n",
      "Epoch:  132 Loss:  0.2061412469177189 Error:  0.14797940595956618 Topo:  18.23668097307582\n",
      "Epoch:  133 Loss:  0.2062025846478468 Error:  0.14815281014777942 Topo:  18.228735158543387\n",
      "Epoch:  134 Loss:  0.20590366681892716 Error:  0.14780866430548137 Topo:  18.127675627519984\n",
      "Epoch:  135 Loss:  0.2061012151712429 Error:  0.1481787834517256 Topo:  18.33824024086227\n",
      "Epoch:  136 Loss:  0.20598414799053513 Error:  0.14818781181545315 Topo:  18.313354857667477\n",
      "Epoch:  137 Loss:  0.20608578952486645 Error:  0.148261190263811 Topo:  18.36494319024914\n",
      "Epoch:  138 Loss:  0.20569356508597642 Error:  0.14797486757446907 Topo:  18.1986051776452\n",
      "Epoch:  139 Loss:  0.20571665251683333 Error:  0.1480765432387055 Topo:  18.2937019827837\n",
      "Epoch:  140 Loss:  0.20575945239937948 Error:  0.1481662603195556 Topo:  18.31700769869867\n",
      "Epoch:  141 Loss:  0.20554479546175747 Error:  0.1481062558864405 Topo:  18.264595254452644\n",
      "Epoch:  142 Loss:  0.20523818988286094 Error:  0.14793184145303542 Topo:  18.245507971255364\n",
      "Epoch:  143 Loss:  0.2054288563971034 Error:  0.14812819949702588 Topo:  18.36562394810294\n",
      "Epoch:  144 Loss:  0.20522373379347567 Error:  0.1479584232061923 Topo:  18.252121634112147\n",
      "Epoch:  145 Loss:  0.20522492713557033 Error:  0.14801971627745086 Topo:  18.28684781696982\n",
      "Epoch:  146 Loss:  0.20510094822523836 Error:  0.14797178178490278 Topo:  18.223847857492412\n",
      "Epoch:  147 Loss:  0.2051781344556523 Error:  0.14811899040095106 Topo:  18.347666557677492\n",
      "Epoch:  148 Loss:  0.20473656950596564 Error:  0.14786510829797048 Topo:  18.238572046428384\n",
      "Epoch:  149 Loss:  0.20519529497195146 Error:  0.14824301513012297 Topo:  18.447726752229794\n",
      "Epoch:  150 Loss:  0.20468934380008788 Error:  0.1479076900614236 Topo:  18.264661366354204\n",
      "Epoch:  151 Loss:  0.20477182833020557 Error:  0.1480366600576989 Topo:  18.321969026576973\n",
      "Epoch:  152 Loss:  0.20451273755755967 Error:  0.14785189310947577 Topo:  18.235305637656573\n",
      "Epoch:  153 Loss:  0.20448976291153959 Error:  0.14791085718277686 Topo:  18.242963014248602\n",
      "Epoch:  154 Loss:  0.2043647324432156 Error:  0.14786552865348176 Topo:  18.271651570668478\n",
      "Epoch:  155 Loss:  0.20429296827244903 Error:  0.14783154999067682 Topo:  18.207772272075722\n",
      "Epoch:  156 Loss:  0.2042308888749448 Error:  0.14801788646839337 Topo:  18.31008123066611\n",
      "Epoch:  157 Loss:  0.20419804618030252 Error:  0.14789054020793138 Topo:  18.19583457101605\n",
      "Epoch:  158 Loss:  0.20415543224997149 Error:  0.14781079548383189 Topo:  18.22943184903996\n",
      "Epoch:  159 Loss:  0.20422836986487497 Error:  0.14799718295564196 Topo:  18.328700168404037\n",
      "Epoch:  160 Loss:  0.20365606069921732 Error:  0.14766707873629953 Topo:  18.23984733170378\n",
      "Epoch:  161 Loss:  0.20404404035942283 Error:  0.1480212740733952 Topo:  18.2834013008072\n",
      "Epoch:  162 Loss:  0.2036415959546666 Error:  0.14781217022748763 Topo:  18.26899300078432\n",
      "Epoch:  163 Loss:  0.20384428440453764 Error:  0.14800418092462117 Topo:  18.252445723482236\n",
      "Epoch:  164 Loss:  0.20386149479957397 Error:  0.1479671545728238 Topo:  18.30214612355489\n",
      "Epoch:  165 Loss:  0.20315935520711773 Error:  0.14753634928764697 Topo:  18.06550019087192\n",
      "Epoch:  166 Loss:  0.20389456752531543 Error:  0.14805929648305127 Topo:  18.434032771401778\n",
      "Epoch:  167 Loss:  0.203444990390789 Error:  0.14780101572682044 Topo:  18.285617205911056\n",
      "Epoch:  168 Loss:  0.20355519545292425 Error:  0.14781188581161156 Topo:  18.264714086840968\n",
      "Epoch:  169 Loss:  0.20337527463892976 Error:  0.14777695088686343 Topo:  18.31299960541868\n",
      "Epoch:  170 Loss:  0.20335775952853127 Error:  0.14782566849343076 Topo:  18.344647578850477\n",
      "Epoch:  171 Loss:  0.2032610147656081 Error:  0.14784609410398736 Topo:  18.277247446025918\n",
      "Epoch:  172 Loss:  0.20315515709494403 Error:  0.1477063636847599 Topo:  18.23227816142008\n",
      "Epoch:  173 Loss:  0.20323896220701185 Error:  0.14782034699431437 Topo:  18.272963586681616\n",
      "Epoch:  174 Loss:  0.20297227808815277 Error:  0.1476280280126783 Topo:  18.20673494281883\n",
      "Epoch:  175 Loss:  0.2027557426227067 Error:  0.14758937475745548 Topo:  18.201384664295677\n",
      "Epoch:  176 Loss:  0.20294137111680952 Error:  0.1476836903188043 Topo:  18.211878508150935\n",
      "Epoch:  177 Loss:  0.2029687507959183 Error:  0.14781859218181964 Topo:  18.27362375202293\n",
      "Epoch:  178 Loss:  0.20282917527738445 Error:  0.14775744571300323 Topo:  18.19883070163384\n",
      "Epoch:  179 Loss:  0.20271185736456318 Error:  0.14764364621417966 Topo:  18.211654069180973\n",
      "Epoch:  180 Loss:  0.20265176053532583 Error:  0.14761645320111405 Topo:  18.19772215517695\n",
      "Epoch:  181 Loss:  0.2026582788564488 Error:  0.14772857114404975 Topo:  18.25813030197235\n",
      "Epoch:  182 Loss:  0.20247633781975616 Error:  0.14766029839565653 Topo:  18.219166544383157\n",
      "Epoch:  183 Loss:  0.20259405360250415 Error:  0.1477227137831157 Topo:  18.279666158253562\n",
      "Epoch:  184 Loss:  0.2022930596046105 Error:  0.14755152397883867 Topo:  18.204757416319705\n",
      "Epoch:  185 Loss:  0.20261347802456267 Error:  0.14778794089477218 Topo:  18.279969015521203\n",
      "Epoch:  186 Loss:  0.20254436572511753 Error:  0.14771627135084062 Topo:  18.353124167390927\n",
      "Epoch:  187 Loss:  0.20213120774237694 Error:  0.1475135760303743 Topo:  18.212398420550866\n",
      "Epoch:  188 Loss:  0.20233177576593298 Error:  0.14769119430266456 Topo:  18.242217183826927\n",
      "Epoch:  189 Loss:  0.20245873963761474 Error:  0.14774605457832712 Topo:  18.27948394935288\n",
      "Epoch:  190 Loss:  0.2020646361355296 Error:  0.14751648791357427 Topo:  18.230062416213716\n",
      "Epoch:  191 Loss:  0.20225911243946967 Error:  0.1476056580236572 Topo:  18.321414576319164\n",
      "Epoch:  192 Loss:  0.20186863768243504 Error:  0.14750256237691034 Topo:  18.201273598356874\n",
      "Epoch:  193 Loss:  0.2019373717422257 Error:  0.14748252562420097 Topo:  18.194634460403535\n",
      "Epoch:  194 Loss:  0.2020194736604919 Error:  0.1476778028878623 Topo:  18.28089948756966\n",
      "Epoch:  195 Loss:  0.20186271888767174 Error:  0.14756174755845955 Topo:  18.206122004343364\n",
      "Epoch:  196 Loss:  0.20208109344194036 Error:  0.14766205459119316 Topo:  18.325460696648694\n",
      "Epoch:  197 Loss:  0.2017354535128542 Error:  0.14752395785675793 Topo:  18.180012051930685\n",
      "Epoch:  198 Loss:  0.20187614902764736 Error:  0.14752808654915073 Topo:  18.27890741039893\n",
      "Epoch:  199 Loss:  0.20169076216435003 Error:  0.14749625633041302 Topo:  18.179468217724096\n",
      "Epoch:  200 Loss:  0.20179423573845162 Error:  0.14758925755580743 Topo:  18.29505624028737\n"
     ]
    }
   ],
   "source": [
    "#final train\n",
    "ks = [0.9, 0.7, 0.6, 0.5]\n",
    "criterion = nn.L1Loss()\n",
    "scores = []\n",
    "\n",
    "for lr_train_data, hr_train_data, lr_val_data, hr_val_data in splits:\n",
    "    final_model = GSRNet(ks, args)\n",
    "    final_model.to(device)\n",
    "    optimizer = optim.Adam(final_model.parameters(), lr=args.lr)\n",
    "    train_data = NoisyDataset(lr_train_data, hr_train_data, noise_level=0.5)\n",
    "    train_data_loader = torch.utils.data.DataLoader(train_data, batch_size=1, shuffle=True) \n",
    "    val_data = NoisyDataset(lr_val_data, hr_val_data, noise_level=0)\n",
    "    train_val_loader = torch.utils.data.DataLoader(val_data, batch_size=1) \n",
    "    \n",
    "\n",
    "    train(final_model, train_data_loader, optimizer, criterion, args)\n",
    "\n",
    "    preds, loss = validate(final_model,train_val_loader, criterion, args)\n",
    "    scores.append(evaluate_all(hr_val_data,preds))\n",
    "\n",
    "print(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename = 'final-model.sav'\n",
    "pickle.dump(final_model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_submission_csv(final_model, args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
