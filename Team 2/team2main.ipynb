{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Library\n",
    "import argparse\n",
    "import random\n",
    "\n",
    "# Third-Party Libraries\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "# Custom Modules\n",
    "from MatrixVectorizer import *\n",
    "from dataloaders import NoisyDataset\n",
    "from model import *\n",
    "from preprocessing import *\n",
    "from train import *\n",
    "from evaluation import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a fixed random seed for reproducibility across multiple libraries\n",
    "random_seed = 42\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load csvs as numpy\n",
    "lr_data_path = './data/lr_train.csv'\n",
    "hr_data_path = './data/hr_train.csv'\n",
    "\n",
    "lr_train_data = pd.read_csv(lr_data_path, delimiter=',').to_numpy()\n",
    "hr_train_data = pd.read_csv(hr_data_path, delimiter=',').to_numpy()\n",
    "lr_train_data[lr_train_data < 0] = 0\n",
    "np.nan_to_num(lr_train_data, copy=False)\n",
    "\n",
    "hr_train_data[hr_train_data < 0] = 0\n",
    "np.nan_to_num(hr_train_data, copy=False)\n",
    "\n",
    "# map the anti-vectorize function to each row of the lr_train_data\n",
    "lr_train_data_vectorized = torch.tensor([MatrixVectorizer.anti_vectorize(row, 160) for row in lr_train_data],\n",
    "                                        dtype=torch.float32)\n",
    "hr_train_data_vectorized = torch.tensor([MatrixVectorizer.anti_vectorize(row, 268) for row in hr_train_data],\n",
    "                                        dtype=torch.float32)\n",
    "\n",
    "splits, (lr_test_data,hr_test_data) = load_random_files(return_matrix=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = NoisyDataset(lr_train_data_vectorized, hr_train_data_vectorized, noise_level=0.5)\n",
    "train_data_loader = torch.utils.data.DataLoader(train_data, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with comments showing the values for our grid search\n",
    "num_splt = 3\n",
    "epochs = 200\n",
    "lr = 0.00005 # try [0.0001, 0.0005, 0.00001, 0.00005]\n",
    "lmbda = 17 # should be around 15-20\n",
    "lamdba_topo = 0.0005 # should be around 0.0001-0.001\n",
    "lr_dim = 160\n",
    "hr_dim = 320\n",
    "hidden_dim = 320 # try smaller and larger - [160-512]\n",
    "padding = 26\n",
    "dropout = 0.2 # try [0., 0.1, 0.2, 0.3]\n",
    "\n",
    "\n",
    "args = argparse.Namespace()\n",
    "args.epochs = epochs\n",
    "args.lr = lr\n",
    "args.lmbda = lmbda\n",
    "args.lamdba_topo = lamdba_topo\n",
    "args.lr_dim = lr_dim\n",
    "args.hr_dim = hr_dim\n",
    "args.hidden_dim = hidden_dim\n",
    "args.padding = padding\n",
    "args.p = dropout\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Model & Kaggle Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final train\n",
    "ks = [0.9, 0.7, 0.6, 0.5]\n",
    "criterion = nn.L1Loss()\n",
    "scores = []\n",
    "\n",
    "for lr_train_data, hr_train_data, lr_val_data, hr_val_data in splits:\n",
    "    final_model = GSRNet(ks, args)\n",
    "    final_model.to(device)\n",
    "    optimizer = optim.Adam(final_model.parameters(), lr=args.lr)\n",
    "    train_data = NoisyDataset(lr_train_data, hr_train_data, noise_level=0.5)\n",
    "    train_data_loader = torch.utils.data.DataLoader(train_data, batch_size=1, shuffle=True) \n",
    "    val_data = NoisyDataset(lr_val_data, hr_val_data, noise_level=0)\n",
    "    train_val_loader = torch.utils.data.DataLoader(val_data, batch_size=1) \n",
    "    \n",
    "\n",
    "    train(final_model, train_data_loader, optimizer, criterion, args)\n",
    "\n",
    "    preds, loss = validate(final_model,train_val_loader, criterion, args)\n",
    "    scores.append(evaluate_all(hr_val_data,preds))\n",
    "\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename = 'final-model.sav'\n",
    "pickle.dump(final_model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_submission_csv(final_model, args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
