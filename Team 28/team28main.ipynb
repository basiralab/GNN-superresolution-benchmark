{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "247dcf7e5b89bd5d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# DGL Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31da158ca771d29f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3817a8e4a127eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-16T11:07:42.988139Z",
     "start_time": "2024-05-16T11:07:40.449311Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from MatrixVectorizer import *\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from preprocessing import *\n",
    "from evaluation import *\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "import pandas as pd\n",
    "from model import *\n",
    "from train import *\n",
    "import psutil\n",
    "import time \n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e587cb33a996c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-16T11:07:43.009735Z",
     "start_time": "2024-05-16T11:07:42.989501Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set a fixed random seed for reproducibility across multiple libraries\n",
    "random_seed = 42\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "# Check for CUDA (GPU support) and set device accordingly\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"CUDA is available. Using GPU.\")\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed)  # For multi-GPU setups\n",
    "    # Additional settings for ensuring reproducibility on CUDA\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDA not available. Using CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c216698bd79a661f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a02b2fd6cdbd861",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-16T11:07:43.025088Z",
     "start_time": "2024-05-16T11:07:43.008517Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def anti_vectorize_samples(dataset, dim):\n",
    "    \"\"\"\n",
    "    Anti-vectorizes each sample in the dataset using MatrixVectorizer.\n",
    "\n",
    "    Args:\n",
    "    - dataset (np.ndarray): Dataset of samples in vectorized form of shape \n",
    "        (num_samples, vectorized form size).\n",
    "    - dim (int): Size of the anti-vectorized matrix (number of rows/columns).\n",
    "\n",
    "    Returns:\n",
    "    - np.ndarray: Anti-vectorized matrices of shape (num_samples, dim, dim).\n",
    "    \"\"\"\n",
    "    num_samples = dataset.shape[0]\n",
    "    dataset_matrices = np.empty((num_samples, dim, dim))\n",
    "    for i in range(num_samples):\n",
    "        dataset_matrices[i] = MatrixVectorizer.anti_vectorize(dataset[i,:], dim, include_diagonal=False)\n",
    "\n",
    "    return dataset_matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76a23a59d6fd7ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-16T11:07:54.115116Z",
     "start_time": "2024-05-16T11:07:43.012325Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read in the training data\n",
    "lr_train_split1 = np.genfromtxt(\"./new_data/lr_split_1.csv\", delimiter=\",\", skip_header=1)\n",
    "lr_train_split2 = np.genfromtxt(\"./new_data/lr_split_2.csv\", delimiter=\",\", skip_header=1)\n",
    "lr_train_split3 = np.genfromtxt(\"./new_data/lr_split_3.csv\", delimiter=\",\", skip_header=1)\n",
    "hr_train_split1 = np.genfromtxt(\"./new_data/hr_split_1.csv\", delimiter=\",\", skip_header=1)\n",
    "hr_train_split2 = np.genfromtxt(\"./new_data/hr_split_2.csv\", delimiter=\",\", skip_header=1)\n",
    "hr_train_split3 = np.genfromtxt(\"./new_data/hr_split_3.csv\", delimiter=\",\", skip_header=1)\n",
    "\n",
    "# Antivectorize\n",
    "lr_train_split1 = anti_vectorize_samples(lr_train_split1, 160)\n",
    "lr_train_split2 = anti_vectorize_samples(lr_train_split2, 160)\n",
    "lr_train_split3 = anti_vectorize_samples(lr_train_split3, 160)\n",
    "hr_train_split1 = anti_vectorize_samples(hr_train_split1, 268)\n",
    "hr_train_split2 = anti_vectorize_samples(hr_train_split2, 268)\n",
    "hr_train_split3 = anti_vectorize_samples(hr_train_split3, 268)\n",
    "\n",
    "lr_train_matrices = [lr_train_split1, lr_train_split2, lr_train_split3]\n",
    "hr_train_matrices = [hr_train_split1, hr_train_split2, hr_train_split3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378e72172a8524c2",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3-fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c1db26280c9855",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-16T11:07:54.126274Z",
     "start_time": "2024-05-16T11:07:54.122374Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def predict(model, lr_matrices, args):\n",
    "    \"\"\"\n",
    "    Returns predictions returned from model and low-resolution matrices.\n",
    "\n",
    "    Args:\n",
    "    - model (torch.nn.Module): trained model for making predictions.\n",
    "    - lr_matrices (list of np.ndarray): List of low-resolution matrices.\n",
    "    - args (dict): Additional arguments including padding information.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Array of predictions for each low-resolution matrix.\n",
    "    \"\"\"\n",
    "    preds_matrices = [] # To store predictions\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for lr in lr_matrices:\n",
    "            lr = torch.from_numpy(lr).type(torch.FloatTensor)\n",
    "            preds, _, _, _ = model(lr)\n",
    "            preds = unpad(preds, args['padding'])\n",
    "            preds_matrices.append(preds.detach().cpu().numpy())\n",
    "\n",
    "    return np.array(preds_matrices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2a22caaa261c9a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-16T11:07:54.129815Z",
     "start_time": "2024-05-16T11:07:54.127643Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_metrics(metrics, title=\"Metrics\"):\n",
    "    \"\"\"\n",
    "    Plot metrics with individual folds and average across folds.\n",
    "\n",
    "    Args:\n",
    "    - metrics (dict): Dictionary containing metric names as keys and lists of \n",
    "        metric values for each fold as values.\n",
    "    - title (str, optional): Title of the plot. Defaults to \"Metrics\".\n",
    "    \"\"\"\n",
    "    # Create a custom color palette (you can adjust the colors as needed)\n",
    "    colors = ['#FF5733', '#3498DB', '#27AE60', '#F39C12', '#9B59B6', '#7D3C98']\n",
    "\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(10, 8))  # 2 rows, 2 columns\n",
    "    plt.subplots_adjust(hspace=0.5)\n",
    "\n",
    "    # Plot individual metrics\n",
    "    for i in range(3):\n",
    "        # Get the ith value for each metric\n",
    "        metric_values = [values[i] for values in metrics.values()]\n",
    "        metric_names = list(metrics.keys())\n",
    "        row = i // 2\n",
    "        col = i % 2\n",
    "        ax = axs[row, col]\n",
    "\n",
    "        ax.bar(range(len(metric_values)), metric_values, align='center', color=colors)\n",
    "        ax.set_xticks(range(len(metric_values)), metric_names, rotation=45)\n",
    "        ax.set_xticklabels(metric_names, fontdict={'rotation': 45})\n",
    "        ax.set_title(f'Fold {i+1}')\n",
    "\n",
    "    # Final plot with average values\n",
    "    ax = axs[-1, -1]  # Select the bottom right subplot\n",
    "    avg_values = [np.mean(values) for values in zip(metrics.values())]\n",
    "    std_values = [np.std(values) for values in zip(metrics.values())]\n",
    "    ax.bar(range(len(metrics)), avg_values, align='center', color=colors)\n",
    "    ax.errorbar(range(len(metrics)), avg_values, yerr=std_values, fmt='o', color='black', capsize=5, label='Average', elinewidth=1)\n",
    "    ax.set_xticks(range(len(metric_values)), metric_names, rotation=45)\n",
    "    ax.set_xticklabels(metric_names, fontdict={'rotation': 45})\n",
    "    ax.set_title(\"Avg. Across Folds\")\n",
    "\n",
    "    plt.suptitle(title)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"./images/barplots.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ff190c8abdef1c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-16T11:07:54.133485Z",
     "start_time": "2024-05-16T11:07:54.131254Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def save_preds_to_csv(preds_matrices, fold_num):\n",
    "    \"\"\"\n",
    "    Save predictions to predictions_fold_<fold_num>.csv\n",
    "\n",
    "    Args:\n",
    "    - preds_matrices (list of np.ndarray): List of prediction matrices.\n",
    "    - fold_num (int): Fold number.\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    for pred in preds_matrices:\n",
    "        # Vectorize\n",
    "        predictions.append(MatrixVectorizer.vectorize(pred, include_diagonal=False))\n",
    "\n",
    "    # Flatten to 1D\n",
    "    predictions = np.array(predictions)\n",
    "    preds_flattened = predictions.flatten()\n",
    "\n",
    "    df = pd.DataFrame(columns=['ID', 'Predicted'])\n",
    "\n",
    "    df['ID'] = np.arange(1, len(preds_flattened) + 1)\n",
    "    df['Predicted'] = preds_flattened\n",
    "\n",
    "    df.to_csv(f\"predictions_fold_{fold_num}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9fcebb0ae7badc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-16T11:07:54.164463Z",
     "start_time": "2024-05-16T11:07:54.137181Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_kfold(lr_train_matrices, hr_train_matrices, ks, args):\n",
    "    start_time = time.time()  # Record the start time for training\n",
    "\n",
    "    process = psutil.Process(os.getpid())\n",
    "    base_memory_usage = process.memory_info().rss\n",
    "    print(f\"Base RAM usage: {base_memory_usage/(1024*1024)} MiB\")\n",
    "    \n",
    "    #concatenate \n",
    "    fold1_lr = [np.concatenate((lr_train_matrices[0], lr_train_matrices[1]), axis=0), lr_train_matrices[2]]\n",
    "    fold2_lr = [np.concatenate((lr_train_matrices[1], lr_train_matrices[2]), axis=0), lr_train_matrices[0]]\n",
    "    fold3_lr = [np.concatenate((lr_train_matrices[0], lr_train_matrices[2]), axis=0), lr_train_matrices[1]]\n",
    "    folds_lr = [fold1_lr, fold2_lr, fold3_lr]\n",
    "    \n",
    "    fold1_hr = [np.concatenate((hr_train_matrices[0], hr_train_matrices[1]), axis=0), hr_train_matrices[2]]\n",
    "    fold2_hr = [np.concatenate((hr_train_matrices[1], hr_train_matrices[2]), axis=0), hr_train_matrices[0]]\n",
    "    fold3_hr = [np.concatenate((hr_train_matrices[0], hr_train_matrices[2]), axis=0), hr_train_matrices[1]]\n",
    "    folds_hr = [fold1_hr, fold2_hr, fold3_hr]\n",
    "    \n",
    "    for i in range(3):\n",
    "        print(f\"\\nFold {i+1}\")\n",
    "        model = EAGSRNet(ks, args)\n",
    "\n",
    "        # Get train-test split\n",
    "        train_lr = folds_lr[i][0]\n",
    "        train_hr = folds_hr[i][0]\n",
    "\n",
    "        test_lr = folds_lr[i][1]\n",
    "        gt_matrices = folds_hr[i][1]\n",
    "        \n",
    "        # Train and test the model\n",
    "        train_with_early_stopping(model, train_lr, train_hr,\n",
    "                                  test_lr, gt_matrices, args) \n",
    "\n",
    "        preds_matrices = predict(model, test_lr, args)\n",
    "        \n",
    "        fold_metrics = evaluate_all(preds_matrices, gt_matrices)\n",
    "\n",
    "    # Calculate total training time in minutes\n",
    "    total_training_time = (time.time() - start_time) / 60\n",
    "    print(f\"Total Training Time for 3F-CF: {total_training_time} minutes\")\n",
    "\n",
    "    # Report total RAM usage\n",
    "    memory_usage = (process.memory_info().rss - base_memory_usage)/(1024*1024)\n",
    "    print(f\"Total RAM used: {memory_usage} MiB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f8993561f7a2df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-16T11:07:54.169873Z",
     "start_time": "2024-05-16T11:07:54.140333Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "# ks is the top percentile that will be sampled at each pooling layer\n",
    "ks = [0.9, 0.7, 0.6, 0.5]\n",
    "args = {\n",
    "    'epochs': 200,\n",
    "    'lr': 0.0001,\n",
    "    'lmbda': 0.1,\n",
    "    'lr_dim': 160,\n",
    "    'hr_dim': 320, # to account for padding this is hr_dim + padding * 2\n",
    "    'hidden_dim': 320,\n",
    "    'padding': 26,\n",
    "    'mean_dense': 0.,\n",
    "    'std_dense': 0.01,\n",
    "    'mean_gaussian': 0.,\n",
    "    'std_gaussian': 0.1, \n",
    "    'zero_penalty': 2.0,\n",
    "    'device': device,\n",
    "    'early_stopping_threshold': 0.0001,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14da4d6f576af1da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-16T13:12:33.692747Z",
     "start_time": "2024-05-16T11:07:54.142623Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "run_kfold(lr_train_matrices, hr_train_matrices, ks, args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
