{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up\n",
    "Before running all codes at once, please make sure all directory settings in this code match with your environment. You can ctr+f \"directory\" to find all directory setting that you need to change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-03-07T16:26:57.058151Z",
     "iopub.status.busy": "2024-03-07T16:26:57.057685Z",
     "iopub.status.idle": "2024-03-07T16:27:10.397018Z",
     "shell.execute_reply": "2024-03-07T16:27:10.396096Z",
     "shell.execute_reply.started": "2024-03-07T16:26:57.058110Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import psutil\n",
    "import time \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "import networkx as nx\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-07T16:27:10.398593Z",
     "iopub.status.busy": "2024-03-07T16:27:10.398109Z",
     "iopub.status.idle": "2024-03-07T16:27:20.371058Z",
     "shell.execute_reply": "2024-03-07T16:27:20.370205Z",
     "shell.execute_reply.started": "2024-03-07T16:27:10.398565Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "hr_df = pd.read_csv(\"/notebooks/hr_train.csv\") # change the directory if needed\n",
    "lr_df = pd.read_csv(\"/notebooks/lr_train.csv\")\n",
    "\n",
    "# Convert to numpy arrays\n",
    "hr_data = hr_df.values\n",
    "lr_data = lr_df.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Distirbution Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-07T16:27:20.374701Z",
     "iopub.status.busy": "2024-03-07T16:27:20.374033Z",
     "iopub.status.idle": "2024-03-07T16:27:21.347640Z",
     "shell.execute_reply": "2024-03-07T16:27:21.346704Z",
     "shell.execute_reply.started": "2024-03-07T16:27:20.374662Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Overlayed histograms for both datasets\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Flatten the data to 1D for histogram plotting of non zero values\n",
    "lr_flat = np.ravel(lr_data[lr_data != 0])\n",
    "hr_flat = np.ravel(hr_data[hr_data != 0])\n",
    "\n",
    "# Calculate the number of bins for the histogram\n",
    "bins = np.histogram(np.hstack((lr_flat, hr_flat)), bins=40)[1] # get the bin edges\n",
    "\n",
    "plt.hist(lr_flat, bins=bins, color='blue', alpha=0.3, label='Low Resolution', density=False)\n",
    "plt.hist(hr_flat, bins=bins, color='green', alpha=0.3, label='High Resolution', density=False)\n",
    "\n",
    "plt.title('Data Distribution of Low and High Resolution Datasets')\n",
    "plt.xlabel('Feature Values')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization & Antivectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-07T16:27:21.349554Z",
     "iopub.status.busy": "2024-03-07T16:27:21.349124Z",
     "iopub.status.idle": "2024-03-07T16:27:21.364088Z",
     "shell.execute_reply": "2024-03-07T16:27:21.362916Z",
     "shell.execute_reply.started": "2024-03-07T16:27:21.349517Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class MatrixVectorizer:\n",
    "    \"\"\"\n",
    "    A class for transforming between matrices and vector representations.\n",
    "    \n",
    "    This class provides methods to convert a symmetric matrix into a vector (vectorize)\n",
    "    and to reconstruct the matrix from its vector form (anti_vectorize), focusing on \n",
    "    vertical (column-based) traversal and handling of elements.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes the MatrixVectorizer instance.\n",
    "        \n",
    "        The constructor currently does not perform any actions but is included for \n",
    "        potential future extensions where initialization parameters might be required.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def vectorize(matrix, include_diagonal=False):\n",
    "        \"\"\"\n",
    "        Converts a matrix into a vector by vertically extracting elements.\n",
    "        \n",
    "        This method traverses the matrix column by column, collecting elements from the\n",
    "        upper triangle, and optionally includes the diagonal elements immediately below\n",
    "        the main diagonal based on the include_diagonal flag.\n",
    "        \n",
    "        Parameters:\n",
    "        - matrix (numpy.ndarray): The matrix to be vectorized.\n",
    "        - include_diagonal (bool, optional): Flag to include diagonal elements in the vectorization.\n",
    "          Defaults to False.\n",
    "        \n",
    "        Returns:\n",
    "        - numpy.ndarray: The vectorized form of the matrix.\n",
    "        \"\"\"\n",
    "        # Determine the size of the matrix based on its first dimension\n",
    "        matrix_size = matrix.shape[0]\n",
    "\n",
    "        # Initialize an empty list to accumulate vector elements\n",
    "        vector_elements = []\n",
    "\n",
    "        # Iterate over columns and then rows to collect the relevant elements\n",
    "        for col in range(matrix_size):\n",
    "            for row in range(matrix_size):\n",
    "                # Skip diagonal elements if not including them\n",
    "                if row != col:  \n",
    "                    if row < col:\n",
    "                        # Collect upper triangle elements\n",
    "                        vector_elements.append(matrix[row, col])\n",
    "                    elif include_diagonal and row == col + 1:\n",
    "                        # Optionally include the diagonal elements immediately below the diagonal\n",
    "                        vector_elements.append(matrix[row, col])\n",
    "\n",
    "        return np.array(vector_elements)\n",
    "\n",
    "    @staticmethod\n",
    "    def anti_vectorize(vector, matrix_size, include_diagonal=False):\n",
    "        \"\"\"\n",
    "        Reconstructs a matrix from its vector form, filling it vertically.\n",
    "        \n",
    "        The method fills the matrix by reflecting vector elements into the upper triangle\n",
    "        and optionally including the diagonal elements based on the include_diagonal flag.\n",
    "        \n",
    "        Parameters:\n",
    "        - vector (numpy.ndarray): The vector to be transformed into a matrix.\n",
    "        - matrix_size (int): The size of the square matrix to be reconstructed.\n",
    "        - include_diagonal (bool, optional): Flag to include diagonal elements in the reconstruction.\n",
    "          Defaults to False.\n",
    "        \n",
    "        Returns:\n",
    "        - numpy.ndarray: The reconstructed square matrix.\n",
    "        \"\"\"\n",
    "        # Initialize a square matrix of zeros with the specified size\n",
    "        matrix = np.zeros((matrix_size, matrix_size))\n",
    "\n",
    "        # Index to keep track of the current position in the vector\n",
    "        vector_idx = 0\n",
    "\n",
    "        # Fill the matrix by iterating over columns and then rows\n",
    "        for col in range(matrix_size):\n",
    "            for row in range(matrix_size):\n",
    "                # Skip diagonal elements if not including them\n",
    "                if row != col:  \n",
    "                    if row < col:\n",
    "                        # Reflect vector elements into the upper triangle and its mirror in the lower triangle\n",
    "                        matrix[row, col] = vector[vector_idx]\n",
    "                        matrix[col, row] = vector[vector_idx]\n",
    "                        vector_idx += 1\n",
    "                    elif include_diagonal and row == col + 1:\n",
    "                        # Optionally fill the diagonal elements after completing each column\n",
    "                        matrix[row, col] = vector[vector_idx]\n",
    "                        matrix[col, row] = vector[vector_idx]\n",
    "                        vector_idx += 1\n",
    "\n",
    "        return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-07T16:27:21.366192Z",
     "iopub.status.busy": "2024-03-07T16:27:21.365668Z",
     "iopub.status.idle": "2024-03-07T16:27:21.381963Z",
     "shell.execute_reply": "2024-03-07T16:27:21.380883Z",
     "shell.execute_reply.started": "2024-03-07T16:27:21.366145Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_data_as_graphs(df, matrix_size):\n",
    "    graphs = []\n",
    "    for index, row in df.iterrows():\n",
    "        vector = row.values\n",
    "        adjacency_matrix = MatrixVectorizer.anti_vectorize(vector, matrix_size)\n",
    "        graph = Data(adjacency_matrix=np.array(adjacency_matrix))\n",
    "        graphs.append(graph)\n",
    "    return graphs\n",
    "\n",
    "def get_matrix_size(num_features):\n",
    "    return int(np.sqrt(num_features * 2)) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data for Training, Testing, and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-07T16:27:21.383732Z",
     "iopub.status.busy": "2024-03-07T16:27:21.383384Z",
     "iopub.status.idle": "2024-03-07T16:27:31.465815Z",
     "shell.execute_reply": "2024-03-07T16:27:31.464854Z",
     "shell.execute_reply.started": "2024-03-07T16:27:21.383701Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Determine the matrix size for low-resolution and high-resolution data\n",
    "lr_matrix_size = get_matrix_size(lr_df.shape[1])\n",
    "hr_matrix_size = get_matrix_size(hr_df.shape[1])\n",
    "\n",
    "# Convert the dataframe to graph objects\n",
    "lr_graphs = load_data_as_graphs(lr_df, lr_matrix_size)\n",
    "hr_graphs = load_data_as_graphs(hr_df, hr_matrix_size)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "lr_train, lr_test, hr_train, hr_test = train_test_split(\n",
    "    lr_graphs, hr_graphs, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-07T16:27:31.467370Z",
     "iopub.status.busy": "2024-03-07T16:27:31.467057Z",
     "iopub.status.idle": "2024-03-07T16:27:31.473474Z",
     "shell.execute_reply": "2024-03-07T16:27:31.472438Z",
     "shell.execute_reply.started": "2024-03-07T16:27:31.467343Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"LR Train Set samples: \",len(lr_train))\n",
    "print(\"LR Test Set samples: \",len(lr_test))\n",
    "\n",
    "print(\"HR Train Set samples: \",len(hr_train))\n",
    "print(\"HR Test Set samples: \",len(lr_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-07T16:27:31.475702Z",
     "iopub.status.busy": "2024-03-07T16:27:31.475134Z",
     "iopub.status.idle": "2024-03-07T16:27:32.064710Z",
     "shell.execute_reply": "2024-03-07T16:27:32.063553Z",
     "shell.execute_reply.started": "2024-03-07T16:27:31.475659Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# convert list to tensor\n",
    "lr_train_array = np.random.normal(0.5, 1,(len(lr_train), 160, 160))\n",
    "hr_train_array = np.random.normal(0.5, 1,(len(hr_train), 268, 268))\n",
    "lr_test_array = np.random.normal(0.5, 1,(len(lr_test), 160, 160))\n",
    "hr_test_array = np.random.normal(0.5, 1,(len(hr_test), 268, 268))\n",
    "i = 0\n",
    "\n",
    "for lr, hr in zip(lr_train, hr_train):\n",
    "    lr_train_array[i] = lr.adjacency_matrix\n",
    "    hr_train_array[i] = hr.adjacency_matrix\n",
    "    i +=1\n",
    "\n",
    "j = 0\n",
    "for lr, hr in zip(lr_test, hr_test):\n",
    "    lr_test_array[j] = lr.adjacency_matrix\n",
    "    hr_test_array[j] = hr.adjacency_matrix\n",
    "    i +=1\n",
    "\n",
    "print(lr_train_array.shape)\n",
    "print(lr_test_array.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LL-GSR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-07T16:27:32.068839Z",
     "iopub.status.busy": "2024-03-07T16:27:32.068532Z",
     "iopub.status.idle": "2024-03-07T16:27:32.080939Z",
     "shell.execute_reply": "2024-03-07T16:27:32.079819Z",
     "shell.execute_reply.started": "2024-03-07T16:27:32.068814Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "## Helper Functions\n",
    "# Initialize a weight matrix\n",
    "def weight_variable_glorot(output_dim):\n",
    "    input_dim = output_dim\n",
    "    init_range = np.sqrt(6.0 / (input_dim + output_dim))\n",
    "    initial = np.random.uniform(-init_range, init_range,(input_dim, output_dim))\n",
    "    return initial\n",
    "\n",
    "# Normalize the adjacency matrix of the graph\n",
    "def normalize_adj_torch(mx):\n",
    "    rowsum = mx.sum(1)\n",
    "    r_inv_sqrt = torch.pow(rowsum, -0.5).flatten()\n",
    "    r_inv_sqrt[torch.isinf(r_inv_sqrt)] = 0.\n",
    "    r_mat_inv_sqrt = torch.diag(r_inv_sqrt)\n",
    "    mx = torch.matmul(mx, r_mat_inv_sqrt)\n",
    "    mx = torch.transpose(mx, 0, 1)\n",
    "    mx = torch.matmul(mx,  r_mat_inv_sqrt)\n",
    "    return mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GAT\n",
    "class GAT(nn.Module):\n",
    "    \"\"\" \n",
    "    This layer applies an attention mechanism in the graph convolution process,\n",
    "    allowing the model to focus on different parts of the neighborhood\n",
    "    of each node.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features, activation=F.relu):\n",
    "        \"\"\"        \n",
    "        Parameters:\n",
    "            in_features (int): The number of features of each input node.\n",
    "            out_features (int): The number of features for each output node.\n",
    "            activation (callable, optional): The activation function to use. Default is F.relu.\n",
    "        \"\"\"\n",
    "\n",
    "        super(GAT, self).__init__()\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        self.bias = nn.Parameter(torch.zeros(out_features))\n",
    "        self.phi = nn.Parameter(torch.FloatTensor(2 * out_features, 1))\n",
    "        self.activation = activation\n",
    "        self.reset_parameters()\n",
    "        self.drop = nn.Dropout(p=0.5)\n",
    " \n",
    "    def reset_parameters(self):\n",
    "        \"\"\"\n",
    "        Initializes or resets the parameters of the layer.\n",
    "        \"\"\"\n",
    "\n",
    "        stdv = 1. / np.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    " \n",
    "        stdv = 1. / np.sqrt(self.phi.size(1))\n",
    "        self.phi.data.uniform_(-stdv, stdv)\n",
    " \n",
    "    def forward(self,adj,input):\n",
    "        \"\"\"\n",
    "        Forward pass of the GAT layer.\n",
    " \n",
    "        Parameters:\n",
    "        input (Tensor): The input features of the nodes.\n",
    "        adj (Tensor): The adjacency matrix of the graph.\n",
    " \n",
    "        Returns:\n",
    "        Tensor: The output features of the nodes after applying the GAT layer.\n",
    "        \"\"\"\n",
    "        \n",
    "        input = self.drop(input)\n",
    "        h = torch.mm(input, self.weight) + self.bias \n",
    " \n",
    "        N = input.size(0) \n",
    "        h_expand = h.unsqueeze(1).expand(N, N, -1)\n",
    "        h_t_expand = h.unsqueeze(0).expand(N, N, -1)\n",
    "        \n",
    "        concat_features = torch.cat([h_expand, h_t_expand], dim=-1)\n",
    "        \n",
    "        S = torch.matmul(concat_features, self.phi).squeeze(-1)\n",
    " \n",
    "        mask = (adj.to(device) + torch.eye(adj.size(0),device=device)).bool()\n",
    "        S_masked = torch.where(mask, S, torch.tensor(-9e15, dtype=S.dtype).to(device))\n",
    "        attention_weights = F.softmax(S_masked, dim=1)\n",
    "        h = torch.matmul(attention_weights, h)\n",
    "        return self.activation(h) if self.activation else h\n",
    "    \n",
    "class GraphUnpool(nn.Module):\n",
    "    \"\"\"    \n",
    "    This layer \"unpools\" a graph to a larger graph based on the provided indices.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(GraphUnpool, self).__init__()\n",
    "\n",
    "    def forward(self, A, X, idx):\n",
    "        \"\"\"        \n",
    "        Parameters:\n",
    "            A (Tensor): The adjacency matrix of the smaller graph.\n",
    "            X (Tensor): The node features of the smaller graph.\n",
    "            idx (Tensor): The indices of nodes in the original graph.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor, Tensor: The adjacency matrix and node features of the unpooled graph.\n",
    "        \"\"\"\n",
    "\n",
    "        new_X = torch.zeros([A.shape[0], X.shape[1]]).to(device)\n",
    "        new_X[idx] = X\n",
    "        return A, new_X\n",
    "\n",
    "    \n",
    "class GraphPool(nn.Module):\n",
    "    \"\"\"    \n",
    "    This layer pools a graph based on the learned scores for each node, reducing the number of nodes in the graph.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, k, in_dim):\n",
    "        super(GraphPool, self).__init__()\n",
    "        self.k = k\n",
    "        self.proj = nn.Linear(in_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, A, X):\n",
    "        scores = self.proj(X)\n",
    "        scores = torch.squeeze(scores)\n",
    "        scores = self.sigmoid(scores/100)\n",
    "        num_nodes = A.shape[0]\n",
    "        values, idx = torch.topk(scores, int(self.k*num_nodes))\n",
    "        new_X = X[idx, :]\n",
    "        values = torch.unsqueeze(values, -1)\n",
    "        new_X = torch.mul(new_X, values)\n",
    "        A = A[idx, :]\n",
    "        A = A[:, idx]\n",
    "        return A, new_X, idx\n",
    "\n",
    "\n",
    "class GraphUnet(nn.Module):\n",
    "    \"\"\"    \n",
    "    This model combines the GAT layers with graph pooling and unpooling layers to create an architecture for graphs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ks, in_dim, out_dim, dim=268):\n",
    "        super(GraphUnet, self).__init__()\n",
    "        self.ks = ks\n",
    "        self.start_gat = GAT(in_dim, dim).to(device)\n",
    "        self.bottom_gat = GAT(dim, dim).to(device)\n",
    "        self.end_gat = GAT(2*dim, out_dim).to(device)\n",
    "        self.down_gats = []\n",
    "        self.up_gats = []\n",
    "        self.pools = []\n",
    "        self.unpools = []\n",
    "        self.l_n = len(ks)\n",
    "        for i in range(self.l_n):\n",
    "            self.down_gats.append(GAT(dim, dim).to(device))\n",
    "            self.up_gats.append(GAT(dim, dim).to(device))\n",
    "            self.pools.append(GraphPool(ks[i], dim).to(device))\n",
    "            self.unpools.append(GraphUnpool().to(device))\n",
    "\n",
    "    def forward(self, A, X):\n",
    "        adj_ms = []\n",
    "        indices_list = []\n",
    "        down_outs = []\n",
    "        X = self.start_gat(A, X)\n",
    "        start_gat_outs = X\n",
    "        org_X = X\n",
    "        for i in range(self.l_n):\n",
    "            X = self.down_gats[i](A, X)\n",
    "            adj_ms.append(A)\n",
    "            down_outs.append(X)\n",
    "            A, X, idx = self.pools[i](A, X)\n",
    "            indices_list.append(idx)\n",
    "        \n",
    "        X = self.bottom_gat(A, X)\n",
    "        \n",
    "        for i in range(self.l_n):\n",
    "            up_idx = self.l_n - i - 1\n",
    "            A, idx = adj_ms[up_idx], indices_list[up_idx]\n",
    "            A, X = self.unpools[i](A, X, idx)\n",
    "            X = self.up_gats[i](A, X)\n",
    "            X = X.add(down_outs[up_idx])\n",
    "        X = torch.cat([X, org_X], 1)\n",
    "        X = self.end_gat(A, X)\n",
    "        \n",
    "        return X, start_gat_outs\n",
    "\n",
    "\n",
    "class GSRLayer(nn.Module):\n",
    "    \"\"\"    \n",
    "    This layer aims to learn a high-resolution representation of a graph from its low-resolution counterpart.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,hr_dim):\n",
    "        super(GSRLayer, self).__init__()\n",
    "        \n",
    "        self.hr_dim = hr_dim\n",
    "        self.weights = torch.from_numpy(weight_variable_glorot(hr_dim)).type(torch.FloatTensor).to(device)\n",
    "        self.weights = torch.nn.Parameter(data=self.weights, requires_grad = True).to(device)\n",
    "\n",
    "    def forward(self,A,X):\n",
    "        lr = A\n",
    "        lr_dim = lr.shape[0]\n",
    "        hr_dim = self.hr_dim\n",
    "        f = X\n",
    "        eig_val_lr, U_lr = torch.linalg.eigh(lr, UPLO='U')\n",
    "        eye_mat = torch.eye(lr_dim).type(torch.FloatTensor).to(device)\n",
    "        s_d = torch.cat((eye_mat, torch.ones(hr_dim - lr_dim, lr_dim).to(device)), dim=0)\n",
    "        a = torch.matmul(self.weights,s_d)\n",
    "        b = torch.matmul(a ,torch.t(U_lr))\n",
    "        f_d = torch.matmul(b ,f)\n",
    "        f_d = torch.abs(f_d)\n",
    "        self.f_d = f_d.fill_diagonal_(1)\n",
    "        adj = normalize_adj_torch(self.f_d)\n",
    "        X = torch.mm(adj, adj.t())\n",
    "        X = (X + X.t())/2\n",
    "        idx = torch.eye(268, dtype=bool)\n",
    "        X[idx]=1\n",
    "        return adj, torch.abs(X)\n",
    "    \n",
    "\n",
    "class GraphConvolution(nn.Module):\n",
    "    \"\"\"    \n",
    "    This layer applies a graph convolution operation on the graph nodes.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, dropout=0.5, act=F.relu):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.dropout = dropout\n",
    "        self.act = act\n",
    "        self.weight = torch.nn.Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        torch.nn.init.xavier_uniform_(self.weight)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        input = F.dropout(input, self.dropout, self.training)\n",
    "        support = torch.mm(input, self.weight)\n",
    "        output = torch.mm(adj, support)\n",
    "        output = self.act(output)\n",
    "        return output\n",
    "    \n",
    "    \n",
    "class GSRNet(nn.Module):\n",
    "    \"\"\"\n",
    "    This network combines graph convolutional layers with a Graph U-Net structure and a super-resolution layer to enhance the resolution of graph data.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,ks,args):\n",
    "        super(GSRNet, self).__init__()\n",
    "\n",
    "        self.lr_dim = args.lr_dim\n",
    "        self.hr_dim = args.hr_dim\n",
    "        self.hidden_dim = args.hidden_dim\n",
    "        self.layer = GSRLayer(self.hr_dim).to(device)\n",
    "        self.net = GraphUnet(ks, self.lr_dim, self.hr_dim).to(device)\n",
    "        self.gc1 = GraphConvolution(self.hr_dim, self.hidden_dim, 0, act=F.relu).to(device)\n",
    "        self.gc2 = GraphConvolution(self.hidden_dim, self.hr_dim, 0, act=F.relu).to(device)\n",
    "\n",
    "    def forward(self,lr):\n",
    "        I = torch.eye(self.lr_dim).type(torch.FloatTensor).to(device)\n",
    "        A = normalize_adj_torch(lr).type(torch.FloatTensor).to(device)\n",
    "\n",
    "        self.net_outs, self.start_gat_outs = self.net(A, I)\n",
    "        self.outputs, self.Z = self.layer(A, self.net_outs)\n",
    "\n",
    "        self.hidden1 = self.gc1(self.Z, self.outputs)\n",
    "        self.hidden2 = self.gc2(self.hidden1, self.outputs)\n",
    "\n",
    "        z = self.hidden2\n",
    "        z = (z + z.t())/2\n",
    "        idx = torch.eye(self.hr_dim, dtype=bool) \n",
    "        z[idx]=1\n",
    "\n",
    "        return torch.abs(z), self.net_outs, self.start_gat_outs, self.outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a fixed random seed for reproducibility across multiple libraries\n",
    "random_seed = 42\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "# Check for CUDA (GPU support) and set device accordingly\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"CUDA is available. Using GPU.\")\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed)  # For multi-GPU setups\n",
    "    # Additional settings for ensuring reproducibility on CUDA\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDA not available. Using CPU.\")\n",
    "\n",
    "def get_memory_usage():\n",
    "    process = psutil.Process()\n",
    "    return process.memory_info().rss / (1024 ** 2)  # Convert to MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "criterion.to(device)\n",
    "\n",
    "def create_batches(data, batch_size):\n",
    "    for i in range(0, len(data), batch_size):\n",
    "        yield data[i:i + batch_size]\n",
    "\n",
    "def train(model, optimizer, subjects_adj, subjects_labels, args):\n",
    "    \"\"\"\n",
    "    Trains a graph super-resolution network on a dataset of low-resolution and high-resolution adjacency matrices.\n",
    "    \n",
    "    Parameters:\n",
    "        model: The graph super-resolution network to be trained.\n",
    "        optimizer: The optimizer used to update the model's weights.\n",
    "        subjects_adj: A list of low-resolution adjacency matrices (the dataset).\n",
    "        subjects_labels: A list of high-resolution adjacency matrices (the labels).\n",
    "        args: A namespace or an object containing training parameters such as epochs and batch_size.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize training variables\n",
    "    i = 0\n",
    "    all_epochs_loss = []\n",
    "    no_epochs = args.epochs\n",
    "    memory_usage_list = []\n",
    "    batch_size = args.batch_size  # Assuming batch_size is added to Args class\n",
    "\n",
    "    # Record start time\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(no_epochs):\n",
    "        epoch_loss = []\n",
    "        epoch_error = []\n",
    "\n",
    "        # Create batches\n",
    "        lr_batches = list(create_batches(subjects_adj, batch_size))\n",
    "        hr_batches = list(create_batches(subjects_labels, batch_size))\n",
    "        \n",
    "        for lr_batch, hr_batch in zip(lr_batches, hr_batches):\n",
    "            model.train()\n",
    "            optimizer.zero_grad()   \n",
    "\n",
    "            # Initialize batch_loss as a zero tensor that requires grad\n",
    "            batch_loss = torch.tensor(0., device=device, requires_grad=True)\n",
    "            batch_error = 0\n",
    "\n",
    "            # Process each (lr, hr) pair in the batch\n",
    "            for lr, hr in zip(lr_batch, hr_batch):\n",
    "                lr_tensor = torch.from_numpy(lr).type(torch.FloatTensor).to(device)\n",
    "                hr_tensor = torch.from_numpy(hr).type(torch.FloatTensor).to(device)  \n",
    "                model_outputs, net_outs, start_gat_outs, layer_outs = model(lr_tensor)\n",
    "                \n",
    "                hr_p = hr_tensor.cpu()\n",
    "                eig_val_hr, U_hr = torch.linalg.eigh(hr_p.to(device), UPLO='U')\n",
    "\n",
    "                # loss function\n",
    "                loss = args.lmbda1 * criterion(net_outs, start_gat_outs) + criterion(model.layer.weights,U_hr) + args.lmbda2*criterion(model_outputs, hr_tensor) \n",
    "                error = criterion(model_outputs, hr_tensor)\n",
    "\n",
    "                if batch_loss.grad_fn is not None:\n",
    "                    batch_loss = batch_loss + loss\n",
    "                else:\n",
    "                    batch_loss = loss.detach().requires_grad_()\n",
    " \n",
    "                batch_error += error.item()\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss.append(batch_loss.item())\n",
    "            epoch_error.append(batch_error / len(lr_batch))  # error\n",
    "      \n",
    "        i+=1\n",
    "        print(\"Epoch: \",i, \"Loss: \", np.mean(epoch_loss), \"Error: \", np.mean(epoch_error)*100,\"%\")\n",
    "        all_epochs_loss.append(np.mean(epoch_loss))\n",
    "\n",
    "        # calculate memory usage in MB\n",
    "        memory_usage = get_memory_usage()\n",
    "        memory_usage_list.append(memory_usage)\n",
    "        \n",
    "    end_time = time.time()\n",
    "    total_training_time = end_time - start_time\n",
    "\n",
    "    torch.save(model, args.model_path)\n",
    "    \n",
    "    # plot loss \n",
    "    plt.plot(all_epochs_loss)\n",
    "    plt.title('GSR-UNet with self reconstruction: Loss')\n",
    "    plt.show(block=False)\n",
    "    # Record end time\n",
    "   \n",
    "    print(f\"Total Training Time: {total_training_time:.2f} seconds\")\n",
    "    print(f\"\\n Avergae Memory Usage: {np.mean(memory_usage)} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    \"\"\"\n",
    "    A class to store the configuration settings for the training process.\n",
    "    \n",
    "    Attributes:\n",
    "        epochs (int): Number of epochs to train the model.\n",
    "        lr (float): Learning rate for the optimizer.\n",
    "        splits (int): Number of splits for cross-validation.\n",
    "        lmbda1 (int): Coefficient for the first part of the composite loss function.\n",
    "        lmbda2 (int): Coefficient for the second part of the composite loss function.\n",
    "        lr_dim (int): Dimension of the low-resolution graphs.\n",
    "        hr_dim (int): Dimension of the high-resolution graphs.\n",
    "        hidden_dim (int): Dimension of the hidden layers in the graph neural network.\n",
    "        batch_size (int): Number of samples per batch.\n",
    "        model_path (str): File path to save the trained model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.epochs = 200\n",
    "        self.lr = 0.0001\n",
    "        self.splits = 5\n",
    "        self.lmbda1 = 16\n",
    "        self.lmbda2 = 2\n",
    "        self.lr_dim = 160\n",
    "        self.hr_dim = 268\n",
    "        self.hidden_dim = 320\n",
    "        self.batch_size = 2\n",
    "        self.model_path = \"/notebooks/gsr_model.pt\" #change directory if needed\n",
    "\n",
    "args = Args()\n",
    "\n",
    "# Define the pooling ratios for the Graph U-Net architecture\n",
    "ks = [0.9, 0.7, 0.6, 0.5]\n",
    "\n",
    "model = GSRNet(ks, args) \n",
    "optimizer = optim.AdamW(model.parameters(), lr=args.lr)\n",
    "\n",
    "def count_parameters(model_check):\n",
    "    return sum(p.numel() for p in model_check.parameters() if p.requires_grad)\n",
    "print(model)\n",
    "\n",
    "model_params = count_parameters(model)\n",
    "print(f\"Model Parameters: {model_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_adj, test_labels, args):\n",
    "    \"\"\"    \n",
    "    Parameters:\n",
    "        model: The trained GSRNet model.\n",
    "        test_adj: Numpy array of adjacency matrices for the test dataset (low-resolution).\n",
    "        test_labels: Numpy array of ground truth adjacency matrices for the test dataset (high-resolution).\n",
    "        args: Configuration settings that include model and dataset specifications.\n",
    "    \n",
    "    Returns:\n",
    "        pred_matrices: Predicted high-resolution adjacency matrices for the test dataset.\n",
    "        gt_matrices: Ground truth high-resolution adjacency matrices for the test dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    test_error = []\n",
    "    preds_list=[]\n",
    "    g_t = []\n",
    "    num_test_samples = test_adj.shape[0]\n",
    "    num_roi = args.hr_dim\n",
    "    pred_matrices = torch.randn(num_test_samples, num_roi, num_roi).numpy()\n",
    "    gt_matrices = torch.randn(num_test_samples, num_roi, num_roi).numpy()\n",
    "\n",
    "    i = 0\n",
    "    \n",
    "    # Iterate over each pair of low-resolution input and high-resolution ground truth\n",
    "    for lr, hr in zip(test_adj,test_labels):\n",
    "\n",
    "        all_zeros_lr = not np.any(lr)\n",
    "        all_zeros_hr = not np.any(hr)\n",
    "\n",
    "        if all_zeros_lr == False and all_zeros_hr==False: #choose representative subject\n",
    "            lr = torch.from_numpy(lr).type(torch.FloatTensor).to(device)\n",
    "            np.fill_diagonal(hr,1)\n",
    "            hr = torch.from_numpy(hr).type(torch.FloatTensor).to(device)\n",
    "            preds,a,b,c = model(lr)\n",
    "\n",
    "            preds_list.append(preds.flatten().detach().cpu().clone().numpy())\n",
    "            error = criterion(preds, hr)\n",
    "            g_t.append(hr.flatten())\n",
    "            test_error.append(error.item())\n",
    "\n",
    "            pred_matrices[i] = preds.detach().cpu().clone().numpy()\n",
    "            gt_matrices[i]   = hr.detach().cpu().clone().numpy()\n",
    "            i += 1\n",
    "            \n",
    "    print (\"Test error MSE: \", np.mean(test_error))\n",
    "    return pred_matrices, gt_matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sample(pred_matrix, gt_matrix):\n",
    "    \"\"\"\n",
    "    Calculates the mean absolute error (MAE) between centrality measures of predicted and ground truth graphs.\n",
    "    \n",
    "    Parameters:\n",
    "        pred_matrix (numpy.ndarray): The predicted adjacency matrix of the graph.\n",
    "        gt_matrix (numpy.ndarray): The ground truth adjacency matrix of the graph.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: Contains MAE for betweenness centrality, eigenvector centrality, and PageRank.\n",
    "    \"\"\"\n",
    "\n",
    "    pred_graph = nx.from_numpy_array(pred_matrix)\n",
    "    gt_graph = nx.from_numpy_array(gt_matrix)\n",
    "    \n",
    "    # Calculate centrality measures for both predicted and ground truth graphs\n",
    "    pred_bc = nx.betweenness_centrality(pred_graph, weight=\"weight\")\n",
    "    pred_ec = nx.eigenvector_centrality(pred_graph, weight=\"weight\")\n",
    "    pred_pc = nx.pagerank(pred_graph, weight=\"weight\")\n",
    "\n",
    "    gt_bc = nx.betweenness_centrality(gt_graph, weight=\"weight\")\n",
    "    gt_ec = nx.eigenvector_centrality(gt_graph, weight=\"weight\")\n",
    "    gt_pc = nx.pagerank(gt_graph, weight=\"weight\")\n",
    "\n",
    "    # Convert centrality measures to lists for comparison\n",
    "    pred_bc_values = list(pred_bc.values())\n",
    "    pred_ec_values = list(pred_ec.values())\n",
    "    pred_pc_values = list(pred_pc.values())\n",
    "\n",
    "    gt_bc_values = list(gt_bc.values())\n",
    "    gt_ec_values = list(gt_ec.values())\n",
    "    gt_pc_values = list(gt_pc.values())\n",
    "\n",
    "    # Calculate MAE for each centrality measure\n",
    "    mae_bc = mean_absolute_error(pred_bc_values, gt_bc_values)\n",
    "    mae_ec = mean_absolute_error(pred_ec_values, gt_ec_values)\n",
    "    mae_pc = mean_absolute_error(pred_pc_values, gt_pc_values)\n",
    "    \n",
    "    return mae_bc, mae_ec, mae_pc\n",
    "\n",
    "def evaluation_parallel(pred_matrices, gt_matrices, num_test_samples, fold=1):\n",
    "    \"\"\"\n",
    "    Evaluates predicted graphs in parallel, calculating various error metrics and correlations.\n",
    "    \n",
    "    Parameters:\n",
    "        pred_matrices (numpy.ndarray): Array of predicted adjacency matrices.\n",
    "        gt_matrices (numpy.ndarray): Array of ground truth adjacency matrices.\n",
    "        num_test_samples (int): Number of test samples to evaluate.\n",
    "        fold (int): Current fold of cross-validation (for tracking purposes).\n",
    "    \n",
    "    Returns:\n",
    "        tuple: Contains overall MAE, Pearson correlation coefficient, Jensen-Shannon distance,\n",
    "               and average MAE for betweenness centrality, eigenvector centrality, and PageRank.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize lists to store MAEs for centrality measures\n",
    "    mae_bc_list = []\n",
    "    mae_ec_list = []\n",
    "    mae_pc_list = []\n",
    "    \n",
    "    # Evaluate each sample in parallel using ProcessPoolExecutor\n",
    "    with ProcessPoolExecutor() as executor:\n",
    "        futures = [executor.submit(process_sample, pred_matrices[i], gt_matrices[i]) for i in range(num_test_samples)]\n",
    "        for future in tqdm(as_completed(futures), total=num_test_samples, desc=\"Evaluating\"):\n",
    "            mae_bc, mae_ec, mae_pc = future.result()\n",
    "            mae_bc_list.append(mae_bc)\n",
    "            mae_ec_list.append(mae_ec)\n",
    "            mae_pc_list.append(mae_pc)\n",
    "\n",
    "    # Prepare data for vectorized operations\n",
    "    pred_1d_list = []\n",
    "    gt_1d_list = []\n",
    "\n",
    "    # Vectorize matrices and accumulate for all samples\n",
    "    for i in range(num_test_samples):\n",
    "        pred_1d_list.append(MatrixVectorizer.vectorize(pred_matrices[i]))\n",
    "        gt_1d_list.append(MatrixVectorizer.vectorize(gt_matrices[i]))\n",
    "        \n",
    "    # Compute average MAEs\n",
    "    avg_mae_bc = np.mean(mae_bc_list) \n",
    "    avg_mae_ec = np.mean(mae_ec_list)\n",
    "    avg_mae_pc = np.mean(mae_pc_list)\n",
    "        \n",
    "    # Concatenate flattened matrices\n",
    "    pred_1d = np.concatenate(pred_1d_list)\n",
    "    gt_1d = np.concatenate(gt_1d_list)\n",
    "\n",
    "    # Scalling Vectorization\n",
    "    scaler1 = MinMaxScaler()\n",
    "    scaler2 = MinMaxScaler()\n",
    "    pred_1d_norm = scaler1.fit_transform(pred_1d.reshape(-1, 1))\n",
    "    gt_1d_norm = scaler2.fit_transform(gt_1d.reshape(-1, 1))\n",
    "    pred_1d_norm = pred_1d_norm.reshape(-1)\n",
    "    gt_1d_norm = gt_1d_norm.reshape(-1)\n",
    "\n",
    "    # Compute metrics\n",
    "    mae = mean_absolute_error(pred_1d, gt_1d)\n",
    "    pcc = pearsonr(pred_1d, gt_1d)[0]\n",
    "    js_dis = jensenshannon(pred_1d_norm, gt_1d_norm)\n",
    "    \n",
    "    return mae, pcc, js_dis, avg_mae_bc, avg_mae_ec, avg_mae_pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_submission(filepath, data_array, fold=1, vector_dim=35778):\n",
    "    n_samples = data_array.shape[0]\n",
    "    results_array = np.zeros((n_samples, vector_dim))\n",
    "    i = 0\n",
    "\n",
    "    # Iterate over each predicted matrix, vectorize it, and store it in the results array\n",
    "    for preds in data_array:\n",
    "        vectorized_pred = MatrixVectorizer.vectorize(preds) \n",
    "        results_array[i, :] = vectorized_pred\n",
    "        i += 1\n",
    "    \n",
    "    melted_result = results_array.flatten()\n",
    "    ids = np.arange(1, melted_result.shape[0]+1)\n",
    "    df = pd.DataFrame({\n",
    "        \"ID\": ids,\n",
    "        \"Predicted\": melted_result\n",
    "    })\n",
    "\n",
    "    print(df.head())\n",
    "    df.to_csv(filepath, index=False)\n",
    "    print(\"the results are saved\")\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-07T16:17:23.521027Z",
     "iopub.status.busy": "2024-03-07T16:17:23.520646Z",
     "iopub.status.idle": "2024-03-07T16:17:43.146224Z",
     "shell.execute_reply": "2024-03-07T16:17:43.145195Z",
     "shell.execute_reply.started": "2024-03-07T16:17:23.520998Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "cv = KFold(n_splits=3, random_state=42, shuffle=True)\n",
    "fold = 0\n",
    "model.to(device)\n",
    "cv_filename = f\"predictions_fold_{fold}.csv\"\n",
    "cv_filepath = \"/notebooks/\" + cv_filename # change directory if needed\n",
    "\n",
    "# Combine all training data for 3f-CV\n",
    "lr_training = np.concatenate((lr_train_array, lr_test_array), axis=0)\n",
    "hr_training = np.concatenate((hr_train_array, hr_test_array), axis=0)\n",
    "\n",
    "# Initialize lists to store evaluation metrics across all folds\n",
    "mae_list = []\n",
    "pcc_list = []\n",
    "js_dis_list = []\n",
    "avg_mae_bc_list = []\n",
    "avg_mae_ec_list = []\n",
    "avg_mae_pc_list = []\n",
    "\n",
    "# Loop through each fold in the cross-validation\n",
    "for train_index, test_index in cv.split(lr_training):\n",
    "    fold +=1\n",
    "    len_train = len(train_index)\n",
    "    len_test = len(test_index)\n",
    "    print(f\"Fold {fold}, train split: {len_train} samples, test split: {len_test} samples\")\n",
    "\n",
    "    subjects_adj, test_adj, subjects_ground_truth, test_ground_truth = lr_training[\n",
    "        train_index], lr_training[test_index], hr_training[train_index], hr_training[test_index]\n",
    "    \n",
    "    train(model, optimizer, subjects_adj, subjects_ground_truth, args)\n",
    "    prediction, ground_truth = test(model, test_adj, test_ground_truth, args)\n",
    "    mae, pcc, js_dis, avg_mae_bc, avg_mae_ec, avg_mae_pc = evaluation_parallel(prediction, ground_truth, len_test, fold)\n",
    "\n",
    "    # Store the evaluation metrics for this fold\n",
    "    mae_list.append(mae)\n",
    "    pcc_list.append(pcc)\n",
    "    js_dis_list.append(js_dis)\n",
    "    avg_mae_bc_list.append(avg_mae_bc)\n",
    "    avg_mae_ec_list.append(avg_mae_ec)\n",
    "    avg_mae_pc_list.append(avg_mae_pc)\n",
    "\n",
    "    cv_submission(cv_filepath, prediction, fold)\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Calculate the mean and standard deviation of the evaluation metrics across all folds\n",
    "mae_fm = np.mean(mae_list)\n",
    "pcc_fm = np.mean(pcc_list)\n",
    "js_dis_fm = np.mean(js_dis_list)\n",
    "avg_mae_bc_fm = np.mean(avg_mae_bc_list)\n",
    "avg_mae_ec_fm = np.mean(avg_mae_ec_list)\n",
    "avg_mae_pc_fm = np.mean(avg_mae_pc_list)\n",
    "\n",
    "mae_fs = np.std(mae_list)\n",
    "pcc_fs = np.std(pcc_list)\n",
    "js_dis_fs = np.std(js_dis_list)\n",
    "avg_mae_bc_fs = np.std(avg_mae_bc_list)\n",
    "avg_mae_ec_fs = np.std(avg_mae_ec_list)\n",
    "avg_mae_pc_fs = np.std(avg_mae_pc_list)\n",
    "\n",
    "\n",
    "### Plot the graphs\n",
    "plt.figure(figsize=(24, 16))\n",
    "colors = ['red', 'green', 'blue', 'orange', 'purple', 'brown']\n",
    " \n",
    "# Loop through each fold to plot its evaluation metrics\n",
    "for i in range(3):\n",
    "    print(\"\\n Evaluation Metrics of Fold: \", i+1)\n",
    "    print(\"MAE: \", mae_list[i])\n",
    "    print(\"PCC: \", pcc_list[i])\n",
    "    print(\"Jensen-Shannon Distance: \", js_dis_list[i])\n",
    "    print(\"Average MAE betweenness centrality:\", avg_mae_bc_list[i])\n",
    "    print(\"Average MAE eigenvector centrality:\", avg_mae_ec_list[i])\n",
    "    print(\"Average MAE PageRank centrality:\", avg_mae_pc_list[i])\n",
    "    \n",
    "    plt.subplot(2, 2, i+1) \n",
    "    bar = np.array([mae_list[i], pcc_list[i], js_dis_list[i], avg_mae_bc_list[i], avg_mae_ec_list[i], avg_mae_pc_list[i]])\n",
    "    x = np.array(['MAE','PCC','JSD','MAE(BC)','MAE(EC)','MAE(PC)'])\n",
    "    y = np.arange(len(bar))\n",
    "    plt.bar(y,bar,color=colors)\n",
    "\n",
    "    for a, b in zip(y,bar):\n",
    "        plt.text(a,b,('%.2f'%b),verticalalignment='bottom',horizontalalignment='right')\n",
    "\n",
    "    plt.xticks(y,x)\n",
    "    plt.title(f'Fold {i+1}')\n",
    "\n",
    "plt.subplot(2, 2, 4) \n",
    "means = [mae_fm, pcc_fm, js_dis_fm, avg_mae_bc_fm, avg_mae_ec_fm, avg_mae_pc_fm]\n",
    "stds = [mae_fs, pcc_fs, js_dis_fs, avg_mae_bc_fs, avg_mae_ec_fs, avg_mae_pc_fs]\n",
    "x = np.array(['MAE','PCC','JSD','MAE(BC)','MAE(EC)','MAE(PC)'])\n",
    "plt.bar(x, means, yerr=stds, capsize=5, color=colors)\n",
    "\n",
    "# Print average evaluation metrics for all folds\n",
    "print(\"\\n Evaluation Metrics of All Folds (Avg.): \")\n",
    "print(\"MAE: \", mae_fm)\n",
    "print(\"PCC: \", pcc_fm)\n",
    "print(\"Jensen-Shannon Distance: \", js_dis_fm)\n",
    "print(\"Average MAE betweenness centrality:\", avg_mae_bc_fm)\n",
    "print(\"Average MAE eigenvector centrality:\", avg_mae_ec_fm)\n",
    "print(\"Average MAE PageRank centrality:\", avg_mae_pc_fm)\n",
    "\n",
    "for x, y, std in zip(x, means, stds):\n",
    "    plt.text(x, y, f'{y:.2f}', ha='center', va='bottom')\n",
    "    \n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Avg. Across Folds')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all training data for 3f-CV\n",
    "train(model, optimizer, lr_training, hr_training, args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-07T16:16:47.361000Z",
     "iopub.status.busy": "2024-03-07T16:16:47.360634Z",
     "iopub.status.idle": "2024-03-07T16:16:48.812434Z",
     "shell.execute_reply": "2024-03-07T16:16:48.811256Z",
     "shell.execute_reply.started": "2024-03-07T16:16:47.360971Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "lr_test_array, hr_test_array\n",
    "lr = lr_test_array[0, :]\n",
    "hr = hr_test_array[0, :]\n",
    "lr = torch.from_numpy(lr).type(torch.FloatTensor)\n",
    "np.fill_diagonal(hr, 1)\n",
    "\n",
    "preds, a, b, c = model(lr)\n",
    "\n",
    "plt.figure(figsize=(24, 16))\n",
    " \n",
    "# Heatmap of a subset of LR data\n",
    "plt.subplot(2,2,1)\n",
    "plt.imshow(lr, aspect='auto', cmap='viridis')  # Adjust subset size as needed\n",
    "plt.colorbar()\n",
    "plt.title('LR Data Heatmap')\n",
    " \n",
    "# Heatmap of a subset of HR data\n",
    "plt.subplot(2,2,2)\n",
    "plt.imshow(hr, aspect='auto', cmap='viridis')  # Adjust subset size as needed\n",
    "plt.colorbar()\n",
    "plt.title('HR Data Heatmap')\n",
    " \n",
    "# Heatmap of a subset of HR data\n",
    "plt.subplot(2,2,3)\n",
    "plt.imshow(preds.detach().cpu().clone().numpy() , aspect='auto', cmap='viridis')  # Adjust subset size as needed\n",
    "plt.colorbar()\n",
    "plt.title('HR_Pre Data Heatmap')\n",
    "\n",
    "# Heatmap of residual\n",
    "plt.subplot(2,2,4)\n",
    "plt.imshow(hr - preds.detach().cpu().clone().numpy() , aspect='auto', cmap='viridis')  # Adjust subset size as needed\n",
    "plt.colorbar()\n",
    "plt.title('Residual between HR_Pre and HR_GT')\n",
    " \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "lrt_df = pd.read_csv(\"/notebooks/lr_test.csv\") # change the directory if needed\n",
    "lrt_matrix_size = get_matrix_size(lrt_df.shape[1])\n",
    "lrt_graphs = load_data_as_graphs(lrt_df, lrt_matrix_size)\n",
    "\n",
    "\n",
    "lrt_array = np.random.normal(0.5, 1,(len(lrt_graphs), 160, 160))\n",
    "i = 0\n",
    "for lrt in lrt_graphs:\n",
    "    lrt_array[i] = lrt.adjacency_matrix\n",
    "    i +=1\n",
    "\n",
    "print(lrt_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def submission(model, data_array, filepath, vector_dim=35778):\n",
    "    n_samples = data_array.shape[0]\n",
    "\n",
    "    results_array = np.zeros((n_samples, vector_dim))\n",
    "    results_array.shape\n",
    "\n",
    "    # Iterate through each sample in the dataset\n",
    "    i = 0\n",
    "    for lr in data_array:\n",
    "        lr_tensor = torch.from_numpy(lr).type(torch.FloatTensor)\n",
    "\n",
    "        with torch.no_grad():  \n",
    "            preds, _, _, _ = model(lr_tensor)\n",
    "\n",
    "        prediction = preds.detach().cpu().numpy()\n",
    "        vectorized_pred = MatrixVectorizer.vectorize(prediction) # Vectorize the prediction matrix to a flat array\n",
    "\n",
    "        results_array[i, :] = vectorized_pred # Store the vectorized prediction in the results array\n",
    "        i += 1\n",
    "\n",
    "    print(results_array.shape)\n",
    "    \n",
    "    melted_result = results_array.flatten()\n",
    "    ids = np.arange(1, melted_result.shape[0]+1)\n",
    "    print(ids.shape)\n",
    "    print(melted_result.shape)\n",
    "    df = pd.DataFrame({\n",
    "        \"ID\": ids,\n",
    "        \"Predicted\": melted_result\n",
    "    })\n",
    "\n",
    "    print(df.shape)\n",
    "    print(df.head())\n",
    "    df.to_csv(filepath,index=False)\n",
    "    print(\"the results are saved\")\n",
    "    return \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "submission_filepath = \"/notebooks/submission.csv\" # change directory if needed\n",
    "submission(model, lrt_array, submission_filepath)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 7783828,
     "sourceId": 71243,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30665,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
