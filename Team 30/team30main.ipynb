{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A57XFiX5RL0J",
    "outputId": "badf8ce3-5643-4c0e-8f8c-36384aa1aba4"
   },
   "outputs": [],
   "source": [
    "%pip install -q networkx==3.2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from evaluation_functions import KFold\n",
    "from evaluation import evaluate_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = KFold(embeddings='Random CV/Embeddings/lr_embeddings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8AUP23eLXCD8",
    "outputId": "03461b12-eaa5-4832-b914-61218140625c"
   },
   "outputs": [],
   "source": [
    "# Set a fixed random seed for reproducibility across multiple libraries\n",
    "random_seed = 42\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "# Check for CUDA (GPU support) and set device accordingly\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    torch.set_default_device(device)\n",
    "    print(\"CUDA is available. Using GPU.\")\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed)  # For multi-GPU setups\n",
    "    # Additional settings for ensuring reproducibility on CUDA\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDA not available. Using CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nEvGbEehlUck"
   },
   "outputs": [],
   "source": [
    "def weight_variable_glorot(output_dim):\n",
    "    input_dim = output_dim\n",
    "    init_range = np.sqrt(6.0 / (input_dim + output_dim))\n",
    "    initial = np.random.uniform(-init_range, init_range, (input_dim, output_dim))\n",
    "    return initial\n",
    "\n",
    "\n",
    "def pad_HR_adj(label, split):\n",
    "    padded_label = torch.nn.functional.pad(label, ((split, split, split, split)), mode=\"constant\")\n",
    "    padded_label.fill_diagonal_(0)\n",
    "    return padded_label\n",
    "\n",
    "\n",
    "def normalize_adj_torch(mx):\n",
    "    rowsum = mx.sum(1)\n",
    "    r_inv_sqrt = torch.pow(rowsum, -0.5).flatten()\n",
    "    r_inv_sqrt[torch.isinf(r_inv_sqrt)] = 0.\n",
    "    r_mat_inv_sqrt = torch.diag(r_inv_sqrt)\n",
    "    mx = torch.matmul(mx, r_mat_inv_sqrt)\n",
    "    mx = torch.transpose(mx, 0, 1)\n",
    "    mx = torch.matmul(mx, r_mat_inv_sqrt)\n",
    "    return mx\n",
    "\n",
    "\n",
    "def unpad(data, split):\n",
    "    idx_0 = data.shape[0]-split\n",
    "    idx_1 = data.shape[1]-split\n",
    "    train = data[split:idx_0, split:idx_1]\n",
    "    return train\n",
    "\n",
    "\n",
    "# For Drop-GNN Integration to Forward pass for AGSRNet\n",
    "def apply_dropout(A, dropout_prob=0.02):\n",
    "    dropout_mask = torch.rand(A.shape) > dropout_prob\n",
    "    dropout_mask = dropout_mask.to(device)\n",
    "    A_dropped = A * dropout_mask.float()\n",
    "    return A_dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s6AWgUuJl5a_"
   },
   "outputs": [],
   "source": [
    "class GCLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Graph convolutional layer with attention\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, act=F.relu):\n",
    "        super(GCLayer, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.act = act\n",
    "        self.weight = torch.nn.Parameter(torch.FloatTensor(in_features, out_features).to(device))\n",
    "        self.bias = nn.Parameter(torch.zeros(out_features).to(device))\n",
    "        self.phi = nn.Parameter(torch.FloatTensor(2 * out_features, 1).to(device))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        torch.nn.init.xavier_uniform_(self.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.phi)\n",
    "\n",
    "    def forward(self, A, X):\n",
    "        # 1. Apply linear transformation and add bias\n",
    "        l = torch.mm(X, self.weight)\n",
    "        N = l.size(0)\n",
    "\n",
    "        # 2. Compute the attention coefficients\n",
    "        a_input = torch.cat([l.repeat(1, N).view(N * N, -1), l.repeat(N, 1)], dim=1).view(N, N, -1).to(device)\n",
    "        S = torch.matmul(a_input, self.phi).view(N, N)\n",
    "\n",
    "        # 3. Compute mask based on adjacency matrix\n",
    "        I = torch.eye(N, device=S.device)\n",
    "        mask = (A + I).bool()\n",
    "        masked_S = torch.where(mask, S, torch.tensor(float('-inf'), device=S.device))\n",
    "\n",
    "        # 4. Apply softmax to compute attention weights\n",
    "        attention = F.softmax(masked_S, dim=1)\n",
    "\n",
    "        # 5. Aggregate features based on attention weights\n",
    "        h = torch.matmul(attention, l)\n",
    "\n",
    "        return self.act(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GSRLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, out_dim):\n",
    "        super(GSRLayer, self).__init__()\n",
    "        self.weights = torch.nn.Parameter(\n",
    "            torch.from_numpy(weight_variable_glorot(out_dim)).type(torch.FloatTensor).to(device))\n",
    "\n",
    "    def forward(self, A, X):\n",
    "        with torch.autograd.set_detect_anomaly(True):\n",
    "            lr = A.to(device)\n",
    "            f = X.to(device)\n",
    "\n",
    "            lr_dim = lr.shape[0]\n",
    "            eye_mat = torch.eye(lr_dim).type(torch.FloatTensor).to(device)\n",
    "            s_d = torch.cat((eye_mat, eye_mat), dim=0)\n",
    "\n",
    "            eig_val_lr, U_lr = torch.linalg.eigh(lr, UPLO='U')\n",
    "\n",
    "            a = torch.matmul(self.weights, s_d)\n",
    "            b = torch.matmul(a, torch.t(U_lr))\n",
    "            f_d = torch.matmul(b, f)\n",
    "            f_d = torch.abs(f_d)\n",
    "            f_d = f_d.fill_diagonal_(0)\n",
    "            A = f_d\n",
    "\n",
    "            X = torch.mm(A, A.t())\n",
    "            X = (X + X.t()) / 2\n",
    "            X = X.fill_diagonal_(0)\n",
    "        return A, torch.abs(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xEm0ICdiynNl"
   },
   "outputs": [],
   "source": [
    "class GraphUnpool(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(GraphUnpool, self).__init__()\n",
    "\n",
    "    def forward(self, A, X, idx):\n",
    "        new_X = torch.zeros([A.shape[0], X.shape[1]])\n",
    "        new_X[idx] = X\n",
    "        return A, new_X\n",
    "\n",
    "\n",
    "class GraphPool(nn.Module):\n",
    "\n",
    "    def __init__(self, k, in_dim):\n",
    "        super(GraphPool, self).__init__()\n",
    "        self.k = k\n",
    "        self.proj = nn.Linear(in_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, A, X):\n",
    "        scores = self.proj(X)\n",
    "        scores = torch.squeeze(scores)\n",
    "        scores = self.sigmoid(scores / 100)\n",
    "        num_nodes = A.shape[0]\n",
    "        values, idx = torch.topk(scores, int(self.k * num_nodes))\n",
    "        new_X = X[idx, :]\n",
    "        values = torch.unsqueeze(values, -1)\n",
    "        new_X = torch.mul(new_X, values)\n",
    "        A = A[idx, :]\n",
    "        A = A[:, idx]\n",
    "        return A, new_X, idx\n",
    "\n",
    "\n",
    "class GraphUnet(nn.Module):\n",
    "\n",
    "    def __init__(self, ks, in_dim, out_dim, dim=320):\n",
    "        super(GraphUnet, self).__init__()\n",
    "        self.ks = ks\n",
    "\n",
    "        self.start_gcn = GCLayer(in_dim, dim)\n",
    "        self.bottom_gcn = GCLayer(dim, dim)\n",
    "        self.end_gcn = GCLayer(2 * dim, out_dim)\n",
    "        self.down_gcns = []\n",
    "        self.up_gcns = []\n",
    "        self.pools = []\n",
    "        self.unpools = []\n",
    "        self.l_n = len(ks)\n",
    "        for i in range(self.l_n):\n",
    "            self.down_gcns.append(GCLayer(dim, dim))\n",
    "            self.up_gcns.append(GCLayer(dim, dim))\n",
    "            self.pools.append(GraphPool(ks[i], dim))\n",
    "            self.unpools.append(GraphUnpool())\n",
    "\n",
    "    def forward(self, A, X):\n",
    "        adj_ms = []\n",
    "        indices_list = []\n",
    "        down_outs = []\n",
    "        X = self.start_gcn(A, X)\n",
    "        start_gcn_outs = X\n",
    "        org_X = X\n",
    "        for i in range(self.l_n):\n",
    "\n",
    "            X = self.down_gcns[i](A, X)\n",
    "            adj_ms.append(A)\n",
    "            down_outs.append(X)\n",
    "            A, X, idx = self.pools[i](A, X)\n",
    "            indices_list.append(idx)\n",
    "\n",
    "        X = self.bottom_gcn(A, X)\n",
    "        for i in range(self.l_n):\n",
    "            up_idx = self.l_n - i - 1\n",
    "\n",
    "            A, idx = adj_ms[up_idx], indices_list[up_idx]\n",
    "            A, X = self.unpools[i](A, X, idx)\n",
    "            X = self.up_gcns[i](A, X)\n",
    "            X = X.add(down_outs[up_idx])\n",
    "        X = torch.cat([X, org_X], 1)\n",
    "\n",
    "        X = self.end_gcn(A, X)\n",
    "\n",
    "        return X, start_gcn_outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tKhkDm2xmASd"
   },
   "outputs": [],
   "source": [
    "class AGSRNet(nn.Module):\n",
    "\n",
    "    def __init__(self, ks, args):\n",
    "        super(AGSRNet, self).__init__()\n",
    "\n",
    "        self.lr_dim = args.lr_dim\n",
    "        self.hr_dim = args.hr_dim\n",
    "        self.hidden_dim = args.hidden_dim\n",
    "        self.conv1 = GCLayer(self.lr_dim, self.hidden_dim)\n",
    "        self.net = GraphUnet(ks, self.hidden_dim, self.hr_dim)\n",
    "        self.gsr_layer = GSRLayer(self.hr_dim)\n",
    "\n",
    "    def forward(self, lr, embeddings, num_runs=3):\n",
    "        with torch.autograd.set_detect_anomaly(True):\n",
    "\n",
    "            A = normalize_adj_torch(lr).type(torch.FloatTensor).to(device)\n",
    "\n",
    "            X = embeddings\n",
    "            X = self.conv1(A, X)\n",
    "\n",
    "            # DropGNN Contribution: Store the embeddings from all runs\n",
    "            all_net_outs, all_start_gcn_outs, all_outputs, all_Z = [], [], [], []\n",
    "\n",
    "            for _ in range(num_runs):\n",
    "                A_dropout = apply_dropout(A)\n",
    "\n",
    "                net_outs, start_gcn_outs = self.net(A_dropout, X)\n",
    "                outputs, Z = self.gsr_layer(A_dropout, net_outs)\n",
    "\n",
    "                # DropGNN Contribution: Store intermediate results for aggregation\n",
    "                all_net_outs.append(net_outs.unsqueeze(0))\n",
    "                all_start_gcn_outs.append(start_gcn_outs.unsqueeze(0))\n",
    "                all_outputs.append(outputs.unsqueeze(0))\n",
    "                all_Z.append(Z.unsqueeze(0))\n",
    "\n",
    "            # DropGNN Contribution: Aggregate\n",
    "            net_outs = torch.mean(torch.cat(all_net_outs, dim=0), dim=0)\n",
    "            start_gcn_outs = torch.mean(torch.cat(all_start_gcn_outs, dim=0), dim=0)\n",
    "            outputs = torch.mean(torch.cat(all_outputs, dim=0), dim=0)\n",
    "            Z = torch.mean(torch.cat(all_Z, dim=0), dim=0)\n",
    "\n",
    "            Z = (Z + Z.t()) / 2\n",
    "            Z.fill_diagonal_(0)\n",
    "\n",
    "        return torch.abs(Z), net_outs, start_gcn_outs, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense(nn.Module):\n",
    "    def __init__(self, n1, n2, args):\n",
    "        super(Dense, self).__init__()\n",
    "        self.weights = torch.nn.Parameter(torch.FloatTensor(n1, n2).to(device))\n",
    "        nn.init.normal_(self.weights, mean=args.mean_dense, std=args.std_dense)\n",
    "\n",
    "    def forward(self, X):\n",
    "        out = torch.mm(X, self.weights)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.dense_1 = Dense(args.hr_dim, args.hr_dim, args)\n",
    "        self.relu_1 = nn.ReLU(inplace=False)\n",
    "        self.dense_2 = Dense(args.hr_dim, args.hr_dim, args)\n",
    "        self.relu_2 = nn.ReLU(inplace=False)\n",
    "        self.dense_3 = Dense(args.hr_dim, 1, args)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        dc_den1 = self.relu_1(self.dense_1(inputs))\n",
    "        dc_den2 = self.relu_2(self.dense_2(dc_den1))\n",
    "        output = dc_den2\n",
    "        output = self.dense_3(dc_den2)\n",
    "        output = self.sigmoid(output)\n",
    "        return torch.abs(output)\n",
    "\n",
    "\n",
    "def gaussian_noise_layer(input_layer, args):\n",
    "    z = torch.empty_like(input_layer)\n",
    "    noise = z.normal_(mean=args.mean_gaussian, std=args.std_gaussian)\n",
    "    z = torch.abs(input_layer + noise)\n",
    "    z = (z + z.t())/2\n",
    "    z = z.fill_diagonal_(0)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PeAIIm9pmDeJ"
   },
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "\n",
    "def train_for_epoch(model, subjects_adj, subjects_labels, val_adj, val_labels, embeddings, val_embeddings, args, min_epochs=100, early_stopping=10):\n",
    "\n",
    "    bce_loss = nn.BCELoss()\n",
    "    netD = Discriminator(args).to(device)\n",
    "\n",
    "    optimizerG = optim.Adam(model.parameters(), lr=args.lr)\n",
    "    optimizerD = optim.Adam(netD.parameters(), lr=args.lr)\n",
    "\n",
    "    all_epochs_training_loss = []\n",
    "    all_epochs_training_error = []\n",
    "    all_epochs_val_loss = []\n",
    "    all_epochs_val_error = []\n",
    "\n",
    "    best_val_error = float('inf')\n",
    "    best_epoch = 0\n",
    "\n",
    "    epoch = 0\n",
    "    epoch_without_improvement = 0\n",
    "\n",
    "    while epoch < min_epochs or epoch_without_improvement < early_stopping:\n",
    "        epoch += 1\n",
    "        with torch.autograd.set_detect_anomaly(True):\n",
    "            epoch_training_loss = []\n",
    "            epoch_training_error = []\n",
    "            epoch_val_loss = []\n",
    "            epoch_val_error = []\n",
    "            model.train()\n",
    "            for lr, hr, x in zip(subjects_adj, subjects_labels, embeddings):\n",
    "                optimizerD.zero_grad()\n",
    "                optimizerG.zero_grad()\n",
    "\n",
    "                lr = torch.from_numpy(lr).type(torch.FloatTensor).to(device)\n",
    "                hr = torch.from_numpy(hr).type(torch.FloatTensor).to(device)\n",
    "                x = torch.from_numpy(x).type(torch.FloatTensor).to(device)\n",
    "\n",
    "                model_outputs, net_outs, start_gcn_outs, layer_outs = model(lr, x)\n",
    "\n",
    "                net_outs = net_outs.to(device)\n",
    "                start_gcn_outs = start_gcn_outs.to(device)\n",
    "                model_outputs = model_outputs.to(device)\n",
    "\n",
    "                padded_hr = pad_HR_adj(hr, args.padding)\n",
    "                eig_val_hr, U_hr = torch.linalg.eigh(padded_hr, UPLO='U')\n",
    "\n",
    "                mse_loss1 = args.lmbda * criterion(net_outs, start_gcn_outs)\n",
    "                mse_loss2 = criterion(model.gsr_layer.weights, U_hr)\n",
    "                mse_loss3 = criterion(model_outputs, padded_hr)\n",
    "                mse_loss = mse_loss1 + mse_loss2 + mse_loss3\n",
    "\n",
    "                error = criterion(model_outputs, padded_hr)\n",
    "                real_data = model_outputs.detach()\n",
    "                fake_data = gaussian_noise_layer(padded_hr, args)\n",
    "\n",
    "                d_real = netD(real_data)\n",
    "                d_fake = netD(fake_data)\n",
    "\n",
    "                dc_loss_real = bce_loss(d_real, torch.ones(args.hr_dim, 1))\n",
    "                dc_loss_fake = bce_loss(d_fake, torch.zeros(args.hr_dim, 1))\n",
    "                dc_loss = dc_loss_real + dc_loss_fake\n",
    "\n",
    "                dc_loss.backward()\n",
    "                optimizerD.step()\n",
    "\n",
    "                d_fake = netD(gaussian_noise_layer(padded_hr, args))\n",
    "\n",
    "                gen_loss = bce_loss(d_fake, torch.ones(args.hr_dim, 1))\n",
    "                generator_loss = gen_loss + mse_loss\n",
    "                generator_loss.backward()\n",
    "                optimizerG.step()\n",
    "\n",
    "                epoch_training_loss.append(generator_loss.item())\n",
    "                epoch_training_error.append(error.item())\n",
    "\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for lr, hr, x in zip(val_adj, val_labels, val_embeddings):\n",
    "\n",
    "                    lr = torch.from_numpy(lr).type(torch.FloatTensor).to(device)\n",
    "                    hr = torch.from_numpy(hr).type(torch.FloatTensor).to(device)\n",
    "                    x = torch.from_numpy(x).type(torch.FloatTensor).to(device)\n",
    "\n",
    "                    model_outputs, net_outs, start_gcn_outs, layer_outs = model(lr, x)\n",
    "\n",
    "                    net_outs = net_outs.to(device)\n",
    "                    start_gcn_outs = start_gcn_outs.to(device)\n",
    "                    model_outputs = model_outputs.to(device)\n",
    "\n",
    "                    padded_hr = pad_HR_adj(hr, args.padding)\n",
    "                    eig_val_hr, U_hr = torch.linalg.eigh(padded_hr, UPLO='U')\n",
    "\n",
    "                    mse_loss1 = args.lmbda * criterion(net_outs, start_gcn_outs)\n",
    "                    mse_loss2 = criterion(model.gsr_layer.weights, U_hr)\n",
    "                    mse_loss3 = criterion(model_outputs, padded_hr)\n",
    "                    mse_loss = mse_loss1 + mse_loss2 + mse_loss3\n",
    "\n",
    "                    error = criterion(model_outputs, padded_hr)\n",
    "                    real_data = model_outputs.detach()\n",
    "                    fake_data = gaussian_noise_layer(padded_hr, args)\n",
    "\n",
    "                    d_real = netD(real_data)\n",
    "                    d_fake = netD(fake_data)\n",
    "\n",
    "                    dc_loss_real = bce_loss(d_real, torch.ones(args.hr_dim, 1))\n",
    "                    dc_loss_fake = bce_loss(d_fake, torch.zeros(args.hr_dim, 1))\n",
    "                    dc_loss = dc_loss_real + dc_loss_fake\n",
    "\n",
    "                    d_fake = netD(gaussian_noise_layer(padded_hr, args))\n",
    "\n",
    "                    gen_loss = bce_loss(d_fake, torch.ones(args.hr_dim, 1))\n",
    "                    generator_loss = gen_loss + mse_loss\n",
    "                    \n",
    "                    epoch_val_loss.append(generator_loss.item())\n",
    "                    epoch_val_error.append(error.item())\n",
    "\n",
    "            print(\"Epoch: \", epoch, \"Loss: \", np.mean(epoch_training_loss),\n",
    "                    \"Error: \", np.mean(epoch_training_error) * 100, \"%\", \"Val Loss: \", np.mean(epoch_val_loss),\n",
    "                    \"Val Error: \", np.mean(epoch_val_error) * 100, \"%\")\n",
    "            all_epochs_training_loss.append(np.mean(epoch_training_loss))\n",
    "            all_epochs_training_error.append(np.mean(epoch_training_error))\n",
    "            all_epochs_val_loss.append(np.mean(epoch_val_loss))\n",
    "            all_epochs_val_error.append(np.mean(epoch_val_error))\n",
    "\n",
    "            epoch_without_improvement += 1\n",
    "\n",
    "            if np.mean(epoch_val_error) < best_val_error:\n",
    "                best_val_error = np.mean(epoch_val_error)\n",
    "                best_epoch = epoch\n",
    "                epoch_without_improvement = 0\n",
    "\n",
    "    return best_epoch, all_epochs_training_loss, all_epochs_training_error, all_epochs_val_loss, all_epochs_val_error\n",
    "\n",
    "\n",
    "def train(model, subjects_adj, subjects_labels, embeddings, args, num_epochs):\n",
    "\n",
    "    bce_loss = nn.BCELoss()\n",
    "    netD = Discriminator(args).to(device)\n",
    "\n",
    "    optimizerG = optim.Adam(model.parameters(), lr=args.lr)\n",
    "    optimizerD = optim.Adam(netD.parameters(), lr=args.lr)\n",
    "\n",
    "    all_epochs_loss = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        with torch.autograd.set_detect_anomaly(True):\n",
    "            epoch_loss = []\n",
    "            epoch_error = []\n",
    "            model.train()\n",
    "            for lr, hr, x in zip(subjects_adj, subjects_labels, embeddings):\n",
    "                optimizerD.zero_grad()\n",
    "                optimizerG.zero_grad()\n",
    "\n",
    "                lr = torch.from_numpy(lr).type(torch.FloatTensor).to(device)\n",
    "                hr = torch.from_numpy(hr).type(torch.FloatTensor).to(device)\n",
    "                x = torch.from_numpy(x).type(torch.FloatTensor).to(device)\n",
    "\n",
    "                model_outputs, net_outs, start_gcn_outs, layer_outs = model(lr, x)\n",
    "\n",
    "                net_outs = net_outs.to(device)\n",
    "                start_gcn_outs = start_gcn_outs.to(device)\n",
    "                model_outputs = model_outputs.to(device)\n",
    "\n",
    "                padded_hr = pad_HR_adj(hr, args.padding)\n",
    "                eig_val_hr, U_hr = torch.linalg.eigh(padded_hr, UPLO='U')\n",
    "\n",
    "                mse_loss1 = args.lmbda * criterion(net_outs, start_gcn_outs)\n",
    "                mse_loss2 = criterion(model.gsr_layer.weights, U_hr)\n",
    "                mse_loss3 = criterion(model_outputs, padded_hr)\n",
    "                mse_loss = mse_loss1 + mse_loss2 + mse_loss3\n",
    "\n",
    "                error = criterion(model_outputs, padded_hr)\n",
    "                real_data = model_outputs.detach()\n",
    "                fake_data = gaussian_noise_layer(padded_hr, args)\n",
    "\n",
    "                d_real = netD(real_data)\n",
    "                d_fake = netD(fake_data)\n",
    "\n",
    "                dc_loss_real = bce_loss(d_real, torch.ones(args.hr_dim, 1))\n",
    "                dc_loss_fake = bce_loss(d_fake, torch.zeros(args.hr_dim, 1))\n",
    "                dc_loss = dc_loss_real + dc_loss_fake\n",
    "\n",
    "                dc_loss.backward()\n",
    "                optimizerD.step()\n",
    "\n",
    "                d_fake = netD(gaussian_noise_layer(padded_hr, args))\n",
    "\n",
    "                gen_loss = bce_loss(d_fake, torch.ones(args.hr_dim, 1))\n",
    "                generator_loss = gen_loss + mse_loss\n",
    "                generator_loss.backward()\n",
    "                optimizerG.step()\n",
    "\n",
    "                epoch_loss.append(generator_loss.item())\n",
    "                epoch_error.append(error.item())\n",
    "\n",
    "            print(\"Epoch: \", epoch, \"Loss: \", np.mean(epoch_loss),\n",
    "                    \"Error: \", np.mean(epoch_error) * 100, \"%\")\n",
    "            all_epochs_loss.append(np.mean(epoch_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NTVEq5SVpySc"
   },
   "source": [
    "# Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NJ7ZabMcRQNk"
   },
   "outputs": [],
   "source": [
    "matrix_size_lr = 160\n",
    "matrix_size_hr = 268\n",
    "\n",
    "data.preprocessing(matrix_size_lr, matrix_size_hr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vvg3MlsjnjmV"
   },
   "outputs": [],
   "source": [
    "def generate_predictions(model, test_adj, test_labels, embeddings, args, vectorize=True, file=None):\n",
    "    model.eval()\n",
    "    preds_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for lr, x in zip(test_adj, embeddings):\n",
    "            all_zeros_lr = not np.any(lr)\n",
    "            if all_zeros_lr == False:\n",
    "                lr = torch.from_numpy(lr).type(torch.FloatTensor).to(device)\n",
    "                x = torch.from_numpy(x).type(torch.FloatTensor).to(device)\n",
    "                preds, a, b, c = model(lr, x)\n",
    "\n",
    "                preds_square = preds.view(args.hr_dim, args.hr_dim).cpu().numpy()\n",
    "                truncated_preds = preds_square[args.padding:-args.padding, args.padding:-args.padding]\n",
    "\n",
    "                if vectorize:\n",
    "                    final_preds = data.mv.vectorize(truncated_preds)\n",
    "                else:\n",
    "                  final_preds = truncated_preds\n",
    "\n",
    "                preds_list.append(final_preds)\n",
    "\n",
    "    all_preds = np.array(preds_list)\n",
    "\n",
    "    _test_labels = np.array([np.array([np.array(x) for x in p]) for p in test_labels])\n",
    "\n",
    "    metrics = evaluate_all(\n",
    "            _test_labels, all_preds, output_path=file\n",
    "        )\n",
    "\n",
    "    return all_preds, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eiiPIkEmtAEJ"
   },
   "outputs": [],
   "source": [
    "class Args:\n",
    "    lr = 0.0001\n",
    "    lmbda = 0.1\n",
    "    lr_dim = 160\n",
    "    hr_dim = 320\n",
    "    hidden_dim = 320\n",
    "    padding = 26\n",
    "    mean_dense = 0.0\n",
    "    std_dense = 0.01\n",
    "    mean_gaussian = 0.0\n",
    "    std_gaussian = 0.1\n",
    "\n",
    "args = Args()\n",
    "\n",
    "ks = [0.9, 0.7, 0.6, 0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_losses(test_index, training_losses, training_error, validation_losses=None, validation_error=None):\n",
    "    fig = plt.figure()\n",
    "    plt.plot(training_losses, label='Training loss')\n",
    "    plt.plot(training_error, label='Training error')\n",
    "    if validation_losses is not None:\n",
    "        plt.plot(validation_losses, label='Validation loss')\n",
    "        plt.plot(validation_error, label='Validation error')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Fold ' + str(test_index))\n",
    "    plt.savefig(f'./evaluation/Random CV/loss_fold_{test_index}_validation.png' if validation_losses is not None else f'./evaluation/Random CV/loss_fold_{test_index}_training.png')\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "lJnJpo-BhvTk",
    "outputId": "247aa4da-f6ac-40f7-831b-638da69643a7"
   },
   "outputs": [],
   "source": [
    "#lr_data_adjacency_matrices = np.array(lr_data_adjacency_matrices)\n",
    "#hr_data_adjacency_matrices = np.array(hr_data_adjacency_matrices)\n",
    "\n",
    "def cross_validation_with_embeddings(k=3):\n",
    "    fold_predictions = []\n",
    "    fold_ground_truths = []\n",
    "\n",
    "    for test_index in range(3):\n",
    "        print(f\"Fold {test_index+1}:\")\n",
    "        #reset model/ initialise\n",
    "        model = AGSRNet(ks, args).to(device)\n",
    "        train_adj, train_ground_truth, dev_adj, dev_ground_truth, train_embeddings, dev_embeddings = data.obtain_folds(test_index, with_validation=True, return_embeddings=True)\n",
    "        best_epoch, all_epochs_training_loss, all_epochs_training_error, all_epochs_val_loss, all_epochs_val_error = train_for_epoch(model, train_adj, train_ground_truth, dev_adj, dev_ground_truth, train_embeddings, dev_embeddings, args)\n",
    "        save_losses(test_index+1, all_epochs_training_loss, all_epochs_training_error, all_epochs_val_loss, all_epochs_val_error)\n",
    "        print(f\"Best epoch for fold {test_index+1}: {best_epoch}\")\n",
    "        train_adj, train_ground_truth, dev_adj, dev_ground_truth, train_embeddings, dev_embeddings = data.obtain_folds(test_index, with_validation=False, return_embeddings=True)\n",
    "        model = AGSRNet(ks, args).to(device)\n",
    "        train(model, train_adj, train_ground_truth, train_embeddings, args, best_epoch)\n",
    "        fold_pred, _ = generate_predictions(model, dev_adj, dev_ground_truth, dev_embeddings, args, vectorize=False, file=f'./evaluation/Random CV/metrics.csv')\n",
    "        # Post-process predictions\n",
    "        fold_pred = np.clip(fold_pred, 0, 1)\n",
    "        # Save predictions in CSV\n",
    "        vec_fold_pred = data.mv.vectorize(fold_pred)\n",
    "        flat_fold_pred = vec_fold_pred.flatten()\n",
    "        ids = np.arange(1, len(flat_fold_pred) + 1)\n",
    "        submission_df = pd.DataFrame({\n",
    "            'ID': ids,\n",
    "            'Predicted': flat_fold_pred\n",
    "        })\n",
    "        submission_df.to_csv(f'./evaluation/Random CV/predictions_fold_{test_index+1}.csv', index=False)\n",
    "        # Store predictions and ground truths\n",
    "        fold_predictions.append(fold_pred)\n",
    "        fold_ground_truths.append(dev_ground_truth)\n",
    "\n",
    "    return fold_predictions, fold_ground_truths\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_preds, cv_gts = cross_validation_with_embeddings(k=3)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "V100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
