{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "12Jg3ox0G2TJ"
   },
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T20:38:59.668254800Z",
     "start_time": "2024-05-23T20:38:59.406420600Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nSR8pbahHlIM",
    "outputId": "7c79d2ec-5e37-4b9c-d9fd-3c95e7317391"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "os.chdir(\"/content/drive/MyDrive/DeepLearning/DGL/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-24T14:01:16.310639600Z",
     "start_time": "2024-05-24T14:01:15.084756800Z"
    },
    "id": "-0_EKY_QG2TN"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import DataLoader, TensorDataset, Subset\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from utils.preprocessing import *\n",
    "from utils.metrics import get_metrics\n",
    "from utils.models import GraphCycleGAN\n",
    "from utils.training import train_graph_cyclegan, train_graph_cyclegan_es\n",
    "from evaluation import evaluate_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-24T14:01:17.130404600Z",
     "start_time": "2024-05-24T14:01:16.914459500Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jQFPMTc_G2TO",
    "outputId": "41d1bc61-267a-436b-b4a3-c454e9aa7b78"
   },
   "outputs": [],
   "source": [
    "# Set a fixed random seed for reproducibility across multiple libraries\n",
    "random_seed = 42\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "# Check for CUDA (GPU support) and set device accordingly\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"CUDA is available. Using GPU.\")\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed)  # For multi-GPU setups\n",
    "    # Additional settings for ensuring reproducibility on CUDA\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDA not available. Using CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bzi9DIRbVeVF"
   },
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W9I_9QcUVgvp"
   },
   "outputs": [],
   "source": [
    "# Load pre-processed data\n",
    "lr_train = np.genfromtxt('data/lr_train.csv', delimiter=',', skip_header=1)\n",
    "hr_train = np.genfromtxt('data/hr_train.csv', delimiter=',', skip_header=1)\n",
    "lr_test = np.genfromtxt('data/lr_test.csv', delimiter=',', skip_header=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1ogK_hZ0Vu21",
    "outputId": "5a4bd637-b7b1-4a3a-9518-f4c402ec3e87"
   },
   "outputs": [],
   "source": [
    "print(lr_train.shape)\n",
    "print(hr_train.shape)\n",
    "print(lr_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hZJGUGYsG2Tu",
    "outputId": "ec8b6bd7-e387-40b1-fe0e-c1147454a3b8"
   },
   "outputs": [],
   "source": [
    "# Check data suitability\n",
    "contains_nan = np.isnan(lr_train).any() or np.isnan(hr_train).any() or np.isnan(lr_test).any()\n",
    "contains_negative = (lr_train < 0).any() or (hr_train < 0).any() or (lr_test < 0).any()\n",
    "\n",
    "print(\"Contains NaN:\", contains_nan)\n",
    "print(\"Contains negative numbers:\", contains_negative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1TIkuiUcXt21"
   },
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-16T16:08:31.657268300Z",
     "start_time": "2024-05-16T16:08:31.652733300Z"
    },
    "id": "W-wszgXZurgG"
   },
   "outputs": [],
   "source": [
    "def plot_evaluation_metrics(fold_results):\n",
    "    # Assuming fold_results is a list of tuples with each tuple containing all metrics for a fold\n",
    "    metrics = np.array(fold_results)\n",
    "\n",
    "    # Calculate mean and standard deviation across folds for each metric\n",
    "    metrics_mean = metrics.mean(axis=0)\n",
    "    metrics_std = metrics.std(axis=0)\n",
    "\n",
    "    # Define metric names\n",
    "    metric_names = ['MAE', 'PCC', 'JSD', 'MAE-PC', 'MAE-EC', 'MAE-BC']\n",
    "\n",
    "    # Set up the subplot for each fold + the average\n",
    "    n_folds = len(fold_results)\n",
    "    fig, axs = plt.subplots(1, n_folds + 1, figsize=(20, 5))\n",
    "\n",
    "    # Define colors for each bar\n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, len(metric_names)))\n",
    "\n",
    "    # Plot each fold's metrics\n",
    "    for i in range(n_folds):\n",
    "        axs[i].bar(metric_names, metrics[i], color=colors)\n",
    "        axs[i].set_title(f'Fold {i+1}')\n",
    "\n",
    "    # Plot the average metrics with error bars\n",
    "    axs[-1].bar(metric_names, metrics_mean, yerr=metrics_std, capsize=5, color=colors)\n",
    "    axs[-1].set_title('Avg. Across Folds')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eI3oUsztWZ76"
   },
   "source": [
    "## Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JNar1m8z6u0s"
   },
   "outputs": [],
   "source": [
    "lr_train_matrix = torch.from_numpy(np.array([anti_vectorize(lr_train[i], 160) for i in range(167)])).float().to(device)\n",
    "hr_train_matrix = torch.from_numpy(np.array([anti_vectorize(hr_train[i], 268) for i in range(167)])).float().to(device)\n",
    "\n",
    "kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "train_data = TensorDataset(torch.from_numpy(lr_train), torch.from_numpy(hr_train), lr_train_matrix, hr_train_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "shTDmWkiPpVt",
    "outputId": "dbdb0692-bc0c-4981-ef4c-3d65aa8f8d65"
   },
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "batch_size = 8\n",
    "\n",
    "fold_results = []\n",
    "\n",
    "for fold, (train_ids, val_ids) in enumerate(kf.split(train_data)):\n",
    "    print('-'*10, f\"Fold {fold+1}\", '-'*10)\n",
    "\n",
    "    # Sample elements randomly from a given list of ids, no replacement.\n",
    "    train_subsampler = Subset(train_data, train_ids)\n",
    "    val_subsampler = Subset(train_data, val_ids)\n",
    "\n",
    "    # Define data loaders for training and testing data in this fold\n",
    "    train_loader = DataLoader(train_subsampler, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_subsampler, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Resetting the model and optimizer for each fold\n",
    "    model = GraphCycleGAN(12720, 35778).to(device)\n",
    "    fold_results.append(train_graph_cyclegan(model, train_loader, val_loader, epochs=num_epochs, device=device, fold_num=fold+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 341
    },
    "id": "BfMYilLo8--p",
    "outputId": "9c1ec04b-cff8-4cf0-932a-215b1366a2a2"
   },
   "outputs": [],
   "source": [
    "plot_evaluation_metrics(fold_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OqbkjncUnWzq"
   },
   "source": [
    "## Export Test Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3aV0vGLGnWzr",
    "outputId": "63920fe1-9714-439f-895d-67a4bc5bd63b"
   },
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "model = GraphCycleGAN(12720, 35778).to(device)\n",
    "train_graph_cyclegan(model, train_loader, epochs=num_epochs, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pA5cNfVlnWzr"
   },
   "outputs": [],
   "source": [
    "hr_test = model.G_A2B(torch.from_numpy(lr_test).float().to(device)).cpu().detach().numpy()\n",
    "\n",
    "hr_test_export = np.clip(hr_test.flatten(), 0, 1)\n",
    "id_column = np.arange(1, len(hr_test_export)+1)\n",
    "\n",
    "submission_df = pd.DataFrame({\n",
    "    'ID': id_column,\n",
    "    'Predicted': hr_test_export\n",
    "})\n",
    "\n",
    "submission_df.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# !NOTE: The Following Code is for Cluster-CV and Random-CV for the paper \"A Benchmark for Graph Super-resolution GNNs\". Not part of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Loading the Cluster-CV and Random-CV Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load pre-processed data\n",
    "lrs = []\n",
    "hrs = []\n",
    "    \n",
    "for i in range(1,4):\n",
    "    # Change the path to the location of the data on your machine for Cluster-CV and Random-CV\n",
    "    lr_train_path = f'../../Random-CV2/Fold{i}/lr_split_{i}.csv'\n",
    "    hr_train_path = f'../../Random-CV2/Fold{i}/hr_split_{i}.csv'\n",
    "    lr_train = np.genfromtxt(lr_train_path, delimiter=',', skip_header=1)\n",
    "    hr_train = np.genfromtxt(hr_train_path, delimiter=',', skip_header=1)\n",
    "    print(lr_train.shape)\n",
    "    print(hr_train.shape)\n",
    "    contains_nan = np.isnan(lr_train).any() or np.isnan(hr_train).any()\n",
    "    contains_negative = (lr_train < 0).any() or (hr_train < 0).any()\n",
    "    print(\"Contains NaN:\", contains_nan)\n",
    "    print(\"Contains negative numbers:\", contains_negative)\n",
    "    lrs.append(lr_train)\n",
    "    hrs.append(hr_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Search for Early Stopping Point by K-Fold Cross Validation (Fair Comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "models = []\n",
    "losseses = []\n",
    "ess = []\n",
    "\n",
    "batch_size = 8\n",
    "for i in range(0,3):\n",
    "    lr_train = []\n",
    "    hr_train = []\n",
    "    lr_validate = []\n",
    "    hr_validate = []\n",
    "    lr_test = []\n",
    "    hr_test = []\n",
    "    if i == 0:\n",
    "        lr_train = np.concatenate((lrs[1][:-10], lrs[2][:-10]), axis=0)\n",
    "        hr_train = np.concatenate((hrs[1][:-10], hrs[2][:-10]), axis=0)\n",
    "        lr_validate = np.concatenate((lrs[1][-10:], lrs[2][-10:]), axis=0)\n",
    "        hr_validate = np.concatenate((hrs[1][-10:], hrs[2][-10:]), axis=0)\n",
    "        lr_test = lrs[0]\n",
    "        hr_test = hrs[0]\n",
    "    elif i == 1:\n",
    "        lr_train = np.concatenate((lrs[0], lrs[2]), axis=0)\n",
    "        hr_train = np.concatenate((hrs[0], hrs[2]), axis=0)\n",
    "        lr_validate = np.concatenate((lrs[0][-10:], lrs[2][-10:]), axis=0)\n",
    "        hr_validate = np.concatenate((hrs[0][-10:], hrs[2][-10:]), axis=0)\n",
    "        lr_test = lrs[1]\n",
    "        hr_test = hrs[1]\n",
    "    elif i == 2:\n",
    "        lr_train = np.concatenate((lrs[0], lrs[1]), axis=0)\n",
    "        hr_train = np.concatenate((hrs[0], hrs[1]), axis=0)\n",
    "        lr_validate = np.concatenate((lrs[0][-10:], lrs[1][-10:]), axis=0)\n",
    "        hr_validate = np.concatenate((hrs[0][-10:], hrs[1][-10:]), axis=0)\n",
    "        lr_test = lrs[2]\n",
    "        hr_test = hrs[2]\n",
    "\n",
    "\n",
    "    lr_train_matrix = torch.from_numpy(np.array([anti_vectorize(lr_train[i], 160) for i in range(lr_train.shape[0])])).float().to(device)\n",
    "    hr_train_matrix = torch.from_numpy(np.array([anti_vectorize(hr_train[i], 268) for i in range(lr_train.shape[0])])).float().to(device)\n",
    "    lr_test_matrix = torch.from_numpy(np.array([anti_vectorize(lr_test[i], 160) for i in range(lr_test.shape[0])])).float().to(device)\n",
    "    hr_test_matrix = torch.from_numpy(np.array([anti_vectorize(hr_test[i], 268) for i in range(lr_test.shape[0])])).float().to(device)\n",
    "    lr_validate_matrix = torch.from_numpy(np.array([anti_vectorize(lr_validate[i], 160) for i in range(lr_validate.shape[0])])).float().to(device)\n",
    "    hr_validate_matrix = torch.from_numpy(np.array([anti_vectorize(hr_validate[i], 268) for i in range(lr_validate.shape[0])])).float().to(device)\n",
    "    \n",
    "    train_data = TensorDataset(torch.from_numpy(lr_train), torch.from_numpy(hr_train), lr_train_matrix, hr_train_matrix)\n",
    "    validate_data = TensorDataset(torch.from_numpy(lr_validate), torch.from_numpy(hr_validate), lr_validate_matrix, hr_validate_matrix)\n",
    "    \n",
    "    \n",
    "    num_epochs = 300\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    validate_loader = DataLoader(validate_data, batch_size=batch_size, shuffle=False)\n",
    "    model = GraphCycleGAN(12720, 35778).to(device)\n",
    "    model_n, losses, es = train_graph_cyclegan_es(model, train_loader, validate_loader, epochs=num_epochs, device=device)\n",
    "    models.append(model_n)\n",
    "    losseses.append(losses)\n",
    "    ess.append(es)\n",
    "    \n",
    "print(f'models: {models}')\n",
    "print(f'losses: {losseses}')\n",
    "print(f'ess: {ess}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Training and Evaluation on Random-CV and Cluster-CV (Paper Benchmark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "for i in range(0,3):\n",
    "    lr_train = []\n",
    "    hr_train = []\n",
    "    lr_test = []\n",
    "    hr_test = []\n",
    "    if i == 0:\n",
    "        lr_train = np.concatenate((lrs[1], lrs[2]), axis=0)\n",
    "        hr_train = np.concatenate((hrs[1], hrs[2]), axis=0)\n",
    "        lr_test = lrs[0]\n",
    "        hr_test = hrs[0]\n",
    "    elif i == 1:\n",
    "        lr_train = np.concatenate((lrs[0], lrs[2]), axis=0)\n",
    "        hr_train = np.concatenate((hrs[0], hrs[2]), axis=0)\n",
    "        lr_test = lrs[1]\n",
    "        hr_test = hrs[1]\n",
    "    elif i == 2:\n",
    "        lr_train = np.concatenate((lrs[0], lrs[1]), axis=0)\n",
    "        hr_train = np.concatenate((hrs[0], hrs[1]), axis=0)\n",
    "        lr_test = lrs[2]\n",
    "        hr_test = hrs[2]\n",
    "\n",
    "\n",
    "    lr_train_matrix = torch.from_numpy(np.array([anti_vectorize(lr_train[i], 160) for i in range(lr_train.shape[0])])).float().to(device)\n",
    "    hr_train_matrix = torch.from_numpy(np.array([anti_vectorize(hr_train[i], 268) for i in range(lr_train.shape[0])])).float().to(device)\n",
    "    lr_test_matrix = torch.from_numpy(np.array([anti_vectorize(lr_test[i], 160) for i in range(lr_test.shape[0])])).float().to(device)\n",
    "    hr_test_matrix = torch.from_numpy(np.array([anti_vectorize(hr_test[i], 268) for i in range(lr_test.shape[0])])).float().to(device)\n",
    "    \n",
    "    train_data = TensorDataset(torch.from_numpy(lr_train), torch.from_numpy(hr_train), lr_train_matrix, hr_train_matrix)\n",
    "    \n",
    "    \n",
    "    num_epochs = 100\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    model = GraphCycleGAN(12720, 35778).to(device)\n",
    "    train_graph_cyclegan(model, train_loader, epochs=num_epochs, device=device)\n",
    "    hr_test_predict = model.G_A2B(torch.from_numpy(lr_test).float().to(device)).cpu().detach().numpy()\n",
    "    print(f'shape: {hr_test_predict.shape}')\n",
    "    hr_predict_matrix = torch.from_numpy(np.array([anti_vectorize(hr_test_predict[i], 268) for i in range(lr_test.shape[0])])).float().to(device)\n",
    "    hr_predict_matrix = torch.where(hr_predict_matrix < 0, torch.tensor(0.0).to(device), hr_predict_matrix)\n",
    "    print(f'anti shape: {hr_predict_matrix.shape}')\n",
    "    evaluate_all(hr_test_matrix.cpu().detach().numpy(), hr_predict_matrix.cpu().detach().numpy(), f'Paper_RANDOM2_{i+1}')\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "12Jg3ox0G2TJ",
    "bzi9DIRbVeVF",
    "1TIkuiUcXt21",
    "mbmP7M4MWy0Q"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
