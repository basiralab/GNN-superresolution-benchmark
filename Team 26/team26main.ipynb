{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7jUXbH9_55Az"
   },
   "source": [
    "# Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4518,
     "status": "ok",
     "timestamp": 1716530862665,
     "user": {
      "displayName": "yiying guan",
      "userId": "06690427779299886783"
     },
     "user_tz": -60
    },
    "id": "qRYDCz9v5sY0",
    "outputId": "404389fd-5306-41bf-f912-045232f0d280"
   },
   "outputs": [],
   "source": [
    "!pip install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d0FrtWWn6EKX"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import networkx as nx\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.spatial.distance import jensenshannon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xDvw_9cZ7q2x"
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8zEckq-icQqN"
   },
   "source": [
    "## MatrixVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BOkg4NPdcDRX"
   },
   "outputs": [],
   "source": [
    "class MatrixVectorizer:\n",
    "    \"\"\"\n",
    "    A class for transforming between matrices and vector representations.\n",
    "\n",
    "    This class provides methods to convert a symmetric matrix into a vector (vectorize)\n",
    "    and to reconstruct the matrix from its vector form (anti_vectorize), focusing on\n",
    "    vertical (column-based) traversal and handling of elements.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes the MatrixVectorizer instance.\n",
    "\n",
    "        The constructor currently does not perform any actions but is included for\n",
    "        potential future extensions where initialization parameters might be required.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def vectorize(matrix, include_diagonal=False):\n",
    "        \"\"\"\n",
    "        Converts a matrix into a vector by vertically extracting elements.\n",
    "\n",
    "        This method traverses the matrix column by column, collecting elements from the\n",
    "        upper triangle, and optionally includes the diagonal elements immediately below\n",
    "        the main diagonal based on the include_diagonal flag.\n",
    "\n",
    "        Parameters:\n",
    "        - matrix (numpy.ndarray): The matrix to be vectorized.\n",
    "        - include_diagonal (bool, optional): Flag to include diagonal elements in the vectorization.\n",
    "          Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "        - numpy.ndarray: The vectorized form of the matrix.\n",
    "        \"\"\"\n",
    "        # Determine the size of the matrix based on its first dimension\n",
    "        matrix_size = matrix.shape[0]\n",
    "\n",
    "        # Initialize an empty list to accumulate vector elements\n",
    "        vector_elements = []\n",
    "\n",
    "        # Iterate over columns and then rows to collect the relevant elements\n",
    "        for col in range(matrix_size):\n",
    "            for row in range(matrix_size):\n",
    "                # Skip diagonal elements if not including them\n",
    "                if row != col:\n",
    "                    if row < col:\n",
    "                        # Collect upper triangle elements\n",
    "                        vector_elements.append(matrix[row, col])\n",
    "                    elif include_diagonal and row == col + 1:\n",
    "                        # Optionally include the diagonal elements immediately below the diagonal\n",
    "                        vector_elements.append(matrix[row, col])\n",
    "\n",
    "        return np.array(vector_elements)\n",
    "\n",
    "    @staticmethod\n",
    "    def anti_vectorize(vector, matrix_size, include_diagonal=False):\n",
    "        \"\"\"\n",
    "        Reconstructs a matrix from its vector form, filling it vertically.\n",
    "\n",
    "        The method fills the matrix by reflecting vector elements into the upper triangle\n",
    "        and optionally including the diagonal elements based on the include_diagonal flag.\n",
    "\n",
    "        Parameters:\n",
    "        - vector (numpy.ndarray): The vector to be transformed into a matrix.\n",
    "        - matrix_size (int): The size of the square matrix to be reconstructed.\n",
    "        - include_diagonal (bool, optional): Flag to include diagonal elements in the reconstruction.\n",
    "          Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "        - numpy.ndarray: The reconstructed square matrix.\n",
    "        \"\"\"\n",
    "        # Initialize a square matrix of zeros with the specified size\n",
    "        matrix = np.zeros((matrix_size, matrix_size))\n",
    "\n",
    "        # Index to keep track of the current position in the vector\n",
    "        vector_idx = 0\n",
    "\n",
    "        # Fill the matrix by iterating over columns and then rows\n",
    "        for col in range(matrix_size):\n",
    "            for row in range(matrix_size):\n",
    "                # Skip diagonal elements if not including them\n",
    "                if row != col:\n",
    "                    if row < col:\n",
    "                        # Reflect vector elements into the upper triangle and its mirror in the lower triangle\n",
    "                        matrix[row, col] = vector[vector_idx]\n",
    "                        matrix[col, row] = vector[vector_idx]\n",
    "                        vector_idx += 1\n",
    "                    elif include_diagonal and row == col + 1:\n",
    "                        # Optionally fill the diagonal elements after completing each column\n",
    "                        matrix[row, col] = vector[vector_idx]\n",
    "                        matrix[col, row] = vector[vector_idx]\n",
    "                        vector_idx += 1\n",
    "\n",
    "        return matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jI7RcubUcTYO"
   },
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7joddMIB72K_"
   },
   "outputs": [],
   "source": [
    "def pad_HR_adj(label, split):\n",
    "  \"\"\"\n",
    "  Pads the adjacency matrix with zeros to match the size of the HR adjacency matrix\n",
    "\n",
    "  Parameters:\n",
    "  - label(np.array): the adjacency matrix\n",
    "  - split(int): the number of zeros to pad\n",
    "\n",
    "  Returns:\n",
    "  - torch.tensor: the padded adjacency matrix\n",
    "  \"\"\"\n",
    "\n",
    "  label=np.pad(label,((split,split),(split,split)),mode=\"constant\")\n",
    "  np.fill_diagonal(label,1)\n",
    "  return torch.from_numpy(label).type(torch.FloatTensor)\n",
    "\n",
    "def normalize_adj_torch(mx):\n",
    "    \"\"\"\n",
    "    Normalize the adjacency matrix\n",
    "\n",
    "    Parameters:\n",
    "    - mx(torch.tensor): the adjacency matrix\n",
    "\n",
    "    Returns:\n",
    "    - mx(torch.tensor): the normalized adjacency matrix\n",
    "    \"\"\"\n",
    "    rowsum = mx.sum(1)\n",
    "    r_inv_sqrt = torch.pow(rowsum, -0.5).flatten()\n",
    "    r_inv_sqrt[torch.isinf(r_inv_sqrt)] = 0.\n",
    "    r_mat_inv_sqrt = torch.diag(r_inv_sqrt)\n",
    "    mx = torch.matmul(mx, r_mat_inv_sqrt)\n",
    "    mx = torch.transpose(mx, 0, 1)\n",
    "    mx = torch.matmul(mx, r_mat_inv_sqrt)\n",
    "    return mx\n",
    "\n",
    "def unpad(data, split):\n",
    "  \"\"\"\n",
    "  Unpads the data to its original size\n",
    "\n",
    "  Parameters:\n",
    "  - data(np.array): the padded data\n",
    "  - split(int): the number of zeros to remove\n",
    "\n",
    "  Returns:\n",
    "  - teain(np.array): the unpadded data\n",
    "  \"\"\"\n",
    "  idx_0 = data.shape[0]-split\n",
    "  idx_1 = data.shape[1]-split\n",
    "  train = data[split:idx_0, split:idx_1]\n",
    "  return train\n",
    "\n",
    "def data():\n",
    "  \"\"\"\n",
    "  Loads the data and returns the training and testing data\n",
    "\n",
    "  Returns:\n",
    "  - train_adj(np.array): the training data (adjacency matrix)\n",
    "  - train_labels(np.array): the training data (labels)\n",
    "  - test_adj(np.array): the testing data (adjacency matrix)\n",
    "  \"\"\"\n",
    "\n",
    "  subjects_adj_split_1 = pd.read_csv('Random-CV/Fold1/lr_split_1.csv')\n",
    "  subjects_labels_split_1 = pd.read_csv('Random-CV/Fold1/hr_split_1.csv')\n",
    "  subjects_adj_split_2 = pd.read_csv('Random-CV/Fold2/lr_split_2.csv')\n",
    "  subjects_labels_split_2 = pd.read_csv('Random-CV/Fold2/hr_split_2.csv')\n",
    "  subjects_adj_split_3 = pd.read_csv('Random-CV/Fold3/lr_split_3.csv')\n",
    "  subjects_labels_split_3 = pd.read_csv('Random-CV/Fold3/hr_split_3.csv')\n",
    "  test_adj = pd.read_csv('Random-CV/Fold1/lr_split_1.csv')\n",
    "  test_labels = pd.read_csv('Random-CV/Fold1/lr_split_1.csv')\n",
    "\n",
    "  # transform to numpy array\n",
    "  subjects_adj_split_1 = subjects_adj_split_1.to_numpy()\n",
    "  subjects_labels_split_1 = subjects_labels_split_1.to_numpy()\n",
    "  subjects_adj_split_2 = subjects_adj_split_2.to_numpy()\n",
    "  subjects_labels_split_2 = subjects_labels_split_2.to_numpy()\n",
    "  subjects_adj_split_3 = subjects_adj_split_3.to_numpy()\n",
    "  subjects_labels_split_3 = subjects_labels_split_3.to_numpy()\n",
    "  test_adj = test_adj.to_numpy()\n",
    "  test_labels = test_labels.to_numpy()\n",
    "\n",
    "  # Reshape the data\n",
    "  new_shape_subjects_adj_split_1 = (subjects_adj_split_1.shape[0], 160, 160)\n",
    "  new_shape_subjects_labels_split_1 = (subjects_labels_split_1.shape[0], 268, 268)\n",
    "  subjects_adj_3d_split_1 = np.empty(new_shape_subjects_adj_split_1)\n",
    "  subjects_labels_3d_split_1 = np.empty(new_shape_subjects_labels_split_1)\n",
    "\n",
    "  new_shape_subjects_adj_split_2 = (subjects_adj_split_2.shape[0], 160, 160)\n",
    "  new_shape_subjects_labels_split_2 = (subjects_labels_split_2.shape[0], 268, 268)\n",
    "  subjects_adj_3d_split_2 = np.empty(new_shape_subjects_adj_split_2)\n",
    "  subjects_labels_3d_split_2 = np.empty(new_shape_subjects_labels_split_2)\n",
    "\n",
    "  new_shape_subjects_adj_split_3 = (subjects_adj_split_3.shape[0], 160, 160)\n",
    "  new_shape_subjects_labels_split_3 = (subjects_labels_split_3.shape[0], 268, 268)\n",
    "  subjects_adj_3d_split_3 = np.empty(new_shape_subjects_adj_split_3)\n",
    "  subjects_labels_3d_split_3 = np.empty(new_shape_subjects_labels_split_3)\n",
    "\n",
    "\n",
    "  new_shape_adj = (test_adj.shape[0], 160, 160)\n",
    "  test_adj_3d = np.empty(new_shape_adj)\n",
    "\n",
    "  # Perform anti-vectorization in the loop\n",
    "  for i in range(subjects_adj_split_1.shape[0]):\n",
    "      # Apply anti-vectorization and reshape the i-th 1D slice to a 2D array\n",
    "      subjects_adj_3d_split_1[i] = MatrixVectorizer.anti_vectorize(subjects_adj_split_1[i], 160).reshape(160, 160)\n",
    "      subjects_labels_3d_split_1[i] = MatrixVectorizer.anti_vectorize(subjects_labels_split_1[i], 268).reshape(268, 268)\n",
    "  for i in range(subjects_adj_split_2.shape[0]):\n",
    "      # Apply anti-vectorization and reshape the i-th 1D slice to a 2D array\n",
    "      subjects_adj_3d_split_2[i] = MatrixVectorizer.anti_vectorize(subjects_adj_split_2[i], 160).reshape(160, 160)\n",
    "      subjects_labels_3d_split_2[i] = MatrixVectorizer.anti_vectorize(subjects_labels_split_2[i], 268).reshape(268, 268)\n",
    "  for i in range(subjects_adj_split_3.shape[0]):\n",
    "      # Apply anti-vectorization and reshape the i-th 1D slice to a 2D array\n",
    "      subjects_adj_3d_split_3[i] = MatrixVectorizer.anti_vectorize(subjects_adj_split_3[i], 160).reshape(160, 160)\n",
    "      subjects_labels_3d_split_3[i] = MatrixVectorizer.anti_vectorize(subjects_labels_split_3[i], 268).reshape(268, 268)\n",
    "  for i in range(test_adj.shape[0]):\n",
    "      # Apply anti-vectorization and reshape the i-th 1D slice to a 2D array\n",
    "      test_adj_3d[i] = MatrixVectorizer.anti_vectorize(test_adj[i], 160).reshape(160, 160)\n",
    "\n",
    "  train_adj_spit_1 = subjects_adj_3d_split_1\n",
    "  train_labels_spit_1 = subjects_labels_3d_split_1\n",
    "  train_adj_spit_2 = subjects_adj_3d_split_2\n",
    "  train_labels_spit_2 = subjects_labels_3d_split_2\n",
    "  train_adj_spit_3 = subjects_adj_3d_split_3\n",
    "  train_labels_spit_3 = subjects_labels_3d_split_3\n",
    "  test_adj = test_adj_3d\n",
    "\n",
    "  return train_adj_spit_1, train_labels_spit_1, train_adj_spit_2, train_labels_spit_2, train_adj_spit_3, train_labels_spit_3, test_adj, None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VhITGrXnc8o_"
   },
   "source": [
    "# Model Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QSz57SCodSlf"
   },
   "outputs": [],
   "source": [
    "# initializations(Glorot)\n",
    "def weight_variable_glorot(output_dim):\n",
    "    \"\"\"\n",
    "    Initialize weights according to Glorot initialization.\n",
    "\n",
    "    Parameters:\n",
    "    - output_dim(int): the number of output dimensions\n",
    "\n",
    "    Returns:\n",
    "    - initial(np.array): the initialized weights\n",
    "    \"\"\"\n",
    "    input_dim = output_dim\n",
    "    init_range = np.sqrt(6.0 / (input_dim + output_dim))\n",
    "    initial = np.random.uniform(-init_range, init_range,(input_dim, output_dim))\n",
    "\n",
    "    return initial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WSdaHyd-dO2k"
   },
   "outputs": [],
   "source": [
    "class GSRLayer(nn.Module):\n",
    "  \"\"\"\n",
    "  The Graph Spectral Regularization layer\n",
    "  \"\"\"\n",
    "  def __init__(self,hr_dim):\n",
    "    super(GSRLayer, self).__init__()\n",
    "\n",
    "    self.weights = torch.from_numpy(weight_variable_glorot(hr_dim)).type(torch.FloatTensor)\n",
    "    self.weights = torch.nn.Parameter(data=self.weights, requires_grad = True)\n",
    "\n",
    "  def forward(self,A,X):\n",
    "    \"\"\"\n",
    "    Forward pass for the GSR layer\n",
    "\n",
    "    Parameters:\n",
    "    - A(torch.tensor): the adjacency matrix\n",
    "    - X(torch.tensor): the input data\n",
    "\n",
    "    Returns:\n",
    "    - adj(torch.tensor): the adjacency matrix\n",
    "    - torch.abs(X)(torch.tensor): the input data\n",
    "    \"\"\"\n",
    "    lr = A\n",
    "    lr_dim = lr.shape[0]\n",
    "    f = X\n",
    "    # Compute eigenvalues and eigenvectors\n",
    "    _, U_lr = torch.linalg.eigh(A, UPLO='U')\n",
    "    eye_mat = torch.eye(lr_dim).type(torch.FloatTensor).to(X.device)\n",
    "    s_d = torch.cat((eye_mat,eye_mat),0)\n",
    "\n",
    "    # Compute the GSR\n",
    "    a = torch.matmul(self.weights,s_d )\n",
    "    b = torch.matmul(a ,torch.t(U_lr))\n",
    "    f_d = torch.matmul(b ,f)\n",
    "    f_d = torch.abs(f_d)\n",
    "    self.f_d = f_d.fill_diagonal_(1)\n",
    "    adj = normalize_adj_torch(self.f_d)\n",
    "    X = torch.mm(adj, adj.t())\n",
    "    X = (X + X.t())/2\n",
    "    idx = torch.eye(320, dtype=bool)\n",
    "    X[idx]=1\n",
    "    return adj, torch.abs(X)\n",
    "\n",
    "\n",
    "class DropGNN(nn.Module):\n",
    "    \"\"\"\n",
    "    The DropGNN layer\n",
    "    \"\"\"\n",
    "    def __init__(self, p_dropgnn):\n",
    "        super(DropGNN, self).__init__()\n",
    "        self.p_dropgnn = p_dropgnn\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        \"\"\"\n",
    "        Forward pass for the DropGNN layer\n",
    "\n",
    "        Parameters:\n",
    "        - x(torch.tensor): the input data\n",
    "        - adj(torch.tensor): the adjacency matrix\n",
    "\n",
    "        Returns:\n",
    "        - x(torch.tensor): the input data\n",
    "        - adj_normalized(torch.tensor): the normalized adjacency matrix\n",
    "        \"\"\"\n",
    "        drop = torch.bernoulli(torch.ones([x.size(0), x.size(1)], device=x.device) * self.p_dropgnn).bool()\n",
    "        x[drop] = 0\n",
    "        adj = adj * drop.unsqueeze(1) * drop.unsqueeze(2)\n",
    "\n",
    "        adj = adj + torch.eye(adj.size(0)).type(torch.FloatTensor).to(x.device)\n",
    "        degree = torch.sum(adj, dim=1)\n",
    "        degree_sqrt_inv = torch.pow(degree, -0.5)\n",
    "        degree_sqrt_inv[degree_sqrt_inv == float('inf')] = 0.\n",
    "        adj_normalized = torch.matmul(torch.matmul(torch.diag(degree_sqrt_inv), adj), torch.diag(degree_sqrt_inv))\n",
    "        return x, adj_normalized\n",
    "\n",
    "\n",
    "class GraphConvolution(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
    "    \"\"\"\n",
    "\n",
    "    # 160x320 320x320 =  160x320\n",
    "    def __init__(self, in_features, out_features, dropout=0., p_dropgnn=0., act=F.relu):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.dropout = dropout\n",
    "        self.p_dropgnn = p_dropgnn\n",
    "        self.act = act\n",
    "        self.weight = torch.nn.Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        torch.nn.init.xavier_uniform_(self.weight)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        \"\"\"\n",
    "        Forward pass for the GraphConvolution layer\n",
    "\n",
    "        Parameters:\n",
    "        - input(torch.tensor): the input data\n",
    "        - adj(torch.tensor): the adjacency matrix\n",
    "\n",
    "        Returns:\n",
    "        - output(torch.tensor): the output data\n",
    "        \"\"\"\n",
    "        dropgnn = DropGNN(self.p_dropgnn)\n",
    "        input, _ = dropgnn(input, adj)\n",
    "\n",
    "        support = torch.mm(input, self.weight)\n",
    "        output = torch.mm(adj, support)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BKF1r23gdymz"
   },
   "outputs": [],
   "source": [
    "class GraphUnpool(nn.Module):\n",
    "    \"\"\"\n",
    "    Graph Unpooling layer\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(GraphUnpool, self).__init__()\n",
    "\n",
    "    def forward(self, A, X, idx):\n",
    "        \"\"\"\n",
    "        Forward pass for the GraphUnpool layer\n",
    "\n",
    "        Parameters:\n",
    "        - A(torch.tensor): the adjacency matrix\n",
    "        - X(torch.tensor): the input data\n",
    "        - idx(torch.tensor): the index\n",
    "\n",
    "        Returns:\n",
    "        - A(torch.tensor): the adjacency matrix\n",
    "        - new_X(torch.tensor): the input data\n",
    "        \"\"\"\n",
    "        new_X = torch.zeros([A.shape[0], X.shape[1]]).to(X.device)\n",
    "        new_X[idx] = X\n",
    "        return A, new_X\n",
    "\n",
    "\n",
    "class GraphPool(nn.Module):\n",
    "    \"\"\"\n",
    "    Graph Pooling layer\n",
    "    \"\"\"\n",
    "    def __init__(self, k, in_dim, hidden_dim=64):\n",
    "        super(GraphPool, self).__init__()\n",
    "        self.k = k\n",
    "        self.proj = nn.Linear(in_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            # nn.Dropout(0.5),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, A, X):\n",
    "        \"\"\"\n",
    "        Forward pass for the GraphPool layer\n",
    "\n",
    "        Parameters:\n",
    "        - A(torch.tensor): the adjacency matrix\n",
    "        - X(torch.tensor): the input data\n",
    "\n",
    "        Returns:\n",
    "        - A(torch.tensor): the adjacency matrix\n",
    "        - new_X(torch.tensor): the input data\n",
    "        - idx(torch.tensor): the index\n",
    "        \"\"\"\n",
    "        scores = self.mlp(X).squeeze()\n",
    "        scores = torch.sigmoid(scores)\n",
    "        num_nodes = A.shape[0]\n",
    "        values, idx = torch.topk(scores, int(self.k * num_nodes))\n",
    "        new_X = X[idx, :]\n",
    "        values = torch.unsqueeze(values, -1)\n",
    "        new_X = torch.mul(new_X, values)\n",
    "        A = A[idx, :]\n",
    "        A = A[:, idx]\n",
    "        return A, new_X, idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J37Pc3I8ItmE"
   },
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    \"\"\"\n",
    "    The Graph Convolutional Network\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(GCN, self).__init__()\n",
    "        self.proj = nn.Linear(in_dim, out_dim)\n",
    "        self.drop = nn.Dropout(p=0)\n",
    "\n",
    "    def forward(self, A, X):\n",
    "        \"\"\"\n",
    "        Forward pass for the GCN layer\n",
    "\n",
    "        Parameters:\n",
    "        - A(torch.tensor): the adjacency matrix\n",
    "        - X(torch.tensor): the input data\n",
    "\n",
    "        Returns:\n",
    "        - X(torch.tensor): the input data\n",
    "        \"\"\"\n",
    "        X = self.drop(X)\n",
    "        # X = torch.matmul(A, X)\n",
    "        X = self.proj(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mGW-fU0FItmF"
   },
   "outputs": [],
   "source": [
    "class GraphAttentionLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple GAT layer, similar to https://arxiv.org/abs/1710.10903\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, dropout=0., alpha=0.2, concat=True):\n",
    "        super(GraphAttentionLayer, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.alpha = alpha\n",
    "        self.concat = concat\n",
    "\n",
    "        self.W = nn.Parameter(torch.empty(size=(in_features, out_features)))\n",
    "        nn.init.xavier_uniform_(self.W.data, gain=1.414)\n",
    "        self.a = nn.Parameter(torch.empty(size=(2 * out_features, 1)))\n",
    "        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n",
    "\n",
    "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
    "\n",
    "    def forward(self, h, adj):\n",
    "        \"\"\"\n",
    "        Forward pass for the GraphAttentionLayer\n",
    "\n",
    "        Parameters:\n",
    "        - h(torch.tensor): the input data\n",
    "        - adj(torch.tensor): the adjacency matrix\n",
    "\n",
    "        Returns:\n",
    "        - h_prime(torch.tensor): the output data\n",
    "        \"\"\"\n",
    "        Wh = torch.mm(h, self.W)  # h.shape: (N, in_features), Wh.shape: (N, out_features)\n",
    "        e = self._prepare_attentional_mechanism_input(Wh)\n",
    "\n",
    "        zero_vec = - torch.inf * torch.ones_like(e)\n",
    "        attention = torch.where(adj > 0, e, zero_vec)\n",
    "        attention = F.softmax(attention, dim=1)\n",
    "        attention = F.dropout(attention, self.dropout, training=self.training)\n",
    "        h_prime = torch.matmul(attention, Wh)\n",
    "\n",
    "        if self.concat:\n",
    "            return F.elu(h_prime)\n",
    "        else:\n",
    "            return h_prime\n",
    "\n",
    "    def _prepare_attentional_mechanism_input(self, Wh):\n",
    "        \"\"\"\n",
    "        Prepare the input for the attentional mechanism\n",
    "        \"\"\"\n",
    "        Wh1 = torch.matmul(Wh, self.a[:self.out_features, :])\n",
    "        Wh2 = torch.matmul(Wh, self.a[self.out_features:, :])\n",
    "        # broadcast add\n",
    "        e = Wh1 + Wh2.T\n",
    "        return self.leakyrelu(e)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' + str(self.in_features) + ' -> ' + str(self.out_features) + ')'\n",
    "\n",
    "\n",
    "class GAT(nn.Module):\n",
    "    \"\"\"\n",
    "    The Graph Attention Network\n",
    "    \"\"\"\n",
    "    def __init__(self, nfeat, nhid, nclass, dropout, alpha, nheads):\n",
    "        \"\"\"Dense version of GAT.\"\"\"\n",
    "        super(GAT, self).__init__()\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.attentions = [GraphAttentionLayer(nfeat, nhid, dropout=dropout, alpha=alpha, concat=True) for _ in\n",
    "                           range(nheads)]\n",
    "        for i, attention in enumerate(self.attentions):\n",
    "            self.add_module('attention_{}'.format(i), attention)\n",
    "\n",
    "        self.out_att = GraphAttentionLayer(nhid * nheads, nclass, dropout=dropout, alpha=alpha, concat=False)\n",
    "\n",
    "    def forward(self, adj, x):\n",
    "        \"\"\"\n",
    "        Forward pass for the GAT layer\n",
    "\n",
    "        Parameters:\n",
    "        - adj(torch.tensor): the adjacency matrix\n",
    "        - x(torch.tensor): the input data\n",
    "\n",
    "        Returns:\n",
    "        - x(torch.tensor): the input data\n",
    "        \"\"\"\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = torch.cat([att(x, adj) for att in self.attentions], dim=1)\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = F.elu(self.out_att(x, adj))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z2AiCIOql0AF"
   },
   "outputs": [],
   "source": [
    "class GraphUnet(nn.Module):\n",
    "    \"\"\"\n",
    "    The Graph Unet\n",
    "    \"\"\"\n",
    "    def __init__(self, ks, in_dim, out_dim, dim=320):\n",
    "        super(GraphUnet, self).__init__()\n",
    "        self.ks = ks\n",
    "\n",
    "        self.start_gcn = GCN(in_dim, dim)\n",
    "        self.bottom_gcn = GCN(dim, dim)\n",
    "        self.end_gcn = GCN(2 * dim, out_dim)\n",
    "        self.down_gcns = []\n",
    "        self.up_gcns = []\n",
    "        self.pools = []\n",
    "        self.unpools = []\n",
    "        self.l_n = len(ks)\n",
    "        for i in range(self.l_n):\n",
    "            # self.down_gcns.append(GCN(dim, dim))\n",
    "            self.down_gcns.append(GAT(dim, dim, dim, 0., 0.2, 1))\n",
    "            # self.up_gcns.append(GCN(dim, dim))\n",
    "            self.up_gcns.append(GAT(dim, dim, dim, 0., 0.2, 1))\n",
    "            self.pools.append(GraphPool(ks[i], dim))\n",
    "            self.unpools.append(GraphUnpool())\n",
    "        self.down_gcns = nn.ModuleList(self.down_gcns)\n",
    "        self.up_gcns = nn.ModuleList(self.up_gcns)\n",
    "        self.pools = nn.ModuleList(self.pools)\n",
    "        self.unpools = nn.ModuleList(self.unpools)\n",
    "\n",
    "    def forward(self, A, X):\n",
    "        \"\"\"\n",
    "        Forward pass for the GraphUnet\n",
    "\n",
    "        Parameters:\n",
    "        - A(torch.tensor): the adjacency matrix\n",
    "        - X(torch.tensor): the input data\n",
    "\n",
    "        Returns:\n",
    "        - X(torch.tensor): the input data\n",
    "        \"\"\"\n",
    "        adj_ms = []\n",
    "        indices_list = []\n",
    "        down_outs = []\n",
    "        X = self.start_gcn(A, X)\n",
    "        start_gcn_outs = X\n",
    "        org_X = X\n",
    "        for i in range(self.l_n):\n",
    "            X = self.down_gcns[i](A, X)\n",
    "            adj_ms.append(A)\n",
    "            down_outs.append(X)\n",
    "            A, X, idx = self.pools[i](A, X)\n",
    "            indices_list.append(idx)\n",
    "        X = self.bottom_gcn(A, X)\n",
    "        for i in range(self.l_n):\n",
    "            up_idx = self.l_n - i - 1\n",
    "\n",
    "            A, idx = adj_ms[up_idx], indices_list[up_idx]\n",
    "            A, X = self.unpools[i](A, X, idx)\n",
    "            X = self.up_gcns[i](A, X)\n",
    "            X = X.add(down_outs[up_idx])\n",
    "        X = torch.cat([X, org_X], 1)\n",
    "        X = self.end_gcn(A, X)\n",
    "\n",
    "        return X, start_gcn_outs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kTBsZRdbc_Ft"
   },
   "outputs": [],
   "source": [
    "class GSRNet(nn.Module):\n",
    "    \"\"\"\n",
    "    The GSR Network\n",
    "    \"\"\"\n",
    "    def __init__(self, ks, args):\n",
    "        super(GSRNet, self).__init__()\n",
    "\n",
    "        self.lr_dim = args.lr_dim\n",
    "        self.hr_dim = args.hr_dim\n",
    "        self.hidden_dim = args.hidden_dim\n",
    "        self.layer = GSRLayer(self.hr_dim)\n",
    "        self.net = GraphUnet(ks, self.lr_dim, self.hr_dim)\n",
    "        self.gc1 = GraphConvolution(self.hr_dim, self.hidden_dim, 0, p_dropgnn=0., act=F.relu)\n",
    "        self.gc2 = GraphConvolution(self.hidden_dim, self.hr_dim, 0, p_dropgnn=0., act=F.relu)\n",
    "\n",
    "    def forward(self, lr):\n",
    "        \"\"\"\n",
    "        Forward pass for the GSRNet\n",
    "\n",
    "        Parameters:\n",
    "        - lr(torch.tensor): the input data\n",
    "\n",
    "        Returns:\n",
    "        - torch.abs(z)(torch.tensor): the input data\n",
    "        - net_outs(torch.tensor): the output data\n",
    "        - start_gcn_outs(torch.tensor): the output data\n",
    "        - outputs(torch.tensor): the output data\n",
    "        \"\"\"\n",
    "        I = torch.eye(self.lr_dim).type(torch.FloatTensor).to(lr.device)\n",
    "        A = normalize_adj_torch(lr).type(torch.FloatTensor).to(lr.device)\n",
    "\n",
    "        self.net_outs, self.start_gcn_outs = self.net(A, I)\n",
    "\n",
    "        self.outputs, self.Z = self.layer(A, self.net_outs)\n",
    "\n",
    "        self.hidden1 = self.gc1(self.Z, self.outputs)\n",
    "        self.hidden2 = self.gc2(self.hidden1, self.outputs)\n",
    "\n",
    "        z = self.hidden2\n",
    "        z = (z + z.t()) / 2\n",
    "        idx = torch.eye(self.hr_dim, dtype=bool)\n",
    "        z[idx] = 1\n",
    "\n",
    "        return torch.abs(z), self.net_outs, self.start_gcn_outs, self.outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VQY9gk_k6TKQ"
   },
   "source": [
    "# Results Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "otrnCtWF6St0"
   },
   "outputs": [],
   "source": [
    "def plot_loss(all_epochs_loss):\n",
    "    \"\"\"\n",
    "    Plots the loss\n",
    "\n",
    "    Parameters:\n",
    "    - all_epochs_loss(list): the loss\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    for i in range(len(all_epochs_loss)):\n",
    "        plt.plot(range(len(all_epochs_loss[i])), all_epochs_loss[i], label = f'Fold {i}')\n",
    "    plt.title('Training Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_mae(val_mae):\n",
    "    \"\"\"\n",
    "    Plots the MAE\n",
    "\n",
    "    Parameters:\n",
    "    - val_mae(list): the MAE\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    plt.bar(range(len(val_mae)), val_mae)\n",
    "    plt.xlabel('Folds')\n",
    "    plt.ylabel('MAE')\n",
    "    plt.title('Validation MAE')\n",
    "    plt.show()\n",
    "\n",
    "def plot_fold_eval_measures(measure_vals, fold, measure_names, ext):\n",
    "    \"\"\"\n",
    "    Plots the evaluation measures for each fold\n",
    "\n",
    "    Parameters:\n",
    "    - measure_vals(list): the evaluation measures\n",
    "    - fold(int): the fold\n",
    "    - measure_names(list): the names of the evaluation measures\n",
    "    - ext(str): the extension\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    x = np.arange(len(measure_names))  # the label locations\n",
    "    width = 0.25  # the width of the bars\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.bar(x, measure_vals, width, label=f'Fold {fold}', color=['red', 'blue', 'green'])\n",
    "\n",
    "    # Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "    ax.set_ylabel('Evaluation Measures')\n",
    "    ax.set_title(f'Fold {fold}')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(measure_names)\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "    plt.savefig(f'Result/eval_fold_{fold}_{ext}.png')\n",
    "    plt.show(block=False)\n",
    "\n",
    "\n",
    "def plot_eval_measures_separate(val_mae, val_pcc, val_js, val_avg_pc, val_avg_bc, val_avg_ec, precision1=4, precision2=6, label_size1=10, label_size2=6):\n",
    "    \"\"\"\n",
    "    Plots the evaluation measures for each fold separately\n",
    "\n",
    "    Parameters:\n",
    "    - val_mae(list): the MAE\n",
    "    - val_pcc(list): the PCC\n",
    "    - val_js(list): the JSD\n",
    "    - val_avg_pc(list): the average PC\n",
    "    - val_avg_bc(list): the average BC\n",
    "    - val_avg_ec(list): the average EC\n",
    "    - precision1(int): the precision\n",
    "    - precision2(int): the precision\n",
    "    - label_size1(int): the label size\n",
    "    - label_size2(int): the label size\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    data1 = np.array([val_mae, val_pcc, val_js])\n",
    "    data2 = np.array([val_avg_pc, val_avg_bc, val_avg_ec])\n",
    "    measure_names1 = ['MAE', 'PCC', 'JSD']\n",
    "    measure_names2 = ['MAE (PC)', 'MAE (EC)', 'MAE (BC)']\n",
    "\n",
    "    def plot_data(data, measure_names, precision, title, file_name , label_size):\n",
    "        barWidth = 0.25\n",
    "        r1 = np.arange(len(data))\n",
    "        r2 = [x + barWidth for x in r1]\n",
    "        r3 = [x + barWidth for x in r2]\n",
    "\n",
    "        plt.figure(figsize=(7, 4))\n",
    "\n",
    "        # for each fold, plot the evaluation measures\n",
    "        for i in range(data.shape[1]):\n",
    "            plt.bar(r1, data[:, i], width=barWidth, edgecolor='grey', label=f'Fold {i}')\n",
    "\n",
    "            # Add text on the bars\n",
    "            for j in range(len(r1)):\n",
    "                plt.text(r1[j], data[j, i] + data[j, i]*0.01, f'{data[j, i]:.{precision}f}', ha='center', fontsize=label_size)\n",
    "\n",
    "            r1 = [x + barWidth for x in r1]\n",
    "\n",
    "        plt.xlabel('Evaluation Measures', fontweight='bold')\n",
    "        plt.xticks([r + barWidth for r in range(len(data))], measure_names)\n",
    "        plt.ylabel('Values')\n",
    "        plt.title(title)\n",
    "\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'Result/{file_name}.png')\n",
    "        plt.show()\n",
    "\n",
    "    plot_data(data1, measure_names1, precision1, 'Evaluation Measures (MAE, PCC, JSD)', 'eval_measures1', label_size1)\n",
    "    plot_data(data2, measure_names2, precision2, 'Evaluation Measures (MAE (PC), MAE (EC), MAE (BC))', 'eval_measures2', label_size2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ugFCqFrbflTf"
   },
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VngAeATKfnhH"
   },
   "outputs": [],
   "source": [
    "def evaluate(pred_matrices, gt_matrices):\n",
    "    \"\"\"\n",
    "    Evaluates the model\n",
    "\n",
    "    Parameters:\n",
    "    - pred_matrices(np.array): the predicted matrices\n",
    "    - gt_matrices(np.array): the ground truth matrices\n",
    "\n",
    "    Returns:\n",
    "    - mae(float): the MAE\n",
    "    - pcc(float): the PCC\n",
    "    - js_dis(float): the JSD\n",
    "    - avg_mae_pc(float): the average MAE PC\n",
    "    - avg_mae_ec(float): the average MAE EC\n",
    "    - avg_mae_bc(float): the average MAE BC\n",
    "    \"\"\"\n",
    "    num_test_samples = pred_matrices.shape[0]\n",
    "\n",
    "    # post-processing\n",
    "    pred_matrices[pred_matrices < 0] = 0\n",
    "    gt_matrices[gt_matrices < 0] = 0\n",
    "\n",
    "    # Initialize lists to store MAEs for each centrality measure\n",
    "    mae_bc = []\n",
    "    mae_ec = []\n",
    "    mae_pc = []\n",
    "\n",
    "    pred_1d_list = []\n",
    "    gt_1d_list = []\n",
    "\n",
    "    # Iterate over each test sample\n",
    "    for i in range(num_test_samples):\n",
    "        print(f'On sample {i}')\n",
    "\n",
    "        # Convert adjacency matrices to NetworkX graphs\n",
    "        pred_graph = nx.from_numpy_array(pred_matrices[i], edge_attr=\"weight\")\n",
    "        gt_graph = nx.from_numpy_array(gt_matrices[i], edge_attr=\"weight\")\n",
    "\n",
    "        # Compute centrality measures\n",
    "        pred_bc = nx.betweenness_centrality(pred_graph, weight=\"weight\")\n",
    "        pred_ec = nx.eigenvector_centrality(pred_graph, weight=\"weight\")\n",
    "        pred_pc = nx.pagerank(pred_graph, weight=\"weight\")\n",
    "\n",
    "        gt_bc = nx.betweenness_centrality(gt_graph, weight=\"weight\")\n",
    "        gt_ec = nx.eigenvector_centrality(gt_graph, weight=\"weight\")\n",
    "        gt_pc = nx.pagerank(gt_graph, weight=\"weight\")\n",
    "\n",
    "        # Convert centrality dictionaries to lists\n",
    "        pred_bc_values = list(pred_bc.values())\n",
    "        pred_ec_values = list(pred_ec.values())\n",
    "        pred_pc_values = list(pred_pc.values())\n",
    "\n",
    "        gt_bc_values = list(gt_bc.values())\n",
    "        gt_ec_values = list(gt_ec.values())\n",
    "        gt_pc_values = list(gt_pc.values())\n",
    "\n",
    "        # Compute MAEs\n",
    "        mae_bc.append(mean_absolute_error(pred_bc_values, gt_bc_values))\n",
    "        mae_ec.append(mean_absolute_error(pred_ec_values, gt_ec_values))\n",
    "        mae_pc.append(mean_absolute_error(pred_pc_values, gt_pc_values))\n",
    "\n",
    "        # Vectorize matrices\n",
    "        pred_1d_list.append(MatrixVectorizer.vectorize(pred_matrices[i]))\n",
    "        gt_1d_list.append(MatrixVectorizer.vectorize(gt_matrices[i]))\n",
    "\n",
    "    # Compute average MAEs\n",
    "    avg_mae_bc = sum(mae_bc) / len(mae_bc)\n",
    "    avg_mae_ec = sum(mae_ec) / len(mae_ec)\n",
    "    avg_mae_pc = sum(mae_pc) / len(mae_pc)\n",
    "\n",
    "    # Concatenate flattened matrices\n",
    "    pred_1d = np.concatenate(pred_1d_list)\n",
    "    gt_1d = np.concatenate(gt_1d_list)\n",
    "\n",
    "    # Compute metrics\n",
    "    mae = mean_absolute_error(pred_1d, gt_1d)\n",
    "    pcc = pearsonr(pred_1d, gt_1d)[0]\n",
    "    js_dis = jensenshannon(pred_1d, gt_1d)\n",
    "\n",
    "    print(\"MAE: \", mae)\n",
    "    print(\"PCC: \", pcc)\n",
    "    print(\"Jensen-Shannon Distance: \", js_dis)\n",
    "    print(\"Average MAE betweenness centrality:\", avg_mae_bc)\n",
    "    print(\"Average MAE eigenvector centrality:\", avg_mae_ec)\n",
    "    print(\"Average MAE PageRank centrality:\", avg_mae_pc)\n",
    "\n",
    "    return mae, pcc, js_dis, avg_mae_pc, avg_mae_ec, avg_mae_bc\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bS4naqo8Un8v"
   },
   "source": [
    "evaluation for paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8876,
     "status": "ok",
     "timestamp": 1716530934239,
     "user": {
      "displayName": "yiying guan",
      "userId": "06690427779299886783"
     },
     "user_tz": -60
    },
    "id": "UouwCfWKUnY2",
    "outputId": "5240e02d-a2df-4e21-b875-15835d86bf2b"
   },
   "outputs": [],
   "source": [
    "!pip uninstall -y community\n",
    "!pip uninstall -y python-louvain\n",
    "!pip install python-louvain\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr, structural_similarity as ssim\n",
    "import community as community_louvain\n",
    "import os\n",
    "\n",
    "def calculate_centralities(adj_matrix):\n",
    "    if adj_matrix.shape[0] != adj_matrix.shape[1]:\n",
    "        raise ValueError(f\"Adjacency matrix is not square: shape={adj_matrix.shape}\")\n",
    "    print(f\"Processing adjacency matrix of shape: {adj_matrix.shape}\")\n",
    "\n",
    "    G = nx.from_numpy_array(adj_matrix)\n",
    "    partition = community_louvain.best_partition(G)\n",
    "\n",
    "    # Calculate the participation coefficient with the partition\n",
    "    pc_dict = participation_coefficient(G, partition)\n",
    "\n",
    "    # Calculate averages of centrality measures\n",
    "    pr = nx.pagerank(G, alpha=0.9)\n",
    "    ec = nx.eigenvector_centrality_numpy(G, max_iter=100)\n",
    "    bc = nx.betweenness_centrality(G, normalized=True, endpoints=False)\n",
    "    ns = np.array(list(nx.degree_centrality(G).values())) * (len(G.nodes()) - 1)\n",
    "    acc = nx.average_clustering(G, weight=None)\n",
    "\n",
    "    # Average participation coefficient\n",
    "    pc_avg = np.mean(list(pc_dict.values()))\n",
    "\n",
    "    return {\n",
    "        'pr': np.mean(list(pr.values())),\n",
    "        'ec': np.mean(list(ec.values())),\n",
    "        'bc': np.mean(list(bc.values())),\n",
    "        'ns': ns,\n",
    "        'pc': pc_avg,\n",
    "        'acc': acc\n",
    "    }\n",
    "\n",
    "def participation_coefficient(G, partition):\n",
    "    # Initialize dictionary for participation coefficients\n",
    "    pc_dict = {}\n",
    "\n",
    "    # Calculate participation coefficient for each node\n",
    "    for node in G.nodes():\n",
    "        node_degree = G.degree(node)\n",
    "        if node_degree == 0:\n",
    "            pc_dict[node] = 0.0\n",
    "        else:\n",
    "            # Count within-module connections\n",
    "            within_module_degree = sum(1 for neighbor in G[node] if partition[neighbor] == partition[node])\n",
    "            # Calculate participation coefficient\n",
    "            pc_dict[node] = 1 - (within_module_degree / node_degree) ** 2\n",
    "\n",
    "    return pc_dict\n",
    "\n",
    "\n",
    "def evaluate_all(true_hr_matrices, predicted_hr_matrices, output_path='26-randomCV.csv'):\n",
    "    print(true_hr_matrices.shape)\n",
    "    print(predicted_hr_matrices.shape)\n",
    "\n",
    "    num_subjects = true_hr_matrices.shape[0]\n",
    "    results = []\n",
    "\n",
    "    for i in range(num_subjects):\n",
    "        true_matrix = true_hr_matrices[i, :, :]\n",
    "        pred_matrix = predicted_hr_matrices[i, :, :]\n",
    "\n",
    "        print(f\"Evaluating subject {i+1} with matrix shapes: true={true_matrix.shape}, pred={pred_matrix.shape}\")\n",
    "\n",
    "        if true_matrix.shape != pred_matrix.shape or true_matrix.shape[0] != true_matrix.shape[1]:\n",
    "            print(f\"Error: Matrix shape mismatch or not square for subject {i+1}: true={true_matrix.shape}, pred={pred_matrix.shape}\")\n",
    "            continue\n",
    "\n",
    "        metrics = {\n",
    "            'ID': i + 1,\n",
    "            'MAE': mean_absolute_error(true_matrix.flatten(), pred_matrix.flatten()),\n",
    "            'PCC': pearsonr(true_matrix.flatten(), pred_matrix.flatten())[0],\n",
    "            'JSD': jensenshannon(true_matrix.flatten(), pred_matrix.flatten()),\n",
    "        }\n",
    "\n",
    "        true_metrics = calculate_centralities(true_matrix)\n",
    "        pred_metrics = calculate_centralities(pred_matrix)\n",
    "\n",
    "        for key in ['NS', 'PR', 'EC', 'BC', 'PC', 'ACC']:\n",
    "            metrics[f'MAE in {key}'] = mean_absolute_error([true_metrics[key.lower()]], [pred_metrics[key.lower()]])\n",
    "\n",
    "        results.append(metrics)\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    if not df.empty:\n",
    "        # Check if the file exists to decide whether to write headers\n",
    "        file_exists = os.path.isfile(output_path)\n",
    "\n",
    "        df.to_csv(output_path, mode='a', header=not file_exists, index=False)\n",
    "        print(f\"Results appended to {output_path}.\")\n",
    "    else:\n",
    "        print(\"No data to save.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wXxwsEUS6nM5"
   },
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hDMVlTTNfOdk"
   },
   "source": [
    "## Def Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qexXcOLqhJCe"
   },
   "outputs": [],
   "source": [
    "# Custom MSLE Loss function - Mean Squared Logarithmic Error Loss\n",
    "class MSLELoss(nn.Module):\n",
    "    \"\"\"\n",
    "    The Mean Squared Logarithmic Error (MSLE) loss function\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(MSLELoss, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        \"\"\"\n",
    "        Apply the Mean Squared Logarithmic Error (MSLE) between `predicted` and `actual`.\n",
    "\n",
    "        Parameters:\n",
    "        predicted (torch.Tensor): The predicted tensor.\n",
    "        actual (torch.Tensor): The actual tensor.\n",
    "\n",
    "        Returns:\n",
    "        torch.Tensor: The MSLE loss.\n",
    "        \"\"\"\n",
    "        # To ensure numerical stability, add a small constant (epsilon) before taking the log.\n",
    "        inputs = torch.clamp(inputs, min=1e-7)\n",
    "        targets = torch.clamp(targets, min=1e-7)\n",
    "\n",
    "        # Calculate the squared logarithmic difference\n",
    "        loss = torch.square(torch.log(inputs + 1) - torch.log(targets + 1))\n",
    "\n",
    "        # Return the mean of the squared logarithmic error\n",
    "        return torch.mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rEexrlVgfNZ7"
   },
   "outputs": [],
   "source": [
    "# criterion = nn.MSELoss()\n",
    "criterion = MSLELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rbY1rTUMfXZ5"
   },
   "outputs": [],
   "source": [
    "def train(model, optimizer, subjects_adj, subjects_labels, device, args, val_adj, val_ground_truth, total_epoch):\n",
    "    \"\"\"\n",
    "    Trains the model\n",
    "\n",
    "    Parameters:\n",
    "    - model(nn.Module): the model\n",
    "    - optimizer(torch.optim): the optimizer\n",
    "    - subjects_adj(np.array): the training data (adjacency matrix)\n",
    "    - subjects_labels(np.array): the training data (labels)\n",
    "    - device(torch.device): the device, either 'cpu' or 'cuda'\n",
    "    - args: the arguments\n",
    "\n",
    "    Returns:\n",
    "    - all_epochs_loss(list): the loss\n",
    "    \"\"\"\n",
    "    all_epochs_loss = []\n",
    "    val_epochs_loss = []\n",
    "    no_epochs = args.epochs\n",
    "    model.to(device)\n",
    "\n",
    "    # Early stopping parameters\n",
    "    early_stopping_patience = 10\n",
    "    early_stopping_min_delta = 0.0001\n",
    "    min_training_epochs = 100\n",
    "\n",
    "    # Early stopping variables\n",
    "    best_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    early_stop = False\n",
    "    epoch = 0\n",
    "\n",
    "    while epoch < min_training_epochs or not early_stop:\n",
    "    # while epoch < total_epoch:\n",
    "        # Initialize lists to store loss and error for each epoch\n",
    "        epoch_loss = []\n",
    "        epoch_error = []\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        for lr, hr in zip(subjects_adj, subjects_labels):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            lr = torch.from_numpy(lr).type(torch.FloatTensor).to(device)\n",
    "            hr = torch.from_numpy(hr).type(torch.FloatTensor).to(device)\n",
    "\n",
    "            model_outputs, net_outs, start_gcn_outs, layer_outs = model(lr)\n",
    "            model_outputs = unpad(model_outputs, args.padding)\n",
    "\n",
    "            padded_hr = pad_HR_adj(hr.cpu(), args.padding)\n",
    "            _, U_hr = torch.linalg.eigh(padded_hr, UPLO='U')\n",
    "            U_hr = U_hr.to(device)\n",
    "\n",
    "            loss = args.lmbda * criterion(net_outs, start_gcn_outs) + criterion(model.layer.weights, U_hr) + criterion(\n",
    "                model_outputs, hr)\n",
    "\n",
    "            error = criterion(model_outputs, hr)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss.append(loss.item())\n",
    "            epoch_error.append(error.item())\n",
    "\n",
    "        mean_epoch_loss = np.mean(epoch_loss)\n",
    "        all_epochs_loss.append(np.mean(epoch_loss))\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = []\n",
    "            for lr, hr in zip(val_adj, val_ground_truth):\n",
    "                lr = torch.from_numpy(lr).type(torch.FloatTensor).to(device)\n",
    "                hr = torch.from_numpy(hr).type(torch.FloatTensor).to(device)\n",
    "\n",
    "                model_outputs, net_outs, start_gcn_outs, layer_outs = model(lr)\n",
    "                model_outputs = unpad(model_outputs, args.padding)\n",
    "\n",
    "                padded_hr = pad_HR_adj(hr.cpu(), args.padding)\n",
    "                _, U_hr = torch.linalg.eigh(padded_hr, UPLO='U')\n",
    "                U_hr = U_hr.to(device)\n",
    "\n",
    "                loss = args.lmbda * criterion(net_outs, start_gcn_outs) + criterion(model.layer.weights, U_hr) + criterion(\n",
    "                    model_outputs, hr)\n",
    "\n",
    "                val_loss.append(loss.item())\n",
    "\n",
    "            mean_val_loss = np.mean(val_loss)\n",
    "            val_epochs_loss.append(mean_val_loss)\n",
    "\n",
    "        if mean_val_loss < best_loss - early_stopping_min_delta:\n",
    "            best_loss = mean_val_loss\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        if epochs_no_improve >= early_stopping_patience and epoch >= min_training_epochs:\n",
    "            early_stop = True\n",
    "\n",
    "        epoch += 1\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{no_epochs}, Training Loss: {np.mean(epoch_loss):.4f}, Error: {np.mean(epoch_error):.4f}, Val Loss: {mean_val_loss}')\n",
    "\n",
    "    return all_epochs_loss, val_epochs_loss\n",
    "    # return all_epochs_loss\n",
    "\n",
    "\n",
    "\n",
    "def test(model, test_adj, test_labels, device, args):\n",
    "    \"\"\"\n",
    "    Tests the model\n",
    "\n",
    "    Parameters:\n",
    "    - model(nn.Module): the model\n",
    "    - test_adj(np.array): the testing data (adjacency matrix)\n",
    "    - test_labels(np.array): the testing data (labels)\n",
    "    - device(torch.device): the device, either 'cpu' or 'cuda'\n",
    "    - args: the arguments\n",
    "\n",
    "    Returns:\n",
    "    - mae(float): the MAE\n",
    "    - pcc(float): the PCC\n",
    "    - js_dis(float): the JSD\n",
    "    - avg_mae_pc(float): the average MAE PC\n",
    "    - avg_mae_ec(float): the average MAE EC\n",
    "    - avg_mae_bc(float): the average MAE BC\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    preds_list = []\n",
    "    g_t = []\n",
    "\n",
    "    i = 0\n",
    "    # TESTING\n",
    "    print(\"-------- Validation --------\")\n",
    "    for lr, hr in zip(test_adj, test_labels):\n",
    "\n",
    "        all_zeros_lr = not np.any(lr)\n",
    "        all_zeros_hr = not np.any(hr)\n",
    "\n",
    "        if all_zeros_lr == False and all_zeros_hr == False:\n",
    "            lr = torch.from_numpy(lr).type(torch.FloatTensor).to(device)\n",
    "            np.fill_diagonal(hr, 1)\n",
    "            hr = torch.from_numpy(hr).type(torch.FloatTensor).to(device)\n",
    "            preds, a, b, c = model(lr)\n",
    "            preds = unpad(preds, args.padding)\n",
    "\n",
    "            preds_list.append(preds.cpu().detach().numpy())\n",
    "\n",
    "            g_t.append(hr.cpu().detach().numpy())\n",
    "            i += 1\n",
    "    pred_mat = np.array(preds_list)\n",
    "    gt_mat = np.array(g_t)\n",
    "    mae, pcc, js_dis, avg_mae_pc, avg_mae_ec, avg_mae_bc = evaluate(pred_mat, gt_mat)\n",
    "    return pred_mat, mae, pcc, js_dis, avg_mae_pc, avg_mae_ec, avg_mae_bc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GIWS4GsdejNn"
   },
   "source": [
    "## hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3q2dQcmo6mgr"
   },
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='GSR-Net')\n",
    "parser.add_argument('--epochs', type=int, default=500, metavar='no_epochs',\n",
    "                    help='number of episode to train ')\n",
    "parser.add_argument('--lr', type=float, default=0.0001, metavar='lr',\n",
    "                    help='learning rate (default: 0.0001 using Adam Optimizer)')\n",
    "parser.add_argument('--splits', type=int, default=3, metavar='n_splits',\n",
    "                    help='no of cross validation folds')\n",
    "parser.add_argument('--lmbda', type=int, default=16, metavar='L',\n",
    "                    help='self-reconstruction error hyperparameter')\n",
    "parser.add_argument('--lr_dim', type=int, default=160, metavar='N',\n",
    "                    help='adjacency matrix input dimensions')\n",
    "parser.add_argument('--hr_dim', type=int, default=320, metavar='N',\n",
    "                    help='super-resolved adjacency matrix output dimensions')\n",
    "parser.add_argument('--hidden_dim', type=int, default=320, metavar='N',\n",
    "                    help='hidden GraphConvolutional layer dimensions')\n",
    "parser.add_argument('--padding', type=int, default=26, metavar='padding',\n",
    "                    help='dimensions of padding')\n",
    "parser.add_argument('--num_sampled_nodes', type=int, default=100, metavar='num_sampled_nodes',\n",
    "                        help='number of sampled nodes')\n",
    "args, unknown = parser.parse_known_args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LA8S69TReljn"
   },
   "source": [
    "## Load Data and Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24026,
     "status": "ok",
     "timestamp": 1716530958255,
     "user": {
      "displayName": "yiying guan",
      "userId": "06690427779299886783"
     },
     "user_tz": -60
    },
    "id": "FbK6LWGw6465",
    "outputId": "8fd962b2-260d-442f-ed8a-2f450f56215f"
   },
   "outputs": [],
   "source": [
    "subjects_adj_split_1, subjects_gt_split_1, subjects_adj_split_2, subjects_gt_split_2, subjects_adj_split_3, subjects_gt_split_3, test_adj, _ = data()\n",
    "\n",
    "def data_es(data_lr_reshaped, data_hr_reshaped):\n",
    "    # use the last 20 samples for early stopping\n",
    "    num_samples = data_lr_reshaped.shape[0]\n",
    "    train_size = int(num_samples - 20)\n",
    "\n",
    "    subjects_adj = data_lr_reshaped[:train_size]\n",
    "    subjects_labels = data_hr_reshaped[:train_size]\n",
    "\n",
    "    test_adj = data_lr_reshaped[train_size:]\n",
    "    test_labels = data_hr_reshaped[train_size:]\n",
    "\n",
    "    return subjects_adj, subjects_labels, test_adj, test_labels\n",
    "\n",
    "train_subjects_adj_split_1, train_subjects_labels_split_1, test_adj_split_1, test_labels_split_1 = data_es(\n",
    "    subjects_adj_split_1, subjects_gt_split_1\n",
    ")\n",
    "train_subjects_adj_split_2, train_subjects_labels_split_2, test_adj_split_2, test_labels_split_2 = data_es(\n",
    "    subjects_adj_split_2, subjects_gt_split_2\n",
    ")\n",
    "train_subjects_adj_split_3, train_subjects_labels_split_3, test_adj_split_3, test_labels_split_3 = data_es(\n",
    "    subjects_adj_split_3, subjects_gt_split_3\n",
    ")\n",
    "\n",
    "\n",
    "# Perform 3-fold validation\n",
    "splits = [\n",
    "    (0, 1, 2),\n",
    "    (0, 2, 1),\n",
    "    (1, 2, 0)\n",
    "]\n",
    "\n",
    "print(\"Torch:\")\n",
    "\n",
    "ks = [0.9, 0.7, 0.6, 0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1716530958256,
     "user": {
      "displayName": "yiying guan",
      "userId": "06690427779299886783"
     },
     "user_tz": -60
    },
    "id": "EM1XdFXn7GLv",
    "outputId": "efcd6dc8-2143-4f2e-ccee-cb533df9342f"
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "# to cuda\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print('Device: ', device)\n",
    "\n",
    "if not os.path.exists('./Result'):\n",
    "    os.mkdir('./Result/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11959330,
     "status": "ok",
     "timestamp": 1716542917576,
     "user": {
      "displayName": "yiying guan",
      "userId": "06690427779299886783"
     },
     "user_tz": -60
    },
    "id": "4ZDd3wj07NRP",
    "outputId": "5dd351fe-10b0-400f-8796-168769e0aed6"
   },
   "outputs": [],
   "source": [
    "STANDARD_MEASURE_NAMES = ['MAE', 'PCC', 'JSD']\n",
    "MAE_MEASURE_NAMES = ['MAE (PC)', 'MAE (EC)', 'MAE (BC)']\n",
    "all_train_loss = []\n",
    "val_mae = []\n",
    "val_pcc = []\n",
    "val_js = []\n",
    "val_avg_pc = []\n",
    "val_avg_ec = []\n",
    "val_avg_bc = []\n",
    "for train_indices in splits:\n",
    "    model = GSRNet(ks, args)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "    total_epoch = 0\n",
    "\n",
    "    print('======== Fold ', i, '========')\n",
    "    if i == 0:\n",
    "        total_epoch = 130\n",
    "        subjects_adj, eval_adj, subjects_gt, eval_gt, val_adj, val_gt= (\n",
    "\n",
    "            np.concatenate((subjects_adj_split_1, subjects_adj_split_2), axis=0),\n",
    "            subjects_adj_split_3,\n",
    "            np.concatenate((subjects_gt_split_1, subjects_gt_split_2), axis=0),\n",
    "            subjects_gt_split_3,\n",
    "            np.concatenate((test_adj_split_1, test_adj_split_2), axis=0),\n",
    "            np.concatenate((test_labels_split_1, test_labels_split_2), axis=0),\n",
    "        )\n",
    "    elif i == 1:\n",
    "        total_epoch = 126\n",
    "        subjects_adj, eval_adj, subjects_gt, eval_gt, val_adj, val_gt= (\n",
    "\n",
    "            np.concatenate((subjects_adj_split_1, subjects_adj_split_3), axis=0),\n",
    "            subjects_adj_split_2,\n",
    "            np.concatenate((subjects_gt_split_1, subjects_gt_split_3), axis=0),\n",
    "            subjects_gt_split_2,\n",
    "            np.concatenate((test_adj_split_1, test_adj_split_3), axis=0),\n",
    "            np.concatenate((test_labels_split_1, test_labels_split_3), axis=0),\n",
    "        )\n",
    "    elif i == 2:\n",
    "        total_epoch = 127\n",
    "        subjects_adj, eval_adj, subjects_gt, eval_gt, val_adj, val_gt= (\n",
    "\n",
    "            np.concatenate((subjects_adj_split_2, subjects_adj_split_3), axis=0),\n",
    "            subjects_adj_split_1,\n",
    "            np.concatenate((subjects_gt_split_2, subjects_gt_split_3), axis=0),\n",
    "            subjects_gt_split_1,\n",
    "            np.concatenate((test_adj_split_2, test_adj_split_3), axis=0),\n",
    "            np.concatenate((test_labels_split_2, test_labels_split_3), axis=0),\n",
    "        )\n",
    "\n",
    "    epoch_loss = train(model, optimizer, subjects_adj, subjects_gt, device, args)\n",
    "    epoch_loss, val_epochs_loss = train(model, optimizer, subjects_adj, subjects_gt, device, args, val_adj, val_gt, total_epoch=total_epoch)\n",
    "\n",
    "    all_train_loss.append(epoch_loss)\n",
    "\n",
    "    epoch_loss_df = pd.DataFrame({'epoch': list(range(1, len(epoch_loss) + 1)), 'loss': epoch_loss})\n",
    "    epoch_loss_df.to_csv(f'26_fold_{i}_loss_curve.csv', index=False)\n",
    "\n",
    "    val_epoch_loss_df = pd.DataFrame({'epoch': list(range(1, len(val_epochs_loss) + 1)), 'loss': val_epochs_loss})\n",
    "    val_epoch_loss_df.to_csv(f'26_fold_{i}_val_loss_curve.csv', index=False)\n",
    "\n",
    "    # save the fold model\n",
    "    torch.save(model, f'./Result/model_fold_{i}.pth')\n",
    "\n",
    "    preds, mae, pcc, js_dis, avg_mae_pc, avg_mae_ec, avg_mae_bc = test(model, eval_adj, eval_gt, device, args)\n",
    "    val_mae.append(mae)\n",
    "    val_pcc.append(pcc)\n",
    "    val_js.append(js_dis)\n",
    "    val_avg_pc.append(avg_mae_pc)\n",
    "    val_avg_ec.append(avg_mae_ec)\n",
    "    val_avg_bc.append(avg_mae_bc)\n",
    "\n",
    "    evaluate_all(eval_gt, preds, output_path=f'26_fold_{i}_evaluation.csv')\n",
    "\n",
    "    plot_fold_eval_measures([mae, pcc, js_dis], i+1, STANDARD_MEASURE_NAMES, 'standard')\n",
    "    plot_fold_eval_measures([avg_mae_pc, avg_mae_ec, avg_mae_bc], i+1, MAE_MEASURE_NAMES, 'mae')\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GbxRxkZme9S7"
   },
   "source": [
    "## Save and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mc_ff0x67RTj"
   },
   "outputs": [],
   "source": [
    "# save the loss and mae\n",
    "all_train_loss = np.array(all_train_loss)\n",
    "val_mae = np.array(val_mae)\n",
    "val_pcc = np.array(val_pcc)\n",
    "val_js = np.array(val_js)\n",
    "val_avg_pc = np.array(val_avg_pc)\n",
    "val_avg_bc = np.array(val_avg_bc)\n",
    "val_avg_ec = np.array(val_avg_ec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oCqI1wbS7SsZ"
   },
   "outputs": [],
   "source": [
    "# plot the training loss\n",
    "plot_loss(all_train_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GgxYUubM7aPF"
   },
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hrPg9ktDfaeU"
   },
   "outputs": [],
   "source": [
    "def predict(model, test_adj, device, args):\n",
    "    \"\"\"\n",
    "    Predicts the data\n",
    "\n",
    "    Parameters:\n",
    "    - model(nn.Module): the model\n",
    "    - test_adj(np.array): the testing data (adjacency matrix)\n",
    "    - device(torch.device): the device, either 'cpu' or 'cuda'\n",
    "    - args: the arguments\n",
    "\n",
    "    Returns:\n",
    "    - preds_list(list): the predictions\n",
    "    \"\"\"\n",
    "    preds_list = []\n",
    "    i = 0\n",
    "    for lr in test_adj:\n",
    "        lr = torch.from_numpy(lr).type(torch.FloatTensor).to(device)\n",
    "        preds, a, b, c = model(lr)\n",
    "        preds = unpad(preds, args.padding)\n",
    "        preds_list.append(preds.cpu().detach().numpy())\n",
    "        i += 1\n",
    "    return preds_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YrJgV_Ln7b9H"
   },
   "outputs": [],
   "source": [
    "# predict\n",
    "vectorizer = MatrixVectorizer()\n",
    "for n in range(args.splits):\n",
    "    model = torch.load(f'./Result/model_fold_{n}.pth')\n",
    "    preds_list = predict(model, test_adj, device, args)\n",
    "    for i in range(len(preds_list)):\n",
    "        # vectorize\n",
    "        preds_list[i] = vectorizer.vectorize(preds_list[i])\n",
    "    # save the predictions\n",
    "    preds_arr = np.array(preds_list)\n",
    "    preds = preds_arr.flatten()\n",
    "\n",
    "    with open(f'Result/predictions_fold_{n}.csv', 'w') as f:\n",
    "        f.write(\"Id,Predicted\\n\")\n",
    "        for i, pred in enumerate(preds):\n",
    "            f.write(f\"{i + 1},{pred}\\n\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
