{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-03-13T01:13:10.929695Z",
     "iopub.status.busy": "2024-03-13T01:13:10.929307Z",
     "iopub.status.idle": "2024-03-13T01:13:24.579312Z",
     "shell.execute_reply": "2024-03-13T01:13:24.577955Z",
     "shell.execute_reply.started": "2024-03-13T01:13:10.929664Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import time\n",
    "from evaluation import evaluate_all\n",
    "from evaluation_functions import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = KFold(cluster=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set random seed and device as per the spec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-13T01:13:24.591764Z",
     "iopub.status.busy": "2024-03-13T01:13:24.591420Z",
     "iopub.status.idle": "2024-03-13T01:13:24.606528Z",
     "shell.execute_reply": "2024-03-13T01:13:24.605277Z",
     "shell.execute_reply.started": "2024-03-13T01:13:24.591736Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "random_seed = 42\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "# Check for CUDA (GPU support) and set device accordingly\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"CUDA is available. Using GPU.\")\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed)  # For multi-GPU setups\n",
    "    # Additional settings for ensuring reproducibility on CUDA\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDA not available. Using CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matrix Vecotrizer class is taken from https://github.com/basiralab/DGL/blob/main/Project/MatrixVectorizer.py to aid in vectorizing and anti-vectorizing our dataset. We develop functions later in the cell to do the same using an object of the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-13T01:13:24.623455Z",
     "iopub.status.busy": "2024-03-13T01:13:24.622942Z",
     "iopub.status.idle": "2024-03-13T01:13:24.635397Z",
     "shell.execute_reply": "2024-03-13T01:13:24.634169Z",
     "shell.execute_reply.started": "2024-03-13T01:13:24.623417Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class MatrixVectorizer:\n",
    "    def _init_(self):\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def vectorize(matrix, include_diagonal=False):\n",
    "        # Determine the size of the matrix based on its first dimension\n",
    "        matrix_size = matrix.shape[0]\n",
    "        # Initialize an empty list to accumulate vector elements\n",
    "        vector_elements = []\n",
    "\n",
    "        # Iterate over columns and then rows to collect the relevant elements\n",
    "        for col in range(matrix_size):\n",
    "            for row in range(matrix_size):\n",
    "                # Skip diagonal elements if not including them\n",
    "                if row != col:  \n",
    "                    if row < col:\n",
    "                        # Collect upper triangle elements\n",
    "                        vector_elements.append(matrix[row, col])\n",
    "                    elif include_diagonal and row == col + 1:\n",
    "                        # Optionally include the diagonal elements immediately below the diagonal\n",
    "                        vector_elements.append(matrix[row, col])\n",
    "\n",
    "        return np.array(vector_elements)\n",
    "\n",
    "    @staticmethod\n",
    "    def anti_vectorize(vector, matrix_size, include_diagonal=False):\n",
    "        matrix = np.zeros((matrix_size, matrix_size))\n",
    "        vector_idx = 0\n",
    "\n",
    "        for col in range(matrix_size):\n",
    "            for row in range(matrix_size):\n",
    "                # Skip diagonal elements if not including them\n",
    "                if row != col:  \n",
    "                    if row < col:\n",
    "                        # Reflect vector elements into the upper triangle and its mirror in the lower triangle\n",
    "                        matrix[row, col] = vector[vector_idx]\n",
    "                        matrix[col, row] = vector[vector_idx]\n",
    "                        vector_idx += 1\n",
    "                    elif include_diagonal and row == col + 1:\n",
    "                        # Optionally fill the diagonal elements after completing each column\n",
    "                        matrix[row, col] = vector[vector_idx]\n",
    "                        matrix[col, row] = vector[vector_idx]\n",
    "                        vector_idx += 1\n",
    "\n",
    "        return matrix\n",
    "    \n",
    "\n",
    "mv = MatrixVectorizer()\n",
    "\n",
    "def anti_vectorize_and_reshape(row, matrix_size):\n",
    "    return mv.anti_vectorize(row, matrix_size)\n",
    "\n",
    "def vectorise_and_reshape(matrix):\n",
    "    return mv.vectorize(matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we preprocess our data to antivectorize and convert it to a matrix form. We also clip and fillna as seen below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-13T01:13:24.680147Z",
     "iopub.status.busy": "2024-03-13T01:13:24.679795Z",
     "iopub.status.idle": "2024-03-13T01:17:58.867694Z",
     "shell.execute_reply": "2024-03-13T01:17:58.866353Z",
     "shell.execute_reply.started": "2024-03-13T01:13:24.680109Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data.preprocessing()\n",
    "print(\"Processing complete for train and test datasets.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea behind symmetric normalization (using $D^{−1/2}AD^{−1/2}$, where D is the degree matrix and A is the adjacency matrix) is to weight the contribution of each node's features by the inverse square root of its degree, promoting balance in the influence of nodes regardless of their connectivity.\n",
    "\n",
    "For a weighted adjacency matrix such as the one we have after antivectorising above, symmetric normalization applied below can be particularly useful when the scale of weights varies widely. This approach helps to mitigate the impact of very large or very small weights by normalizing the contributions of each edge in accordance with both the source and target nodes' weight. In essence, this normalization is useful because:\n",
    "\n",
    "- It makes the scale of feature values and edge weights more uniform, which can be beneficial for learning algorithms that are sensitive to the scale of input data.\n",
    "- The normalized weights can sometimes offer a more interpretable basis for understanding the relative importance of edges in the context of the entire graph structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-13T01:17:58.879958Z",
     "iopub.status.busy": "2024-03-13T01:17:58.879504Z",
     "iopub.status.idle": "2024-03-13T01:17:59.143381Z",
     "shell.execute_reply": "2024-03-13T01:17:59.142118Z",
     "shell.execute_reply.started": "2024-03-13T01:17:58.879916Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def normalize_adjacency_matrix(A):\n",
    "    \"\"\"\n",
    "    Normalize the adjacency matrix with symmetric normalization.\n",
    "\n",
    "    Args:\n",
    "    A (numpy.ndarray): The adjacency matrix to normalize.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: The normalized adjacency matrix.\n",
    "    \"\"\"\n",
    "    # Add self-connections\n",
    "    I = np.eye(A.shape[0])\n",
    "    A_with_self_loops = A + I\n",
    "    \n",
    "    # Compute the degree matrix\n",
    "    D_with_self_loops = np.array(np.sum(A_with_self_loops, axis=1)).reshape(-1)\n",
    "    D_with_self_loops_inv_sqrt = np.power(D_with_self_loops, -0.5).flatten()\n",
    "    D_with_self_loops_inv_sqrt[np.isinf(D_with_self_loops_inv_sqrt)] = 0.\n",
    "    D_with_self_loops_inv_sqrt_matrix = np.diag(D_with_self_loops_inv_sqrt)\n",
    "    \n",
    "    # Compute the normalized adjacency matrix\n",
    "    A_normalized = D_with_self_loops_inv_sqrt_matrix.dot(A_with_self_loops).dot(D_with_self_loops_inv_sqrt_matrix)\n",
    "    \n",
    "    return A_normalized\n",
    "\n",
    "for i in range(3):\n",
    "    data.lr[i] = data.lr[i].apply(lambda A: np.round(normalize_adjacency_matrix(A),4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some helper functions for our GSR, and the Graph U-Net class which is quite important for our GNN. This code is applied from https://github.com/basiralab/GSR-Net/tree/master. The Graph U-Net forward pass is described as follows:\n",
    "\n",
    "- The input features X are first processed by an initial graph convolution layer. For A the adjacency matrix, X the input feature matrix, W the weight matrix, b​ bias term, and LeakyReLU the activation function, this can be represented by the equation:\n",
    "$$X'= LeakyReLU(AXW​+b​)$$\n",
    "- For each level i in self.ks, the features are further processed by a graph convolution layer, followed by a graph pooling operation. The pooling operation selects a subset of nodes based on top-k based on feature scores, reducing the graph size at each level.\n",
    "- After downsampling to the smallest graph, the features are processed by self.bottom_gcn\n",
    "- The process of UpSampling reverses the downsampling steps. Using stored indices from the pooling steps, graph unpooling operations reintegrate nodes back into the graph, followed by graph convolution layers to update the node features. The features from the corresponding downsampling level are added to the upsampled features to ensure the model captures both local and global graph structures.\n",
    "- The upsampled features are concatenated with the original features processed by the initial graph convolution layer, and then passed through a final graph convolution layer to produce the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-13T01:17:59.157488Z",
     "iopub.status.busy": "2024-03-13T01:17:59.156668Z",
     "iopub.status.idle": "2024-03-13T01:17:59.188743Z",
     "shell.execute_reply": "2024-03-13T01:17:59.187435Z",
     "shell.execute_reply.started": "2024-03-13T01:17:59.157444Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def pad_HR_adj(label, split):\n",
    "\n",
    "    label=np.pad(label,((split,split),(split,split)),mode=\"constant\")\n",
    "    np.fill_diagonal(label,1)\n",
    "    return torch.from_numpy(label).type(torch.FloatTensor)\n",
    "\n",
    "def unpad(data, split):\n",
    "  \n",
    "    idx_0 = data.shape[0]-split\n",
    "    idx_1 = data.shape[1]-split\n",
    "    train = data[split:idx_0, split:idx_1]\n",
    "    return train\n",
    "\n",
    "class GraphUnpool(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(GraphUnpool, self).__init__()\n",
    "\n",
    "    def forward(self, A, X, idx):\n",
    "        new_X = torch.zeros([A.shape[0], X.shape[1]])\n",
    "        new_X[idx] = X\n",
    "        return A, new_X\n",
    "\n",
    "    \n",
    "class GraphPool(nn.Module):\n",
    "\n",
    "    def __init__(self, k, in_dim):\n",
    "        super(GraphPool, self).__init__()\n",
    "        self.k = k\n",
    "        self.proj = nn.Linear(in_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, A, X):\n",
    "        scores = self.proj(X)\n",
    "        # scores = torch.abs(scores)\n",
    "        scores = torch.squeeze(scores)\n",
    "        scores = self.sigmoid(scores/100)\n",
    "        num_nodes = A.shape[0]\n",
    "        values, idx = torch.topk(scores, int(self.k*num_nodes))\n",
    "        new_X = X[idx, :]\n",
    "        values = torch.unsqueeze(values, -1)\n",
    "        new_X = torch.mul(new_X, values)\n",
    "        A = A[idx, :]\n",
    "        A = A[:, idx]\n",
    "        return A, new_X, idx\n",
    "\n",
    "\n",
    "class GCN(nn.Module):\n",
    "\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(GCN, self).__init__()\n",
    "        self.proj = nn.Linear(in_dim, out_dim)\n",
    "        self.drop = nn.Dropout(p=0.2)\n",
    "\n",
    "    def forward(self, A, X):\n",
    "        X = self.drop(X)\n",
    "        X = self.proj(X)\n",
    "        return X\n",
    "\n",
    "class GraphUnet(nn.Module):\n",
    "\n",
    "    def __init__(self, ks, in_dim, out_dim, dim=268):\n",
    "        super(GraphUnet, self).__init__()\n",
    "        self.ks = ks\n",
    "       \n",
    "        self.start_gcn = GCN(in_dim, dim)\n",
    "        self.bottom_gcn = GCN(dim, dim)\n",
    "        self.end_gcn = GCN(2*dim, out_dim)\n",
    "        self.down_gcns = []\n",
    "        self.up_gcns = []\n",
    "        self.pools = []\n",
    "        self.unpools = []\n",
    "        self.l_n = len(ks)\n",
    "        for i in range(self.l_n):\n",
    "            self.down_gcns.append(GCN(dim, dim))\n",
    "            self.up_gcns.append(GCN(dim, dim))\n",
    "            self.pools.append(GraphPool(ks[i], dim))\n",
    "            self.unpools.append(GraphUnpool())\n",
    "\n",
    "    def forward(self, A, X):\n",
    "        adj_ms = []\n",
    "        indices_list = []\n",
    "        down_outs = []\n",
    "        X = self.start_gcn(A, X)\n",
    "        start_gcn_outs = X\n",
    "        org_X = X\n",
    "        for i in range(self.l_n):\n",
    "           \n",
    "            X = self.down_gcns[i](A, X)\n",
    "            adj_ms.append(A)\n",
    "            down_outs.append(X)\n",
    "            A, X, idx = self.pools[i](A, X)\n",
    "            indices_list.append(idx)\n",
    "        X = self.bottom_gcn(A, X)\n",
    "        for i in range(self.l_n):\n",
    "            up_idx = self.l_n - i - 1\n",
    "           \n",
    "            A, idx = adj_ms[up_idx], indices_list[up_idx]\n",
    "            A, X = self.unpools[i](A, X, idx)\n",
    "            X = self.up_gcns[i](A, X)\n",
    "            X = X.add(down_outs[up_idx])\n",
    "        X = torch.cat([X, org_X], 1)\n",
    "        X = self.end_gcn(A, X)\n",
    "        \n",
    "        return X, start_gcn_outs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are some more of the helper functions for our model class. Specifically, the interesting class is NodeAttention. The NodeAttention class defines a simple node-level attention mechanism for graph neural networks. This mechanism allows the model to weigh nodes differently based on their features, enabling it to focus more on important nodes during processing. The class is built with PyTorch and comprises two main parts: an attention scoring mechanism and the application of these scores to modulate the node features. The forward pass computes and applies attention as follows:\n",
    "\n",
    "- It computes raw attention scores for each node by passing the node features through the linear layer. These raw scores are then normalized across nodes using the softmax function, ensuring that the scores sum up to 1 across the nodes for a given feature. This step can be represented by the equation, where W and b are the weights and bias of the linear layer self.attn, and x is the input feature matrix.:\n",
    "$$attn_scores=softmax(Wx+b)$$\n",
    "- The normalized attention scores are then element-wise multiplied with the original node features to yield the attended node features. This step modulates the importance of each node's features based on the learned attention scores.\n",
    "\n",
    "The resulting attended feature matrix retains the original shape where each node's features are now scaled by its importance as determined by the model. This class effectively enables the network to focus on more relevant nodes by adjusting their influence on the model's output, which can be crucial for high resolution graph generation where not all nodes contribute equally to the final task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-13T01:17:59.191932Z",
     "iopub.status.busy": "2024-03-13T01:17:59.191021Z",
     "iopub.status.idle": "2024-03-13T01:17:59.217391Z",
     "shell.execute_reply": "2024-03-13T01:17:59.216186Z",
     "shell.execute_reply.started": "2024-03-13T01:17:59.191881Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def normalize_adj_torch(mx):\n",
    "    rowsum = mx.sum(1)\n",
    "    r_inv_sqrt = torch.pow(rowsum, -0.5).flatten()\n",
    "    r_inv_sqrt[torch.isinf(r_inv_sqrt)] = 0.\n",
    "    r_mat_inv_sqrt = torch.diag(r_inv_sqrt)\n",
    "    mx = torch.matmul(mx, r_mat_inv_sqrt)\n",
    "    mx = torch.transpose(mx, 0, 1)\n",
    "    mx = torch.matmul(mx, r_mat_inv_sqrt)\n",
    "    return mx\n",
    "\n",
    "def weight_variable_glorot(output_dim):\n",
    "\n",
    "    input_dim = output_dim\n",
    "    init_range = np.sqrt(6.0 / (input_dim + output_dim))\n",
    "    initial = np.random.uniform(-init_range, init_range,(input_dim, output_dim))\n",
    "    \n",
    "    return initial\n",
    "\n",
    "class GSRLayer(nn.Module):\n",
    "  \n",
    "    def __init__(self,hr_dim):\n",
    "        super(GSRLayer, self).__init__()\n",
    "\n",
    "        self.weights = torch.from_numpy(weight_variable_glorot(hr_dim)).type(torch.FloatTensor)\n",
    "        self.weights = torch.nn.Parameter(data=self.weights, requires_grad = True)\n",
    "    def forward(self,A,X):\n",
    "        lr = A\n",
    "        lr_dim = lr.shape[0]\n",
    "        f = X\n",
    "        eig_val_lr, U_lr = torch.linalg.eigh(A, UPLO='U')\n",
    "        eye_mat = torch.eye(lr_dim).type(torch.FloatTensor)\n",
    "        s_d = torch.cat((eye_mat,eye_mat),0)\n",
    "        s_d_adj = s_d[:268, :]\n",
    "        a = torch.matmul(self.weights,s_d_adj )\n",
    "        b = torch.matmul(a ,torch.t(U_lr))\n",
    "        f_d = torch.matmul(b ,f)\n",
    "        f_d = torch.abs(f_d)\n",
    "        self.f_d = f_d.fill_diagonal_(1)\n",
    "        adj = normalize_adj_torch(self.f_d)\n",
    "        X = torch.mm(adj, adj.t())\n",
    "        X = (X + X.t())/2\n",
    "        idx = torch.eye(268, dtype=bool)\n",
    "        X[idx]=1\n",
    "        return adj, torch.abs(X)\n",
    "    \n",
    "\n",
    "\n",
    "class GraphConvolution(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features, dropout=0.2, act=F.leaky_relu):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.dropout = dropout\n",
    "        self.act = act\n",
    "        self.weight = torch.nn.Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        torch.nn.init.xavier_uniform_(self.weight)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        input = F.dropout(input, self.dropout, self.training)\n",
    "        support = torch.mm(input, self.weight)\n",
    "        output = torch.mm(adj, support)\n",
    "        output = self.act(output)\n",
    "        return output\n",
    "\n",
    "class NodeAttention(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super(NodeAttention, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.attn = nn.Linear(in_features, 1)\n",
    "    def forward(self, x):\n",
    "        # Compute attention scores\n",
    "        attn_scores = F.softmax(self.attn(x), dim=1)\n",
    "        # Apply attention scores\n",
    "        attended_x = x * attn_scores\n",
    "        return attended_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we have our model class, GSRapid. The name is relevant because out of all our model implementations this class trained the fastest due to its inherent simplicity. Our trials in AGSR and increasingly complex forward functions did not yield ideal results showing the relevance of the concept of Occam's Razor https://simple.wikipedia.org/wiki/Occam%27s_razor. Through synergising the existing GSR-Net model with NodeAttention, DropEdge, and relevant data augmentation in Symmetric Normalisation-- we were able to significantly improve the baseline performance. In this class, we introduce DropEdge:\n",
    "\n",
    "DropEdge randomly removes a percentage of edges from the weight matrix of the graph in each training iteration. This operation is equivalent to applying dropout to the edges of the graph, which can be seen as a form of regularization. The goal is to prevent the model from over-relying on specific graph structures and to encourage the learning of more robust features that generalize well to unseen data. \n",
    "\n",
    "DropEdge is applied as follows in the class:\n",
    "\n",
    "- Dropout is applied to the normalized weight matrix (A) using F.dropout(A, self.p_drop, training=self.training), where self.p_drop specifies the dropout rate. This operation randomly sets a fraction of elements in A to zero, effectively removing the edge weights and hence edges from the graph.\n",
    "\n",
    "In the forward pass of GSRapid, we use the DropEdge-modified adjacency matrix to compute node representations at different scales. The process involves a Graph U-Net for hierarchical feature learning, followed by graph convolutions and NodeAttention to refine the node features. The final output is a symmetric matrix z, which has been regularized by DropEdge, promoting model generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-13T01:17:59.220893Z",
     "iopub.status.busy": "2024-03-13T01:17:59.219683Z",
     "iopub.status.idle": "2024-03-13T01:17:59.237582Z",
     "shell.execute_reply": "2024-03-13T01:17:59.236189Z",
     "shell.execute_reply.started": "2024-03-13T01:17:59.220845Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class GSRapid(nn.Module):\n",
    "\n",
    "    def __init__(self,ks,lr_dim=160,hr_dim=268,hidden_dim=268, p_drop=0.25):\n",
    "        super(GSRapid, self).__init__()\n",
    "        self.p_drop = p_drop\n",
    "        self.lr_dim = lr_dim\n",
    "        self.hr_dim = hr_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layer = GSRLayer(self.hr_dim)\n",
    "        self.net = GraphUnet(ks, self.lr_dim, self.hr_dim)\n",
    "        self.gc1 = GraphConvolution(self.hr_dim, self.hidden_dim, 0, act=F.leaky_relu)\n",
    "        self.gc2 = GraphConvolution(self.hidden_dim, self.hr_dim, 0, act=F.leaky_relu)\n",
    "        self.attn1 = NodeAttention(self.hidden_dim)\n",
    "        self.attn2 = NodeAttention(self.hr_dim)\n",
    "\n",
    "    def forward(self,lr):\n",
    "\n",
    "        I = torch.eye(self.lr_dim).type(torch.FloatTensor)\n",
    "        A = normalize_adj_torch(lr).type(torch.FloatTensor)\n",
    "        A = F.dropout(A, self.p_drop, training=self.training)\n",
    "        self.net_outs, self.start_gcn_outs = self.net(A, I)\n",
    "\n",
    "        self.outputs, self.Z = self.layer(A, self.net_outs)\n",
    "\n",
    "        self.hidden1 = self.gc1(self.Z, self.outputs)\n",
    "        self.hidden1 = self.attn1(self.hidden1)\n",
    "        self.hidden2 = self.gc2(self.hidden1, self.outputs)\n",
    "        self.hidden2 = self.attn2(self.hidden2)\n",
    "\n",
    "        z = self.hidden2\n",
    "        z = (z + z.t())/2\n",
    "        idx = torch.eye(self.hr_dim, dtype=bool) \n",
    "        z[idx]=1\n",
    "\n",
    "        return torch.abs(z), self.net_outs, self.start_gcn_outs, self.outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we have the train and test functions for GSRapid. Interestingly, we use MAE loss as the critereon instead of MSE. At first, theoretically we believed that using either would not change the model learning much as MSE is just a square function on MAE. But, MSE is more sensitive to outliers than MAE because the errors are squared before they are averaged, which disproportionately increases the influence of large errors. This means that models trained with MSE are likely to focus more on data points with larger errors, potentially at the expense of performing well on the majority of the data. This is what we observed as well. Switching to MAE yielded better performance on the public test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-13T01:17:59.245204Z",
     "iopub.status.busy": "2024-03-13T01:17:59.244274Z",
     "iopub.status.idle": "2024-03-13T01:17:59.271887Z",
     "shell.execute_reply": "2024-03-13T01:17:59.270917Z",
     "shell.execute_reply.started": "2024-03-13T01:17:59.245147Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.L1Loss()\n",
    "\n",
    "def train(model, optimizer, subjects_adj, subjects_labels, test_adj, test_labels, min_epochs=100, max_epochs=-1, early_stopping=10, padding=0, lmbda=32):\n",
    "\n",
    "    best_model_dict = None\n",
    "    best_loss = np.inf\n",
    "\n",
    "    best_epoch = -1\n",
    "\n",
    "    training_losses = []\n",
    "    test_losses = []\n",
    "\n",
    "    epochs_without_improvement = 0\n",
    "\n",
    "    epoch = 0\n",
    "\n",
    "    while (epochs_without_improvement < early_stopping or epoch < min_epochs) and (max_epochs == -1 or epoch < max_epochs):\n",
    "\n",
    "        epoch+=1\n",
    "\n",
    "        epoch_training_loss = []\n",
    "        epoch_test_loss = []\n",
    "\n",
    "        for lr,hr in zip(subjects_adj,subjects_labels):\n",
    "\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            lr = torch.from_numpy(lr).type(torch.FloatTensor)\n",
    "            hr = torch.from_numpy(hr).type(torch.FloatTensor)\n",
    "\n",
    "            model_outputs,net_outs,start_gcn_outs,layer_outs = model(lr)\n",
    "            model_outputs  = unpad(model_outputs, padding)\n",
    "            padded_hr = pad_HR_adj(hr,padding)\n",
    "            eig_val_hr, U_hr = torch.linalg.eigh(padded_hr, UPLO='U')\n",
    "\n",
    "            loss = lmbda * criterion(net_outs, start_gcn_outs) + criterion(model.layer.weights,U_hr) + criterion(model_outputs, hr)\n",
    "            epoch_training_loss.append(loss.item())\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(\"Epoch: \", epoch, \"Training loss: \", np.mean(epoch_training_loss))\n",
    "        training_losses.append(np.mean(epoch_training_loss))\n",
    "\n",
    "        # Find test error if we want to early stop to seek the best number of epochs\n",
    "        if max_epochs == -1:\n",
    "            model.eval()\n",
    "\n",
    "            for lr, hr in zip(test_adj,test_labels):\n",
    "\n",
    "                lr = torch.from_numpy(lr).type(torch.FloatTensor)\n",
    "                hr = torch.from_numpy(hr).type(torch.FloatTensor)\n",
    "\n",
    "                model_outputs,net_outs,start_gcn_outs,layer_outs = model(lr)\n",
    "                model_outputs  = unpad(model_outputs, padding)\n",
    "                padded_hr = pad_HR_adj(hr,padding)\n",
    "                eig_val_hr, U_hr = torch.linalg.eigh(padded_hr, UPLO='U')\n",
    "\n",
    "                loss = lmbda * criterion(net_outs, start_gcn_outs) + criterion(model.layer.weights,U_hr) + criterion(model_outputs, hr)\n",
    "                epoch_test_loss.append(loss.item())\n",
    "\n",
    "            epochs_without_improvement += 1\n",
    "\n",
    "            print(\"Epoch: \", epoch, \"Test loss: \", np.mean(epoch_test_loss))\n",
    "            test_losses.append(np.mean(epoch_test_loss))\n",
    "\n",
    "            if np.mean(epoch_test_loss) < best_loss:\n",
    "                best_loss = np.mean(epoch_test_loss)\n",
    "                best_model_dict = model.state_dict()\n",
    "                epochs_without_improvement = 0\n",
    "                best_epoch = epoch\n",
    "\n",
    "    return (model, training_losses) if max_epochs != -1 else (best_epoch, training_losses, test_losses)\n",
    "    \n",
    "\n",
    "def test(model, test_adj, test_labels, file, padding=0):\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    all_preds = []\n",
    "\n",
    "    for lr, hr in zip(test_adj,test_labels):\n",
    "\n",
    "        lr = torch.from_numpy(lr).type(torch.FloatTensor)\n",
    "        hr = torch.from_numpy(hr).type(torch.FloatTensor)\n",
    "\n",
    "        preds,_,_,_ = model(lr)\n",
    "        preds = unpad(preds, padding)\n",
    "        all_preds.append(preds.detach().numpy())\n",
    "\n",
    "    all_preds = np.array(all_preds)\n",
    "\n",
    "    _test_labels = test_labels.to_numpy()\n",
    "    _test_labels = np.array([np.array([np.array(x) for x in p]) for p in _test_labels])\n",
    "\n",
    "    metrics = evaluate_all(\n",
    "            _test_labels, all_preds, output_path=file\n",
    "        )\n",
    "\n",
    "    return all_preds, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the code for 3 fold cross validation. We track the memory usage as well as time taken to train and plot the memory usage at the end of the training. The testing takes significant time as processing different test metrics is very computationally heavy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-13T01:17:59.274316Z",
     "iopub.status.busy": "2024-03-13T01:17:59.273677Z",
     "iopub.status.idle": "2024-03-13T03:07:24.170829Z",
     "shell.execute_reply": "2024-03-13T03:07:24.169151Z",
     "shell.execute_reply.started": "2024-03-13T01:17:59.274284Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# experimented with different values for ks\n",
    "# ks = [0]\n",
    "# ks = [0.9, 0.7, 0.6, 0.5, 0.4, 0.3]\n",
    "ks = [0.9, 0.7, 0.6]\n",
    "\n",
    "fold_results = []\n",
    "\n",
    "def save_losses(test_index, training_losses, validation_losses=None):\n",
    "    fig = plt.figure()\n",
    "    plt.plot(training_losses, label='Training loss')\n",
    "    if validation_losses is not None:\n",
    "        plt.plot(validation_losses, label='Validation loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Fold ' + str(test_index))\n",
    "    plt.savefig(f'./loss_fold_{test_index}_validation.png' if validation_losses is not None else f'.loss_fold_{test_index}_training.png')\n",
    "    plt.close(fig)\n",
    "\n",
    "def find_max_epochs(test_index):\n",
    "    model = GSRapid(ks)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    subjects_adj, subjects_ground_truth, test_adj, test_ground_truth = data.obtain_folds(test_index, with_validation=True)\n",
    "    max_epochs, training_losses, validation_losses = train(model, optimizer, subjects_adj, subjects_ground_truth, test_adj, test_ground_truth)\n",
    "    save_losses(test_index+1, training_losses, validation_losses)\n",
    "    return max_epochs\n",
    "\n",
    "total_training_time = 0\n",
    "for test_index in range(3):\n",
    "    start_time = time.time()\n",
    "    max_epochs = find_max_epochs(test_index)\n",
    "    print(\"Number of epochs for fold\", test_index+1, \"is\", max_epochs)\n",
    "    model = GSRapid(ks)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    subjects_adj, subjects_ground_truth, test_adj, test_ground_truth = data.obtain_folds(test_index, with_validation=False)\n",
    "    model, losses = train(model, optimizer, subjects_adj, subjects_ground_truth, test_adj, test_ground_truth, max_epochs=max_epochs)\n",
    "    total_training_time += time.time() - start_time\n",
    "    save_losses(test_index+1, losses)\n",
    "    predictions, metrics = test(model, test_adj, test_ground_truth, file=f'./Cluster CV/metrics.csv')\n",
    "    fold_results.append(metrics)\n",
    "    outputs = []\n",
    "    for i,matrix in enumerate(predictions):\n",
    "        vector = vectorise_and_reshape(matrix)\n",
    "        outputs.append(vector)\n",
    "    outputs_df = pd.DataFrame(outputs)\n",
    "    meltedDF = outputs_df.to_numpy().flatten()\n",
    "    predicted_df = pd.DataFrame({\n",
    "    \"Predicted\": meltedDF\n",
    "    })\n",
    "    predicted_df['ID'] = range(1, len(meltedDF)+1)\n",
    "    predicted_df = predicted_df[['ID', 'Predicted']]\n",
    "    prediction_csv_path = f'./Cluster CV/predictions_fold_{test_index+1}.csv'\n",
    "    predicted_df.to_csv(prediction_csv_path, index=False)\n",
    "    \n",
    "print(f\"Total training time: {total_training_time} seconds.\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 7783828,
     "sourceId": 71243,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30646,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
