{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tgYJMVfK5rzt",
    "outputId": "b9c7461a-291a-4330-bdd3-9df5bd663e7d"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ueu6yOrf8sHO",
    "outputId": "fd949ff4-c4d7-4e2b-d833-27c36e45f24b"
   },
   "outputs": [],
   "source": [
    "!pip install -r requirements19.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4S0FCc_-Mzjp"
   },
   "source": [
    "# **Set the file paths and notes**\n",
    "\n",
    "<p>Use option = 0 if you want to do training and validation using 2 of the folds (keeping the 3rd fold for final training and testing).</p>\n",
    "\n",
    "<p>Use option = 1 if you want to do final training (using 2 folds) and testing (with the 3rd fold).</p>\n",
    "\n",
    "Be sure to update the name of the files - doing 3 cross-validation in this project is done manually with the prepared data to create comparable results with the rest of the projects. See an example below.\n",
    "\n",
    "Due to the nature of this team's model, early stopping is carried out by manual experimentation during testing and validation for 3CV. To help reach a decision on the best number of epochs for training - refer to the tables with the loss of the generator and discriminator.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zVbwCqFLMxpr"
   },
   "outputs": [],
   "source": [
    "option = 1\n",
    "\n",
    "# path for training and validation (without using the 3rd fold). This is an example from the ClusterCV folder\n",
    "path_lr_data_tr = 'lr_split_AandB_training.csv'\n",
    "path_hr_data_tr = 'hr_split_AandB_training.csv'\n",
    "path_lr_data_valid = 'lr_split_AandB_validation.csv'\n",
    "path_hr_data_valid = 'hr_split_AandB_validation.csv'\n",
    "\n",
    "# path for final training and testing (using fold A and B for training and C for testing).This is an example from the ClusterCV folder\n",
    "path_lr_data = 'lr_split_AandB_finaltraining.csv'\n",
    "path_hr_data = 'hr_split_AandB_finaltraining.csv'\n",
    "path_lr_data_test = 'lr_clusterC.csv'\n",
    "path_hr_data_test = 'hr_clusterC.csv'\n",
    "\n",
    "# path to save the evaluation metrics to - this means that Cluster C will be used for testing\n",
    "path_eval_matrics = '19-clusterCV-split3.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KLSIpZUC49Lw"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv, GINConv\n",
    "\n",
    "\"\"\"\n",
    "Define layers:\n",
    "    GraphConvolution\n",
    "    GIN\n",
    "    GAT\n",
    "    GCN\n",
    "    GraphUnpool\n",
    "    GraphPool\n",
    "    Dense\n",
    "\"\"\"\n",
    "\n",
    "class GraphConvolution(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple Graph Convolutional Network (GCN) layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, dropout, act=F.relu):\n",
    "        \"\"\"\n",
    "        Initialize the GraphConvolution layer.\n",
    "\n",
    "        Args:\n",
    "            in_features (int): Number of input features.\n",
    "            out_features (int): Number of output features.\n",
    "            dropout (float): Dropout probability.\n",
    "            act (function, optional): Activation function, default is ReLU.\n",
    "        \"\"\"\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.dropout = dropout\n",
    "        self.act = act\n",
    "        self.weight = torch.nn.Parameter(\n",
    "            torch.FloatTensor(in_features, out_features))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        \"\"\"\n",
    "        Resets the parameters using Xavier uniform initialization.\n",
    "        \"\"\"\n",
    "        torch.nn.init.xavier_uniform_(self.weight)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        \"\"\"\n",
    "        Forward pass of the GraphConvolution layer.\n",
    "\n",
    "        Args:\n",
    "            input (torch.Tensor): Input matrix.\n",
    "            adj (torch.Tensor): Adjacency matrix.\n",
    "\n",
    "        Returns:\n",
    "            output (torch.Tensor): Aggregation output.\n",
    "        \"\"\"\n",
    "        input = F.dropout(input, self.dropout, self.training)\n",
    "        support = torch.mm(input, self.weight)\n",
    "        output = torch.mm(adj, support)\n",
    "        output = self.act(output)\n",
    "        return output\n",
    "\n",
    "class GIN(nn.Module):\n",
    "    \"\"\"\n",
    "    Graph Isomorphism Network (GIN) module.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        \"\"\"\n",
    "        Initialize the GIN module.\n",
    "\n",
    "        Args:\n",
    "            in_dim (int): Dimensionality of input features.\n",
    "            out_dim (int): Dimensionality of output features.\n",
    "        \"\"\"\n",
    "        super(GIN, self).__init__()\n",
    "        self.gin_conv = GINConv(nn.Linear(in_dim, out_dim))\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "    def forward(self, A, X):\n",
    "        \"\"\"\n",
    "        Forward pass of the GIN module.\n",
    "\n",
    "        Args:\n",
    "            A (torch.Tensor): The adjacency matrix.\n",
    "            X (torch.Tensor): The input matrix.\n",
    "\n",
    "        Returns:\n",
    "            X (torch.Tensor): The output matrix after GIN convolution.\n",
    "        \"\"\"\n",
    "        edge_index = convert_adj_to_edge_index(A)\n",
    "        X = self.gin_conv(X, edge_index)\n",
    "        X = self.act(X)\n",
    "\n",
    "        return X\n",
    "\n",
    "class GAT(nn.Module):\n",
    "    \"\"\"\n",
    "    Graph Attention Network (GAT) module.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        \"\"\"\n",
    "        Initialize the GIN module.\n",
    "\n",
    "        Args:\n",
    "            in_dim (int): Dimensionality of input features.\n",
    "            out_dim (int): Dimensionality of output features.\n",
    "        \"\"\"\n",
    "        super(GAT, self).__init__()\n",
    "        self.gat_conv = GATConv(in_dim, out_dim)\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "    def forward(self, A, X):\n",
    "        \"\"\"\n",
    "        Forward pass of the GIN module.\n",
    "\n",
    "        Args:\n",
    "            A (torch.Tensor): The adjacency matrix.\n",
    "            X (torch.Tensor): The input matrix.\n",
    "\n",
    "        Returns:\n",
    "            X (torch.Tensor): The output matrix after GIN convolution.\n",
    "        \"\"\"\n",
    "        edge_index = convert_adj_to_edge_index(A)\n",
    "        X = self.gat_conv(X, edge_index)\n",
    "        X = self.act(X)\n",
    "        # Add dropout\n",
    "        X = F.dropout(X, p=0.6, training=self.training)\n",
    "\n",
    "        return X\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    \"\"\"\n",
    "    Graph Convolution Network (GCN) module.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        \"\"\"\n",
    "        Initialize the GCN module.\n",
    "\n",
    "        Args:\n",
    "            in_dim (int): Dimensionality of input features.\n",
    "            out_dim (int): Dimensionality of output features.\n",
    "        \"\"\"\n",
    "        super(GCN, self).__init__()\n",
    "        self.proj = nn.Linear(in_dim, out_dim)\n",
    "        self.drop = nn.Dropout(p=0.6)\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "    def forward(self, A, X):\n",
    "        \"\"\"\n",
    "        Forward pass of the GCN module.\n",
    "\n",
    "        Args:\n",
    "            A (torch.Tensor): The adjacency matrix.\n",
    "            X (torch.Tensor): The input matrix.\n",
    "\n",
    "        Returns:\n",
    "            X (torch.Tensor): The output matrix after GCN convolution.\n",
    "        \"\"\"\n",
    "\n",
    "        X = self.drop(X)\n",
    "        X = torch.matmul(A, X)\n",
    "        X = self.proj(X)\n",
    "        X = self.act(X) # activation\n",
    "        return X\n",
    "\n",
    "class GraphUnpool(nn.Module):\n",
    "    \"\"\"\n",
    "    Graph Unpooling layer module.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the GraphUnpool layer.\n",
    "        \"\"\"\n",
    "        super(GraphUnpool, self).__init__()\n",
    "\n",
    "    def forward(self, A, X, idx):\n",
    "        \"\"\"\n",
    "        Forward pass of the GraphUnpool layer.\n",
    "\n",
    "        Args:\n",
    "            A (torch.Tensor): The adjacency matrix.\n",
    "            X (torch.Tensor): The node embedding matrix.\n",
    "            idx (torch.Tensor): Nodes to be unpolled.\n",
    "\n",
    "        Returns:\n",
    "            A (torch.Tensor): The unpolled adjacency matrix.\n",
    "            new_X (torch.Tensor): The unpolled node embedding matrix.\n",
    "        \"\"\"\n",
    "        new_X = torch.zeros([A.shape[0], X.shape[1]])\n",
    "        new_X[idx] = X\n",
    "        return A, new_X\n",
    "\n",
    "class GraphPool(nn.Module):\n",
    "    \"\"\"\n",
    "    Graph Pooling layer module.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, k, in_dim):\n",
    "        \"\"\"\n",
    "        Initialize the GraphPool layer.\n",
    "\n",
    "        Args:\n",
    "            k (float): The ratio of nodes to keep after pooling.\n",
    "            in_dim (int): Dimensionality of input features.\n",
    "        \"\"\"\n",
    "        super(GraphPool, self).__init__()\n",
    "        self.k = k\n",
    "        self.proj = nn.Linear(in_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, A, X):\n",
    "        \"\"\"\n",
    "        Forward pass of the GraphPool layer.\n",
    "\n",
    "        Args:\n",
    "            A (torch.Tensor): The adjacency matrix.\n",
    "            X (torch.Tensor): The node embedding matrix.\n",
    "\n",
    "        Returns:\n",
    "            A (torch.Tensor): The pooled adjacency matrix.\n",
    "            new_X (torch.Tensor): The pooled node embedding matrix.\n",
    "            idx (torch.Tensor): Indices of the nodes selected during pooling.\n",
    "        \"\"\"\n",
    "        scores = self.proj(X)\n",
    "        scores = torch.squeeze(scores)\n",
    "        scores = self.sigmoid(scores/100)\n",
    "        num_nodes = A.shape[0]\n",
    "        values, idx = torch.topk(scores, int(self.k*num_nodes))\n",
    "        new_X = X[idx, :]\n",
    "        values = torch.unsqueeze(values, -1)\n",
    "        new_X = torch.mul(new_X, values)\n",
    "        A = A[idx, :]\n",
    "        A = A[:, idx]\n",
    "        return A, new_X, idx\n",
    "\n",
    "class Dense(nn.Module):\n",
    "    \"\"\"\n",
    "    Dense layer module.\n",
    "    \"\"\"\n",
    "    def __init__(self, n1, n2, args):\n",
    "        \"\"\"\n",
    "        Initialize the Dense layer module.\n",
    "\n",
    "        Args:\n",
    "            n1 (int): Number of input features.\n",
    "            n2 (int): Number of output features.\n",
    "            args (object): Arguments object containing hyperparameters.\n",
    "        \"\"\"\n",
    "        super(Dense, self).__init__()\n",
    "        self.weights = torch.nn.Parameter(\n",
    "            torch.FloatTensor(n1, n2), requires_grad=True)\n",
    "        nn.init.normal_(self.weights, mean=args.mean_dense, std=args.std_dense)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the Dense (fully-connected) layer module.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input feature matrix.\n",
    "\n",
    "        Returns:\n",
    "            out (torch.Tensor): Output feature matrix after dense linear transformation.\n",
    "        \"\"\"\n",
    "        out = torch.mm(x, self.weights) # linear transformation\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aU5MIusO4HW0"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class MatrixVectorizer:\n",
    "    \"\"\"\n",
    "    A class for transforming between matrices and vector representations.\n",
    "\n",
    "    This class provides methods to convert a symmetric matrix into a vector (vectorize)\n",
    "    and to reconstruct the matrix from its vector form (anti_vectorize), focusing on\n",
    "    vertical (column-based) traversal and handling of elements.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes the MatrixVectorizer instance.\n",
    "\n",
    "        The constructor currently does not perform any actions but is included for\n",
    "        potential future extensions where initialization parameters might be required.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def vectorize(matrix, include_diagonal=False):\n",
    "        \"\"\"\n",
    "        Converts a matrix into a vector by vertically extracting elements.\n",
    "\n",
    "        This method traverses the matrix column by column, collecting elements from the\n",
    "        upper triangle, and optionally includes the diagonal elements immediately below\n",
    "        the main diagonal based on the include_diagonal flag.\n",
    "\n",
    "        Args:\n",
    "            matrix (numpy.ndarray): The matrix to be vectorized.\n",
    "            include_diagonal (bool, optional): Flag to include diagonal elements in the vectorization.\n",
    "                                                Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: The vectorized form of the matrix.\n",
    "        \"\"\"\n",
    "        # Determine the size of the matrix based on its first dimension\n",
    "        matrix_size = matrix.shape[0]\n",
    "\n",
    "        # Initialize an empty list to accumulate vector elements\n",
    "        vector_elements = []\n",
    "\n",
    "        # Iterate over columns and then rows to collect the relevant elements\n",
    "        for col in range(matrix_size):\n",
    "            for row in range(matrix_size):\n",
    "                # Skip diagonal elements if not including them\n",
    "                if row != col:\n",
    "                    if row < col:\n",
    "                        # Collect upper triangle elements\n",
    "                        vector_elements.append(matrix[row, col])\n",
    "                    elif include_diagonal and row == col + 1:\n",
    "                        # Optionally include the diagonal elements immediately below the diagonal\n",
    "                        vector_elements.append(matrix[row, col])\n",
    "\n",
    "        return np.array(vector_elements)\n",
    "\n",
    "    @staticmethod\n",
    "    def anti_vectorize(vector, matrix_size, include_diagonal=False):\n",
    "        \"\"\"\n",
    "        Reconstructs a matrix from its vector form, filling it vertically.\n",
    "\n",
    "        The method fills the matrix by reflecting vector elements into the upper triangle\n",
    "        and optionally including the diagonal elements based on the include_diagonal flag.\n",
    "\n",
    "        Args:\n",
    "            vector (numpy.ndarray): The vector to be transformed into a matrix.\n",
    "            matrix_size (int): The size of the square matrix to be reconstructed.\n",
    "            include_diagonal (bool, optional): Flag to include diagonal elements in the reconstruction.\n",
    "                                                Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: The reconstructed square matrix.\n",
    "        \"\"\"\n",
    "        # Initialize a square matrix of zeros with the specified size\n",
    "        matrix = np.zeros((matrix_size, matrix_size))\n",
    "\n",
    "        # Index to keep track of the current position in the vector\n",
    "        vector_idx = 0\n",
    "\n",
    "        # Fill the matrix by iterating over columns and then rows\n",
    "        for col in range(matrix_size):\n",
    "            for row in range(matrix_size):\n",
    "                # Skip diagonal elements if not including them\n",
    "                if row != col:\n",
    "                    if row < col:\n",
    "                        # Reflect vector elements into the upper triangle and its mirror in the lower triangle\n",
    "                        matrix[row, col] = vector[vector_idx]\n",
    "                        matrix[col, row] = vector[vector_idx]\n",
    "                        vector_idx += 1\n",
    "                    elif include_diagonal and row == col + 1:\n",
    "                        # Optionally fill the diagonal elements after completing each column\n",
    "                        matrix[row, col] = vector[vector_idx]\n",
    "                        matrix[col, row] = vector[vector_idx]\n",
    "                        vector_idx += 1\n",
    "\n",
    "        return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "25KPGbkL4JGE"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def weight_variable_glorot(output_dim):\n",
    "    \"\"\"\n",
    "    Initialize weights using Xavier initialization.\n",
    "\n",
    "    Args:\n",
    "        output_dim (int): The number of output dimensions for the weight matrix.\n",
    "\n",
    "    Returns:\n",
    "        initial (numpy.ndarray): Randomly initialized square weight matrix.\n",
    "    \"\"\"\n",
    "\n",
    "    input_dim = output_dim\n",
    "    init_range = np.sqrt(6.0 / (input_dim + output_dim))\n",
    "    initial = np.random.uniform(-init_range, init_range,\n",
    "                                (input_dim, output_dim))\n",
    "\n",
    "    return initial\n",
    "\n",
    "def convert_adj_to_edge_index(adj):\n",
    "    \"\"\"\n",
    "    Convert adjacency matrix to edge index representation.\n",
    "\n",
    "    Args:\n",
    "        adj (torch.Tensor): Adjacency matrix.\n",
    "\n",
    "    Returns:\n",
    "        edge_index (torch.Tensor): Edge index representation of the graph.\n",
    "    \"\"\"\n",
    "    # Assuming adj is a square matrix\n",
    "    # Find non-zero elements in the adjacency matrix and\n",
    "    # transpose the result for edge index representation\n",
    "    edge_index = torch.nonzero(adj, as_tuple=False).t().contiguous()\n",
    "    return edge_index\n",
    "\n",
    "def pad_HR_adj(label, split):\n",
    "    \"\"\"\n",
    "    Pad the HR matrix with zero padding.\n",
    "\n",
    "    Args:\n",
    "        label (numpy.ndarray): The HR matrix.\n",
    "        split (int): The amount of padding to add to each edge of the matrix.\n",
    "\n",
    "    Returns:\n",
    "        label (numpy.ndarray): The padded HR adjacency matrix.\n",
    "    \"\"\"\n",
    "\n",
    "    label = np.pad(label, ((split, split), (split, split)), mode=\"constant\")\n",
    "    # Set diagonal elements to 1\n",
    "    np.fill_diagonal(label, 1)\n",
    "    return label\n",
    "\n",
    "def unpad(data, split):\n",
    "    \"\"\"\n",
    "    Unpad the padded matrix.\n",
    "\n",
    "    Args:\n",
    "        data (numpy.ndarray): The padded matrix .\n",
    "        split (int): The amount of padding to be removed from each edge of the array.\n",
    "\n",
    "    Returns:\n",
    "        train (numpy.ndarray): The unpadded matrix.\n",
    "    \"\"\"\n",
    "\n",
    "    idx_0 = data.shape[0]-split\n",
    "    idx_1 = data.shape[1]-split\n",
    "    train = data[split:idx_0, split:idx_1]\n",
    "    return train\n",
    "\n",
    "def normalize_adj_torch(mx):\n",
    "    \"\"\"\n",
    "    Normalize the input adjacency matrix using Kipf normalization.\n",
    "\n",
    "    Args:\n",
    "        mx (torch.Tensor): The input adjacency matrix.\n",
    "\n",
    "    Returns:\n",
    "        mx (torch.Tensor): The normalized adjacency matrix.\n",
    "    \"\"\"\n",
    "    rowsum = mx.sum(1) # calculate the row sums\n",
    "    r_inv_sqrt = torch.pow(rowsum, -0.5).flatten()\n",
    "    r_inv_sqrt[torch.isinf(r_inv_sqrt)] = 0. # replace any infinite values with 0\n",
    "    r_mat_inv_sqrt = torch.diag(r_inv_sqrt)\n",
    "\n",
    "    # Perform Kipf normalization\n",
    "    mx = torch.matmul(mx, r_mat_inv_sqrt)\n",
    "    mx = torch.transpose(mx, 0, 1)\n",
    "    mx = torch.matmul(mx, r_mat_inv_sqrt)\n",
    "    return mx\n",
    "\n",
    "def gaussian_noise_layer(input_layer, args):\n",
    "    \"\"\"\n",
    "    Apply Gaussian noise to the input layer.\n",
    "\n",
    "    Args:\n",
    "        input_layer (torch.Tensor): Input tensor to which Gaussian noise will be added.\n",
    "        args (object): Arguments object containing hyperparameters.\n",
    "\n",
    "    Returns:\n",
    "        z (torch.Tensor): Output tensor with Gaussian noise.\n",
    "    \"\"\"\n",
    "    z = torch.empty_like(input_layer)\n",
    "    noise = z.normal_(mean=args.mean_gaussian, std=args.std_gaussian)\n",
    "    z = torch.abs(input_layer + noise)\n",
    "\n",
    "    z = (z + z.t())/2\n",
    "    z = z.fill_diagonal_(1)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RHqlvINd4JIY"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Define core blocks:\n",
    "    GraphUnet\n",
    "    GSRLayer\n",
    "    IWASAGSRNet\n",
    "    Discriminator\n",
    "\"\"\"\n",
    "\n",
    "class GraphUnet(nn.Module):\n",
    "    \"\"\"\n",
    "    Graph U-Net module.\n",
    "    \"\"\"\n",
    "    def __init__(self, ks, in_dim, out_dim, dim=320):\n",
    "        \"\"\"\n",
    "        Initialize the GraphUnet module.\n",
    "\n",
    "        Args:\n",
    "            ks (list): List of pooling ratios for each down-sampling step.\n",
    "            in_dim (int): Dimensionality of input features.\n",
    "            out_dim (int): Dimensionality of output features.\n",
    "            dim (int): Dimensionality of hidden features (default is 320).\n",
    "        \"\"\"\n",
    "        super(GraphUnet, self).__init__()\n",
    "        self.ks = ks\n",
    "\n",
    "        # Using GAT\n",
    "        self.start_gat = GAT(in_dim, dim)\n",
    "        self.bottom_gat = GAT(dim, dim)\n",
    "        self.end_gat = GAT(2*dim, out_dim)\n",
    "        self.down_gcns = []\n",
    "        self.up_gcns = []\n",
    "        self.pools = []\n",
    "        self.unpools = []\n",
    "        self.l_n = len(ks)\n",
    "        for i in range(self.l_n):\n",
    "            self.down_gcns.append(GCN(dim, dim))\n",
    "            self.up_gcns.append(GCN(dim, dim))\n",
    "            self.pools.append(GraphPool(ks[i], dim))\n",
    "            self.unpools.append(GraphUnpool())\n",
    "\n",
    "    def forward(self, A, X):\n",
    "        \"\"\"\n",
    "        Forward pass of the GraphUnet module.\n",
    "\n",
    "        Args:\n",
    "            A (torch.Tensor): The adjacency matrix of the input graph.\n",
    "            X (torch.Tensor): The input node embedding matrix.\n",
    "\n",
    "        Returns:\n",
    "            X (torch.Tensor): The output node embedding matrix.\n",
    "            start_gat_outs (torch.Tensor): The start GAT layer output.\n",
    "        \"\"\"\n",
    "        adj_ms = [] # list of adjacency matrices at each down-sampling step\n",
    "        indices_list = [] # list of indices for unpooling\n",
    "        down_outs = [] # list of node embedding matrices at each down-sampling step\n",
    "\n",
    "        # Initial GAT convolution\n",
    "        X = self.start_gat(A, X)\n",
    "        start_gat_outs = X\n",
    "        org_X = X # original node embedding matrix\n",
    "\n",
    "        # Build stacked U-Net\n",
    "        for _ in range(2):\n",
    "            for i in range(self.l_n):\n",
    "                X = self.down_gcns[i](A, X) # down-sampling with GCN\n",
    "                adj_ms.append(A)\n",
    "                down_outs.append(X)\n",
    "                A, X, idx = self.pools[i](A, X) # pooling\n",
    "                indices_list.append(idx)\n",
    "\n",
    "            X = self.bottom_gat(A, X)\n",
    "\n",
    "            for i in range(self.l_n):\n",
    "                up_idx = self.l_n - i - 1 # index for unpooling\n",
    "                A, idx = adj_ms[up_idx], indices_list[up_idx]\n",
    "                A, X = self.unpools[i](A, X, idx) # unpooling\n",
    "                X = self.up_gcns[i](A, X) # up-sampling with GCN\n",
    "                X = X.add(down_outs[up_idx])\n",
    "\n",
    "            X = torch.cat([X, org_X], 1)\n",
    "            # Final GAT convolution\n",
    "            X = self.end_gat(A, X)\n",
    "\n",
    "        return X, start_gat_outs\n",
    "\n",
    "class GSRLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Graph Super Resolution module.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hr_dim):\n",
    "        \"\"\"\n",
    "        Initialize the GSRLayer.\n",
    "\n",
    "        Args:\n",
    "            hr_dim (int): Dimension of the high-resolution (HR) matrix.\n",
    "        \"\"\"\n",
    "        super(GSRLayer, self).__init__()\n",
    "\n",
    "        # Initialize weight matrix\n",
    "        self.weights = torch.from_numpy(\n",
    "            weight_variable_glorot(hr_dim)).type(torch.FloatTensor)\n",
    "        self.weights = torch.nn.Parameter(\n",
    "            data=self.weights, requires_grad=True)\n",
    "\n",
    "    def forward(self, A, X):\n",
    "        \"\"\"\n",
    "        Forward pass of the GSRLayer.\n",
    "\n",
    "        Args:\n",
    "            A (torch.Tensor): Adjacency matrix of the low-resolution (LR) graph.\n",
    "            X (torch.Tensor): The input node embedding matrix.\n",
    "\n",
    "        Returns:\n",
    "            adj (torch.Tensor): The super-resolved graph structure.\n",
    "            torch.abs(X) (torch.Tensor): The super-resolved graph node features.\n",
    "        \"\"\"\n",
    "        with torch.autograd.set_detect_anomaly(True):\n",
    "\n",
    "            # Extract LR adjacency matrix and dimension\n",
    "            lr = A\n",
    "            lr_dim = lr.shape[0]\n",
    "\n",
    "            # Extract node embedding matrix\n",
    "            f = X\n",
    "\n",
    "            # Eigenvector decomposition of the LR adjacency matrix\n",
    "            eig_val_lr, U_lr = torch.linalg.eigh(lr, UPLO='U')\n",
    "\n",
    "            # Generate S_d (the transposed concatenation of 2 identity matrices)\n",
    "            eye_mat = torch.eye(lr_dim).type(torch.FloatTensor)\n",
    "            s_d = torch.cat((eye_mat, eye_mat), 0)\n",
    "\n",
    "            # Super-resolving the graph structure\n",
    "            a = torch.matmul(self.weights, s_d)\n",
    "            b = torch.matmul(a, torch.t(U_lr))\n",
    "            f_d = torch.matmul(b, f)\n",
    "            f_d = torch.abs(f_d)\n",
    "            f_d = f_d.fill_diagonal_(1)\n",
    "            adj = f_d\n",
    "\n",
    "            # Super-resolving the graph node features\n",
    "            X = torch.mm(adj, adj.t())\n",
    "            X = (X + X.t())/2\n",
    "            X = X.fill_diagonal_(1)\n",
    "\n",
    "        return adj, torch.abs(X)\n",
    "\n",
    "class IWASAGSRNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Graph Super-Resolution Network (IWAS-AGSRNet) module.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, args):\n",
    "        \"\"\"\n",
    "        Initialize the IWAS-AGSRNet module.\n",
    "\n",
    "        Args:\n",
    "            args (object): Arguments of input hyperparameters.\n",
    "        \"\"\"\n",
    "        super(IWASAGSRNet, self).__init__()\n",
    "\n",
    "        self.lr_dim = args.lr_dim\n",
    "        self.hr_dim = args.hr_dim\n",
    "        self.hidden_dim = args.hidden_dim\n",
    "\n",
    "        # Initialize graph layers\n",
    "        self.layer = GSRLayer(self.hr_dim)\n",
    "        self.net = GraphUnet(args.ks, self.lr_dim, self.hr_dim)\n",
    "        self.gc1 = GraphConvolution(\n",
    "            self.hr_dim, self.hidden_dim, args.dropout, act=F.relu)\n",
    "        self.gc2 = GraphConvolution(\n",
    "            self.hidden_dim, self.hr_dim, args.dropout, act=F.relu)\n",
    "        self.gin = GIN(self.hr_dim, self.hr_dim)\n",
    "\n",
    "    def forward(self, lr, lr_dim):\n",
    "        \"\"\"\n",
    "        Forward pass of the IWASAGSRNet module.\n",
    "\n",
    "        Args:\n",
    "            lr (torch.Tensor): LR adjacency matrix.\n",
    "            lr_dim (int): Dimensionality of the LR input features.\n",
    "\n",
    "        Returns:\n",
    "            z (torch.Tensor): Generated HR adjacency matrix.\n",
    "            unet_outs (torch.Tensor): The output node embedding matrix from the stacked U-Net.\n",
    "            start_gat_outs (torch.Tensor): The output node embedding matrix from the start GAT layer from the stacked U-Net.\n",
    "        \"\"\"\n",
    "        with torch.autograd.set_detect_anomaly(True):\n",
    "            # Initialize the node embedding matrix to be identity matrix\n",
    "            I = torch.eye(lr_dim).type(torch.FloatTensor)\n",
    "            # Normalize LR adjacency matrix\n",
    "            A = normalize_adj_torch(lr).type(torch.FloatTensor)\n",
    "\n",
    "            # Pass LR graph through stacked U-Net\n",
    "            unet_outs, start_gat_outs = self.net(A, I)\n",
    "\n",
    "            # Generate HR graph using GSRLayer\n",
    "            outputs, Z = self.layer(A, unet_outs)\n",
    "\n",
    "            # Perform GCN and GIN to aggregate information\n",
    "            hidden1 = self.gc1(Z, outputs)\n",
    "            hidden2 = self.gc2(hidden1, outputs)\n",
    "            z = self.gin(hidden2, outputs)\n",
    "\n",
    "            # Symmetrize the output adjacency matrix\n",
    "            z = (z + z.t())/2\n",
    "            z = z.fill_diagonal_(1)\n",
    "\n",
    "            # Post-process the output to be non-negative values\n",
    "            z = torch.abs(z)\n",
    "\n",
    "        return z, unet_outs, start_gat_outs\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    \"\"\"\n",
    "    Discriminator module.\n",
    "    \"\"\"\n",
    "    def __init__(self, args):\n",
    "        \"\"\"\n",
    "        Initialize the Discriminator module.\n",
    "\n",
    "        Args:\n",
    "            args (object): Arguments object containing hyperparameters.\n",
    "        \"\"\"\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        # Initialize dense layers and activation functions\n",
    "        self.dense_1 = Dense(args.hr_dim, args.hr_dim, args)\n",
    "        self.relu_1 = nn.LeakyReLU(0.1, inplace=False)\n",
    "        self.dense_2 = Dense(args.hr_dim, args.hr_dim, args)\n",
    "        self.relu_2 = nn.LeakyReLU(0.1, inplace=False)\n",
    "        self.dense_3 = Dense(args.hr_dim, 1, args)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Forward pass of the Discriminator module.\n",
    "\n",
    "        Args:\n",
    "            inputs (torch.Tensor): Input feature matrix (generated HR matrix).\n",
    "\n",
    "        Returns:\n",
    "            output (torch.Tensor): Probability scores indicating real or fake samples.\n",
    "        \"\"\"\n",
    "        dc_den1 = self.relu_1(self.dense_1(inputs))\n",
    "        dc_den2 = self.relu_2(self.dense_2(dc_den1))\n",
    "        output = self.sigmoid(self.dense_3(dc_den2))\n",
    "        output = torch.abs(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kuhm4Y4N4JK9"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\"\"\"\n",
    "Define training and testing functions\n",
    "\"\"\"\n",
    "\n",
    "def train(model, subjects_adj, subjects_labels, args):\n",
    "    \"\"\"\n",
    "    Train the IWAS-AGSRNet model.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): IWAS-AGSRNet model to be trained.\n",
    "        subjects_adj (list): Training set.\n",
    "        subjects_labels (list): Training labels.\n",
    "        args (object): Arguments object containing hyperparameters.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define criterions to calculate loss\n",
    "    mse_criterion = nn.MSELoss() # for IWAS-AGSRNet\n",
    "    bce_criterion = nn.BCELoss() # for Discriminator\n",
    "\n",
    "    # Initialize Discriminator\n",
    "    netD = Discriminator(args)\n",
    "\n",
    "    # Prepare optimizers\n",
    "    optimizerG = optim.Adam(model.parameters(), lr=args.lr_d, betas=(0.5, 0.999))\n",
    "    optimizerD = optim.Adam(netD.parameters(), lr=args.lr_g, betas=(0.5, 0.999))\n",
    "\n",
    "    # List for loss in all epochs\n",
    "    all_epochs_loss = []\n",
    "    for epoch in range(args.epochs):\n",
    "        with torch.autograd.set_detect_anomaly(True):\n",
    "            epoch_loss = []\n",
    "            epoch_error = []\n",
    "            epoch_gen = []\n",
    "            epoch_mse = []\n",
    "            epoch_dc = []\n",
    "            epoch_unet = []\n",
    "\n",
    "            for lr, hr in zip(subjects_adj, subjects_labels):\n",
    "                optimizerD.zero_grad()\n",
    "                optimizerG.zero_grad()\n",
    "\n",
    "                # Process input LR and HR matrices\n",
    "                hr = pad_HR_adj(hr, args.padding) # pad input HR adj matrix\n",
    "                # Convert to tensor\n",
    "                lr = torch.from_numpy(lr).type(torch.FloatTensor)\n",
    "                padded_hr = torch.from_numpy(hr).type(torch.FloatTensor)\n",
    "\n",
    "                # Eigenvector decomposition of HR matrix\n",
    "                _, U_hr = torch.linalg.eigh(padded_hr, UPLO='U')\n",
    "\n",
    "                # Pass through IWAS-AGSRNet to generate outputs\n",
    "                model_outputs, net_outs, start_gat_outs = model(lr, args.lr_dim)\n",
    "\n",
    "                # Define MSE loss\n",
    "                mse_loss = args.lmbda * mse_criterion(net_outs, start_gat_outs) + mse_criterion(\n",
    "                    model.layer.weights, U_hr) + mse_criterion(model_outputs, padded_hr)\n",
    "\n",
    "                # Calculate MSE loss between the generated and real HR matrices\n",
    "                error = mse_criterion(model_outputs, padded_hr)\n",
    "\n",
    "                # Train Discriminator\n",
    "                real_data = model_outputs.detach()\n",
    "                fake_data = gaussian_noise_layer(padded_hr, args)\n",
    "\n",
    "                d_real = netD(real_data)\n",
    "                d_fake = netD(fake_data)\n",
    "\n",
    "                dc_loss_real = bce_criterion(d_real, torch.ones(args.hr_dim, 1))\n",
    "                dc_loss_fake = bce_criterion(d_fake, torch.zeros(args.hr_dim, 1))\n",
    "                dc_loss = dc_loss_real + dc_loss_fake\n",
    "\n",
    "                dc_loss.backward()\n",
    "                optimizerD.step()\n",
    "\n",
    "                d_fake = netD(gaussian_noise_layer(padded_hr, args))\n",
    "\n",
    "                gen_loss = bce_criterion(d_fake, torch.ones(args.hr_dim, 1))\n",
    "                generator_loss = gen_loss + mse_loss\n",
    "                generator_loss.backward()\n",
    "                optimizerG.step()\n",
    "\n",
    "                epoch_loss.append(generator_loss.item())\n",
    "                epoch_error.append(error.item())\n",
    "\n",
    "                # Append other losses for monitoring\n",
    "                unet_loss = args.lmbda * mse_criterion(net_outs, start_gat_outs)\n",
    "                epoch_gen.append(gen_loss.item())\n",
    "                epoch_mse.append(mse_loss.item())\n",
    "                epoch_dc.append(dc_loss.item())\n",
    "                epoch_unet.append(unet_loss.item())\n",
    "\n",
    "            print(\"Epoch: \", epoch, \"Loss: \", np.mean(epoch_loss),\n",
    "                  \"Error: \", np.mean(epoch_error)*100, \"%\",\n",
    "                  \"UNet Loss: \", np.mean(epoch_unet),\n",
    "                  \"DC Loss: \", np.mean(epoch_dc),\n",
    "                  \"Gen Loss: \", np.mean(epoch_gen),\n",
    "                  \"MSE Loss: \", np.mean(epoch_mse))\n",
    "\n",
    "            all_epochs_loss.append(np.mean(epoch_loss))\n",
    "\n",
    "def test(model, test_adj, test_labels, args):\n",
    "    \"\"\"\n",
    "    Make prediction on a test set to generate HR matrix for every test sample.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The neural network model to be evaluated.\n",
    "        test_adj (list): List of adjacency matrices representing the test set.\n",
    "        test_labels (list): List of ground truth matrices corresponding to the test set.\n",
    "        args (Argument): An Argument object containing various parameters.\n",
    "\n",
    "    Returns:\n",
    "        pred_matrices (numpy.ndarray): Array containing the model predictions for each test sample.\n",
    "        gt_matrices (numpy.ndarray): Array containing the ground truth matrices for each test sample.\n",
    "    \"\"\"\n",
    "    # set model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Loss function\n",
    "    criterion = nn.MSELoss()\n",
    "    criterion_mae = nn.L1Loss()\n",
    "\n",
    "    # Initializations\n",
    "    pred_matrices_list = []\n",
    "    gt_matrices_list = []\n",
    "    test_error = []\n",
    "    test_mae_padded = []\n",
    "    test_mae_unpadded = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for lr, hr in zip(test_adj, test_labels):\n",
    "            # check whether lr and hr are empty matrix\n",
    "            all_zeros_lr = not np.any(lr)\n",
    "            all_zeros_hr = not np.any(hr)\n",
    "\n",
    "            if all_zeros_lr == True or all_zeros_hr == True:\n",
    "                print(all_zeros_lr, all_zeros_hr)\n",
    "\n",
    "            if all_zeros_lr == False and all_zeros_hr == False:\n",
    "                # Pad hr from 268 to 320\n",
    "                lr = torch.from_numpy(lr).type(torch.FloatTensor)\n",
    "                np.fill_diagonal(hr, 1)\n",
    "                hr_padded = pad_HR_adj(hr, args.padding)\n",
    "                hr_padded = torch.from_numpy(hr_padded).type(torch.FloatTensor)\n",
    "\n",
    "                # Forward pass to make prediction\n",
    "                preds, _, _ = model(lr, args.lr_dim)\n",
    "\n",
    "                # Store errors\n",
    "                error = criterion(preds, hr_padded)\n",
    "                test_error.append(error.item())\n",
    "\n",
    "                # Store unpadded predictoins and ground truth for plots\n",
    "                preds_unpadded = unpad(preds, args.padding)\n",
    "                hr_unpadded = torch.from_numpy(hr).type(torch.FloatTensor)\n",
    "                pred_matrices_list.append(preds_unpadded.numpy())\n",
    "                gt_matrices_list.append(hr_unpadded.numpy())\n",
    "\n",
    "                # Use torch L1 norm to calculate MAE (padded & unpadded)\n",
    "                test_mae_padded.append(criterion_mae(preds, hr_padded).item())\n",
    "                test_mae_unpadded.append(criterion_mae(preds_unpadded, hr_unpadded).item())\n",
    "\n",
    "    pred_matrices = np.stack(pred_matrices_list, axis=0)\n",
    "    gt_matrices = np.stack(gt_matrices_list, axis=0)\n",
    "\n",
    "    print(\"Test error MSE: \", np.mean(test_error))\n",
    "    print(\"Test MAE (using L1Loss, padded): \", np.mean(test_mae_padded))\n",
    "    print(\"Test MAE (using L1Loss, unpadded): \", np.mean(test_mae_unpadded))\n",
    "    print(\"====================================================\")\n",
    "\n",
    "    return pred_matrices, gt_matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h5OXkgtJ4JNK"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "\n",
    "def calculate_measures(num_test_samples, num_roi, pred_matrices, gt_matrices):\n",
    "    \"\"\"\n",
    "    Calculate and evaluate various evaluation meatures.\n",
    "\n",
    "    Args:\n",
    "        num_test_samples (int): Number of samples in the test set.\n",
    "        num_roi (int): Number of regions of interest in the adjacency matrices.\n",
    "        pred_matrices (numpy.ndarray): Array containing predicted adjacency matrices for each test sample.\n",
    "        gt_matrices (numpy.ndarray): Array containing ground truth adjacency matrices for each test sample.\n",
    "\n",
    "    Returns:\n",
    "    - measures (list): List containing computed performance measures, including MAE, PCC, Jensen-Shannon Distance,\n",
    "                      and average MAE for betweenness centrality, eigenvector centrality, and PageRank centrality.\n",
    "    \"\"\"\n",
    "\n",
    "    # post-processing\n",
    "    pred_matrices[pred_matrices < 0] = 0\n",
    "    gt_matrices[gt_matrices < 0] = 0\n",
    "\n",
    "    # Initialize lists to store MAEs for each centrality measure\n",
    "    mae_bc = []\n",
    "    mae_ec = []\n",
    "    mae_pc = []\n",
    "\n",
    "    pred_1d_list = []\n",
    "    gt_1d_list = []\n",
    "\n",
    "    # Iterate over each test sample\n",
    "    for i in range(num_test_samples):\n",
    "        # Convert adjacency matrices to NetworkX graphs\n",
    "        pred_graph = nx.from_numpy_array(pred_matrices[i], edge_attr=\"weight\")\n",
    "        gt_graph = nx.from_numpy_array(gt_matrices[i], edge_attr=\"weight\")\n",
    "\n",
    "        # Compute centrality measures\n",
    "        pred_bc = nx.betweenness_centrality(pred_graph, weight=\"weight\")\n",
    "        pred_ec = nx.eigenvector_centrality(pred_graph, weight=\"weight\")\n",
    "        pred_pc = nx.pagerank(pred_graph, weight=\"weight\")\n",
    "\n",
    "        gt_bc = nx.betweenness_centrality(gt_graph, weight=\"weight\")\n",
    "        gt_ec = nx.eigenvector_centrality(gt_graph, weight=\"weight\")\n",
    "        gt_pc = nx.pagerank(gt_graph, weight=\"weight\")\n",
    "\n",
    "        # Convert centrality dictionaries to lists\n",
    "        pred_bc_values = list(pred_bc.values())\n",
    "        pred_ec_values = list(pred_ec.values())\n",
    "        pred_pc_values = list(pred_pc.values())\n",
    "\n",
    "        gt_bc_values = list(gt_bc.values())\n",
    "        gt_ec_values = list(gt_ec.values())\n",
    "        gt_pc_values = list(gt_pc.values())\n",
    "\n",
    "        # Compute MAEs\n",
    "        mae_bc.append(mean_absolute_error(pred_bc_values, gt_bc_values))\n",
    "        mae_ec.append(mean_absolute_error(pred_ec_values, gt_ec_values))\n",
    "        mae_pc.append(mean_absolute_error(pred_pc_values, gt_pc_values))\n",
    "\n",
    "        # Vectorize matrices\n",
    "        pred_1d_list.append(MatrixVectorizer.vectorize(pred_matrices[i]))\n",
    "        gt_1d_list.append(MatrixVectorizer.vectorize(gt_matrices[i]))\n",
    "\n",
    "    # Compute average MAEs\n",
    "    avg_mae_bc = sum(mae_bc) / len(mae_bc)\n",
    "    avg_mae_ec = sum(mae_ec) / len(mae_ec)\n",
    "    avg_mae_pc = sum(mae_pc) / len(mae_pc)\n",
    "\n",
    "    # Concatenate flattened matrices\n",
    "    pred_1d = np.concatenate(pred_1d_list)\n",
    "    gt_1d = np.concatenate(gt_1d_list)\n",
    "\n",
    "    # Compute metrics\n",
    "    mae = mean_absolute_error(pred_1d, gt_1d)\n",
    "    pcc = pearsonr(pred_1d, gt_1d)[0]\n",
    "    js_dis = jensenshannon(pred_1d, gt_1d)\n",
    "\n",
    "    print(\"MAE: \", mae)\n",
    "    print(\"PCC: \", pcc)\n",
    "    print(\"Jensen-Shannon Distance: \", js_dis)\n",
    "    print(\"Average MAE PageRank centrality:\", avg_mae_pc)\n",
    "    print(\"Average MAE eigenvector centrality:\", avg_mae_ec)\n",
    "    print(\"Average MAE betweenness centrality:\", avg_mae_bc)\n",
    "\n",
    "\n",
    "    return [mae, pcc, js_dis, avg_mae_pc, avg_mae_ec, avg_mae_bc]\n",
    "\n",
    "\n",
    "def plot_measures(measures = None, fold_index = None, avg = False, mean_values = None, std_dev_values = None):\n",
    "    \"\"\"\n",
    "    Plot and visualize evaluation measures(MAE', 'PCC', 'JSD', 'MAE(EC)', 'MAE(BC)') across folds or for a specific fold.\n",
    "\n",
    "    Args:\n",
    "        measures (list): List of numerical values representing evaluation measures for each category.\n",
    "        fold_index (int or None): Index of the fold if plotting for a specific fold, None for plotting across all folds.\n",
    "        avg (bool): Flag indicating whether to plot averages across folds or for a specific fold.\n",
    "        mean_values (list or None): List of mean values for each category across folds.\n",
    "        std_dev_values (list or None): List of standard deviation values for each category across folds.\n",
    "\n",
    "    Returns:\n",
    "        None: The function generates and saves the plot based on the specified parameters.\n",
    "    \"\"\"\n",
    "    # Initialization\n",
    "    categories = ['MAE', 'PCC', 'JSD', 'MAE(EC)', 'MAE(BC)']\n",
    "    colors = ['blue', 'green', 'red', 'purple', 'pink']\n",
    "\n",
    "    if avg:\n",
    "        # Plot across all folds\n",
    "        plt.figure(4)\n",
    "        mean_values = np.delete(mean_values, 3)\n",
    "        std_dev_values = np.delete(std_dev_values, 3)\n",
    "        plt.bar(range(len(mean_values)), mean_values, yerr=std_dev_values, capsize=5, color=colors)\n",
    "        plt.xticks(range(len(mean_values)), categories, rotation=45)\n",
    "        plt.title('Avg. Across Folds')\n",
    "        plt.tight_layout()\n",
    "         # Save plot\n",
    "        plt.savefig('Avg_Across_Folds_measures.png', dpi=300)\n",
    "        plt.close()\n",
    "    else:\n",
    "        # Plot for each fold\n",
    "        plt.figure(fold_index)\n",
    "        fold_measures = measures[:3] + measures[4:]\n",
    "        plt.bar(categories, fold_measures, color=colors)\n",
    "        plt.title(f'Fold {fold_index}')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        # Save plot\n",
    "        plt.savefig(f'Fold_{fold_index}_measures.png', dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "def plot_mae_pc(measures, mean_value = None, std_dev_value = None):\n",
    "    \"\"\"\n",
    "    Plot and visualize 'MAE(PC)' across folds and for a specific fold.\n",
    "\n",
    "    Args:\n",
    "        measures (list): List of 'MAE(PC)' scores.\n",
    "        mean_value (float): Mean value for MAE(PC).\n",
    "        std_dev_value (float): Standard deviation value for 'MAE(PC)'.\n",
    "\n",
    "    Returns:\n",
    "        None: The function generates and saves the plot based on the specified parameters.\n",
    "    \"\"\"\n",
    "    categories = ['Fold1', 'Fold2', 'Fold3', 'AcrossFolds']\n",
    "    bar_positions = np.arange(4)\n",
    "    plt.figure(5)\n",
    "    plt.bar(bar_positions[:3], measures , color='orange')\n",
    "    plt.bar(bar_positions[3], mean_value, yerr=std_dev_value, color='orange', capsize=5)\n",
    "    plt.title('MAE(PC) Scores')\n",
    "    plt.xticks(bar_positions, categories, rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'MAE(PC)_Scores.png', dpi=300)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F3JqIWaej1y9"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr, structural_similarity as ssim\n",
    "import community.community_louvain as community_louvain\n",
    "import os\n",
    "def calculate_centralities(adj_matrix):\n",
    "    if adj_matrix.shape[0] != adj_matrix.shape[1]:\n",
    "        raise ValueError(f\"Adjacency matrix is not square: shape={adj_matrix.shape}\")\n",
    "    print(f\"Processing adjacency matrix of shape: {adj_matrix.shape}\")\n",
    "\n",
    "    G = nx.from_numpy_array(adj_matrix)\n",
    "    partition = community_louvain.best_partition(G)\n",
    "\n",
    "    # Calculate the participation coefficient with the partition\n",
    "    pc_dict = participation_coefficient(G, partition)\n",
    "\n",
    "    # Calculate averages of centrality measures\n",
    "    pr = nx.pagerank(G, alpha=0.9)\n",
    "    ec = nx.eigenvector_centrality_numpy(G, max_iter=100)\n",
    "    bc = nx.betweenness_centrality(G, normalized=True, endpoints=False)\n",
    "    ns = np.array(list(nx.degree_centrality(G).values())) * (len(G.nodes()) - 1)\n",
    "    acc = nx.average_clustering(G, weight=None)\n",
    "\n",
    "    # Average participation coefficient\n",
    "    pc_avg = np.mean(list(pc_dict.values()))\n",
    "\n",
    "    return {\n",
    "        'pr': np.mean(list(pr.values())),\n",
    "        'ec': np.mean(list(ec.values())),\n",
    "        'bc': np.mean(list(bc.values())),\n",
    "        'ns': ns,\n",
    "        'pc': pc_avg,\n",
    "        'acc': acc\n",
    "    }\n",
    "\n",
    "def participation_coefficient(G, partition):\n",
    "    # Initialize dictionary for participation coefficients\n",
    "    pc_dict = {}\n",
    "\n",
    "    # Calculate participation coefficient for each node\n",
    "    for node in G.nodes():\n",
    "        node_degree = G.degree(node)\n",
    "        if node_degree == 0:\n",
    "            pc_dict[node] = 0.0\n",
    "        else:\n",
    "            # Count within-module connections\n",
    "            within_module_degree = sum(1 for neighbor in G[node] if partition[neighbor] == partition[node])\n",
    "            # Calculate participation coefficient\n",
    "            pc_dict[node] = 1 - (within_module_degree / node_degree) ** 2\n",
    "\n",
    "    return pc_dict\n",
    "\n",
    "def evaluate_all(true_hr_matrices, predicted_hr_matrices, output_path=path_eval_matrics):\n",
    "    print(true_hr_matrices.shape)\n",
    "    print(predicted_hr_matrices.shape)\n",
    "\n",
    "    num_subjects = true_hr_matrices.shape[0]\n",
    "    results = []\n",
    "\n",
    "    for i in range(num_subjects):\n",
    "        true_matrix = true_hr_matrices[i, :, :]\n",
    "        pred_matrix = predicted_hr_matrices[i, :, :]\n",
    "\n",
    "        print(f\"Evaluating subject {i+1} with matrix shapes: true={true_matrix.shape}, pred={pred_matrix.shape}\")\n",
    "\n",
    "        if true_matrix.shape != pred_matrix.shape or true_matrix.shape[0] != true_matrix.shape[1]:\n",
    "            print(f\"Error: Matrix shape mismatch or not square for subject {i+1}: true={true_matrix.shape}, pred={pred_matrix.shape}\")\n",
    "            continue\n",
    "\n",
    "        metrics = {\n",
    "            'ID': i + 1,\n",
    "            'MAE': mean_absolute_error(true_matrix.flatten(), pred_matrix.flatten()),\n",
    "            'PCC': pearsonr(true_matrix.flatten(), pred_matrix.flatten())[0],\n",
    "            'JSD': jensenshannon(true_matrix.flatten(), pred_matrix.flatten()),\n",
    "        }\n",
    "\n",
    "        true_metrics = calculate_centralities(true_matrix)\n",
    "        pred_metrics = calculate_centralities(pred_matrix)\n",
    "\n",
    "        for key in ['NS', 'PR', 'EC', 'BC', 'PC', 'ACC']:\n",
    "            metrics[f'MAE in {key}'] = mean_absolute_error([true_metrics[key.lower()]], [pred_metrics[key.lower()]])\n",
    "\n",
    "        results.append(metrics)\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    if not df.empty:\n",
    "        # Check if the file exists to decide whether to write headers\n",
    "        file_exists = os.path.isfile(output_path)\n",
    "\n",
    "        df.to_csv(output_path, mode='a', header=not file_exists, index=False)\n",
    "        print(f\"Results appended to {output_path}.\")\n",
    "    else:\n",
    "        print(\"No data to save.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IIGwAObr4JPR",
    "outputId": "6626ab69-06e3-480b-f165-382654552f9a"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import time\n",
    "import random\n",
    "from memory_profiler import profile\n",
    "import pandas as pd\n",
    "import scipy.io\n",
    "from scipy.io import loadmat\n",
    "\n",
    "# Set a fixed random seed for reproducibility across multiple libraries\n",
    "random_seed = 42\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"CUDA is available. Using GPU.\")\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed)  # For multi-GPU setups\n",
    "    # Additional settings for ensuring reproducibility on CUDA\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDA not available. Using CPU.\")\n",
    "\n",
    "\"\"\"\n",
    "Main utility files\n",
    "\"\"\"\n",
    "class Argument():\n",
    "    \"\"\"\n",
    "    Argument class to parse hyperparameters\n",
    "    Please change the args during hyperparameter tuning\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Argument, self).__init__()\n",
    "        self.epochs = 108\n",
    "        self.lr_d = 2e-4\n",
    "        self.lr_g = 4e-4\n",
    "        self.splits = 3\n",
    "        self.lmbda = 16\n",
    "        self.lr_dim = 160\n",
    "        self.hr_dim = 320\n",
    "        self.hidden_dim = 100\n",
    "        self.padding = 26\n",
    "        self.mean_dense = 0.0\n",
    "        self.std_dense = 0.01\n",
    "        self.mean_gaussian = 0.0\n",
    "        self.std_gaussian = 0.1\n",
    "        self.dropout = 0\n",
    "        self.ks = [0.9, 0.7, 0.6, 0.5]\n",
    "        self.data_path = '' # path of input csv files\n",
    "\n",
    "def convert_csv_to_mat(data_path = './'):\n",
    "    \"\"\"\n",
    "    Convert CSV files to MATLAB (.mat) format and save.\n",
    "\n",
    "    Args:\n",
    "        data_path (str): The path to the directory containing CSV files. (Default is current directory)\n",
    "    \"\"\"\n",
    "\n",
    "    if option == 0:\n",
    "        lr_train = pd.read_csv(path_lr_data_tr)\n",
    "        hr_train = pd.read_csv(path_hr_data_tr)\n",
    "\n",
    "        lr_valid = pd.read_csv(path_lr_data_valid) # ignore thios - this doesnt matter - it is for consistency\n",
    "        hr_valid = pd.read_csv(path_hr_data_valid) # ignore this - this doesnt matter  - it is for consistency\n",
    "\n",
    "        lr_test = pd.read_csv(path_lr_data_valid)\n",
    "        hr_test = pd.read_csv(path_hr_data_valid)\n",
    "\n",
    "\n",
    "    if option == 1:\n",
    "        lr_train = pd.read_csv(path_lr_data)\n",
    "        hr_train = pd.read_csv(path_hr_data)\n",
    "\n",
    "        lr_valid = pd.read_csv(path_lr_data_valid) # IGNORE this doesnt matter  - it is for consistency\n",
    "        hr_valid = pd.read_csv(path_hr_data_valid) # IGNORE this doesnt matter  - it is for consistency\n",
    "\n",
    "        lr_test = pd.read_csv(path_lr_data_test)\n",
    "        hr_test = pd.read_csv(path_hr_data_test)\n",
    "\n",
    "    scipy.io.savemat(data_path + 'lr_train.mat', {'data': lr_train})\n",
    "    scipy.io.savemat(data_path + 'hr_train.mat', {'data': hr_train})\n",
    "    scipy.io.savemat(data_path + 'lr_valid.mat', {'data': lr_valid})\n",
    "    scipy.io.savemat(data_path + 'hr_valid.mat', {'data': hr_valid})\n",
    "\n",
    "    scipy.io.savemat(data_path + 'lr_test.mat', {'data': lr_test})\n",
    "    scipy.io.savemat(data_path + 'hr_test.mat', {'data': hr_test})\n",
    "\n",
    "\n",
    "def load_data(data_path = './'):\n",
    "    \"\"\"\n",
    "    Load data from CSV files converted to MATLAB (.mat) format.\n",
    "\n",
    "    Args:\n",
    "        data_path (str): The path to the directory containing the .mat files. (Default is current directory)\n",
    "\n",
    "    Returns:\n",
    "        lr_train_vector (pd.DataFrame): Vector containing data from 'lr_train.mat'.\n",
    "        hr_train_vector (pd.DataFrame): Vector containing data from 'hr_train.mat'.\n",
    "        lr_test_vector (pd.DataFrame): Vector containing data from 'lr_test.mat'.\n",
    "    \"\"\"\n",
    "    convert_csv_to_mat(data_path) # load input csv files and store in .mat format\n",
    "\n",
    "    # Load .mat into pd\n",
    "    lr_train_data = loadmat(data_path + 'lr_train.mat')\n",
    "    hr_train_data = loadmat(data_path + 'hr_train.mat')\n",
    "\n",
    "    lr_valid_data = loadmat(data_path + 'lr_valid.mat')\n",
    "    hr_valid_data = loadmat(data_path + 'hr_valid.mat')\n",
    "\n",
    "    lr_test_data = loadmat(data_path + 'lr_test.mat')\n",
    "    hr_test_data = loadmat(data_path + 'hr_test.mat')\n",
    "\n",
    "\n",
    "    lr_train_vector = lr_train_data['data']\n",
    "    hr_train_vector = hr_train_data['data']\n",
    "    lr_valid_vector = lr_valid_data['data']\n",
    "    hr_valid_vector = hr_valid_data['data']\n",
    "\n",
    "    lr_test_vector = lr_test_data['data']\n",
    "    hr_test_vector = hr_test_data['data']\n",
    "\n",
    "    return lr_train_vector, hr_train_vector, lr_valid_vector, hr_valid_vector, lr_test_vector, hr_test_vector\n",
    "\n",
    "def construct_dataset(lr_train_vector, hr_train_vector, lr_valid_vector, hr_valid_vector, lr_test_vector, hr_test_vector):\n",
    "    \"\"\"\n",
    "    Construct numpy dataset from vectorized adjacency matrices.\n",
    "\n",
    "    Args:\n",
    "        lr_train_vector (pd.DataFrame): LR adjacency matrices for training.\n",
    "        hr_train_vector (pd.DataFrame): HR adjacency matrices for training.\n",
    "        lr_test_vector (pd.DataFrame): LR adjacency matrices for testing.\n",
    "\n",
    "    Returns:\n",
    "        lr_train_matrix_all_np (numpy.ndarray): Numpy array of size (n, 160, 160) containing anti-vectorized LR adjacency matrices for training.\n",
    "        hr_train_matrix_all_np (numpy.ndarray): Numpy array of size (n, 268, 268) containing anti-vectorized HR adjacency matrices for training.\n",
    "        lr_test_matrix_all_np (numpy.ndarray): Numpy array of size (n, 160, 160) containing anti-vectorized LR adjacency matrices for testing.\n",
    "    \"\"\"\n",
    "    # Lists to store all anti-vectorized graphs\n",
    "    lr_train_matrix_all = []\n",
    "    hr_train_matrix_all = []\n",
    "    lr_valid_matrix_all = []\n",
    "    hr_valid_matrix_all = []\n",
    "\n",
    "    lr_test_matrix_all = []\n",
    "    hr_test_matrix_all = []\n",
    "\n",
    "\n",
    "    # Anti-vectorize the adj matrix and append to a list\n",
    "    for i in range(lr_train_vector.shape[0]):\n",
    "        lr_train_matrix_all.append(MatrixVectorizer.anti_vectorize(lr_train_vector[i,:], 160, include_diagonal=False))\n",
    "        hr_train_matrix_all.append(MatrixVectorizer.anti_vectorize(hr_train_vector[i,:], 268, include_diagonal=False))\n",
    "\n",
    "    for i in range(lr_valid_vector.shape[0]):\n",
    "        lr_valid_matrix_all.append(MatrixVectorizer.anti_vectorize(lr_valid_vector[i,:], 160, include_diagonal=False))\n",
    "        hr_valid_matrix_all.append(MatrixVectorizer.anti_vectorize(hr_valid_vector[i,:], 268, include_diagonal=False))\n",
    "\n",
    "    for i in range(lr_test_vector.shape[0]):\n",
    "        lr_test_matrix_all.append(MatrixVectorizer.anti_vectorize(lr_test_vector[i,:], 160, include_diagonal=False))\n",
    "        hr_test_matrix_all.append(MatrixVectorizer.anti_vectorize(hr_test_vector[i,:], 268, include_diagonal=False))\n",
    "\n",
    "    # Convert list to Numpy\n",
    "    lr_train_matrix_all_np = np.array(lr_train_matrix_all)\n",
    "    hr_train_matrix_all_np = np.array(hr_train_matrix_all)\n",
    "\n",
    "    lr_valid_matrix_all_np = np.array(lr_valid_matrix_all)\n",
    "    hr_valid_matrix_all_np = np.array(hr_valid_matrix_all)\n",
    "\n",
    "    lr_test_matrix_all_np = np.array(lr_test_matrix_all)\n",
    "    hr_test_matrix_all_np = np.array(hr_test_matrix_all)\n",
    "\n",
    "    return lr_train_matrix_all_np, hr_train_matrix_all_np, lr_valid_matrix_all_np, hr_valid_matrix_all_np, lr_test_matrix_all_np, hr_test_matrix_all_np\n",
    "\n",
    "def cross_validation_3folds(args, train_adj, train_labels, valid_adj, valid_labels, cv):\n",
    "    \"\"\"\n",
    "    Perform 3-fold cross-validation on the given data.\n",
    "\n",
    "    Args:\n",
    "        args (object): Input arguments.\n",
    "        train_adj (numpy.ndarray): Array of adjacency matrices for training.\n",
    "        train_labels (numpy.ndarray): Array of labels for training.\n",
    "        cv (object): Cross-validation generator.\n",
    "\n",
    "    Returns:\n",
    "        measures_folds (list): Evaluation measures for each fold.\n",
    "    \"\"\"\n",
    "    fold_index = 0\n",
    "    measures_folds = []\n",
    "\n",
    "    model = IWASAGSRNet(args)\n",
    "    subjects_adj, val_adj, subjects_ground_truth, val_ground_truth = train_adj, valid_adj, train_labels, valid_labels\n",
    "    train(model, subjects_adj, subjects_ground_truth, args)\n",
    "\n",
    "    pred_matrices, gt_matrices = test(model, val_adj, val_ground_truth, args)\n",
    "\n",
    "\n",
    "    # Save prediction results for every fold\n",
    "    fold_outputs = pred_matrices.reshape(-1)\n",
    "    print(\"========== Predictions ARE MADE! ==========\")\n",
    "\n",
    "    return fold_outputs\n",
    "\n",
    "def predict_test(test_adj, args, model):\n",
    "    \"\"\"\n",
    "    Predict the output for test adjacency matrices using the given model.\n",
    "\n",
    "    Args:\n",
    "        test_adj (numpy.ndarray): Array of test adjacency matrices.\n",
    "        args (object): Input arguments.\n",
    "        model (object): Model for prediction.\n",
    "\n",
    "    Returns:\n",
    "        total_outputs (numpy.ndarray): Predicted outputs for test adjacency matrices.\n",
    "    \"\"\"\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    output_list = []\n",
    "\n",
    "    # Forward pass for prediction\n",
    "    with torch.no_grad():\n",
    "        for lr in test_adj:\n",
    "            lr = torch.from_numpy(lr).type(torch.FloatTensor)\n",
    "            outputs, _, _ = model(lr, args.lr_dim)\n",
    "            outputs = unpad(outputs, args.padding)\n",
    "            output_list.append(outputs)\n",
    "        preds_list = np.asarray(output_list)\n",
    "    return preds_list\n",
    "\n",
    "\n",
    "def save_predictions(total_outputs, fold_num):\n",
    "    \"\"\"\n",
    "    Save predictions into a CSV file with the required format.\n",
    "\n",
    "    Args:\n",
    "        total_outputs (numpy.ndarray): Predicted outputs.\n",
    "        fold_num (int): Fold number for naming the CSV file.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a DataFrame with ID and Predicted columns\n",
    "    df = pd.DataFrame({'ID': range(1, len(total_outputs)+1), 'Predicted': total_outputs})\n",
    "    # Save DataFrame to a CSV file\n",
    "    df.to_csv(f'predictions_fold_{fold_num}.csv', index=False)\n",
    "\n",
    "def main():\n",
    "    args = Argument()\n",
    "\n",
    "    print(\"Please Specify the Data Path of the Input CSV Files! (currently using the default path under ./data)\")\n",
    "    print(\"======== Loading Data... ========\")\n",
    "    lr_train_vector, hr_train_vector, lr_valid_vector, hr_valid_vector, lr_test_vector, hr_test_vector = load_data(args.data_path)\n",
    "    train_adj, train_labels, valid_adj, valid_labels, test_adj, test_labels = construct_dataset(lr_train_vector, hr_train_vector, lr_valid_vector, hr_valid_vector, lr_test_vector, hr_test_vector)\n",
    "    print(\"======== Data Loaded! ========\")\n",
    "\n",
    "    print(\"========== Start Fold Cross Validation ==========\")\n",
    "    #cv = KFold(n_splits=args.splits, shuffle=True, random_state = random_seed)\n",
    "    cv = None\n",
    "    start_time = time.time() # timer for cv run time\n",
    "    #measures_folds = cross_validation_3folds(args, train_adj, train_labels, valid_adj, valid_labels, cv)\n",
    "    #end_time = time.time()\n",
    "    #elapsed_time = end_time - start_time\n",
    "    #print(f\"Total training time for 3F-CV: {elapsed_time} seconds\")\n",
    "\n",
    "    # Plot measures across all folds\n",
    "    #mean_values = np.mean(measures_folds, axis=0)\n",
    "    #std_dev_values = np.std(measures_folds, axis=0)\n",
    "    #plot_measures(avg = True, mean_values = mean_values, std_dev_values = std_dev_values)\n",
    "\n",
    "    # Plot MAE(pc) measure\n",
    "    #mae_pc_folds = [measures[3] for measures in measures_folds]\n",
    "    #plot_mae_pc(mae_pc_folds, mean_value = mean_values[3], std_dev_value = std_dev_values[3])\n",
    "\n",
    "    print(\"========== Generate Final Model with All Training Samples ==========\")\n",
    "    final_model = IWASAGSRNet(args)\n",
    "    train(final_model, train_adj, train_labels, args)\n",
    "    final_output = predict_test(test_adj, args, final_model)\n",
    "    #save_predictions(final_output, 'all')\n",
    "\n",
    "    metrics = evaluate_all(\n",
    "    test_labels, final_output\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
