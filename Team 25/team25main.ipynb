{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a9f6163",
   "metadata": {
    "id": "7a9f6163"
   },
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "JA6E1ghH5V1y",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JA6E1ghH5V1y",
    "outputId": "2f870966-1859-490c-e027-8e081bb1999e"
   },
   "outputs": [],
   "source": [
    "# we worked with google collab so this is the needed setup to run the project\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')\n",
    "\n",
    "my_project_folder = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "HqVHA11041c3",
   "metadata": {
    "id": "HqVHA11041c3"
   },
   "source": [
    "# **Set the file paths and notes**\n",
    "\n",
    "<p>Use option = 0 if you want to do training and validation using 2 of the folds (keeping the 3rd fold for final training and testing).</p>\n",
    "\n",
    "<p>Use option = 1 if you want to do final training (using 2 folds) and testing (with the 3rd fold).</p>\n",
    "\n",
    "Be sure to update the name of the files - doing 3 cross-validation in this project is done manually with the prepared data to create comparable results with the rest of the projects. See an example below.\n",
    "\n",
    "Due to the nature of this team's model, early stopping is carried out by manual experimentation during testing and validation for 3CV. To help reach a decision on the best number of epochs for training - refer to the tables with the loss of the generator and discriminator.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Vkb9yM4T40st",
   "metadata": {
    "id": "Vkb9yM4T40st"
   },
   "outputs": [],
   "source": [
    "option = 1\n",
    "\n",
    "# path - !!! Comment out the option you don't need. Currently it will proceed with latter option !!!\n",
    "# path for training and validation (without using the 3rd fold). This is an example from the ClusterCV folder\n",
    "path_lr_data_tr = 'lr_split_AandB_training.csv'\n",
    "path_hr_data_tr = 'hr_split_AandB_training.csv'\n",
    "path_lr_data_valid = 'lr_split_AandB_validation.csv'\n",
    "path_hr_data_valid = 'hr_split_AandB_validation.csv'\n",
    "\n",
    "# path for final training and testing (using fold A and B for training and C for testing).This is an example from the ClusterCV folder\n",
    "path_lr_data = 'lr_split_AandB_finaltraining.csv'\n",
    "path_hr_data = 'hr_split_AandB_finaltraining.csv'\n",
    "path_lr_data_test = 'lr_clusterC.csv'\n",
    "path_hr_data_test = 'hr_clusterC.csv'\n",
    "\n",
    "# path to save the evaluation metrics to - this means that Cluster C will be used for testing\n",
    "path_eval_matrics = '25-clusterCV-split3.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1ccd80",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5a1ccd80",
    "outputId": "c6e9dee8-80bd-44af-8c65-00bba33bfada"
   },
   "outputs": [],
   "source": [
    "!pip install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2888fe54",
   "metadata": {
    "id": "2888fe54"
   },
   "outputs": [],
   "source": [
    "# Standard scientific computing and data visualization libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Machine learning and deep learning libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Network analysis and graph libraries\n",
    "import networkx as nx\n",
    "\n",
    "# Statistical metrics and distance calculations\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "\n",
    "# Additional utilities\n",
    "import random\n",
    "import csv\n",
    "import scipy.io\n",
    "from scipy.io import loadmat\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d69369c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7d69369c",
    "outputId": "55bd1947-b9eb-4705-ccb8-2dd2510b0fb8"
   },
   "outputs": [],
   "source": [
    "# Set a fixed random seed for reproducibility across multiple libraries\n",
    "random_seed = 42\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "# Check for CUDA (GPU support) and set device accordingly\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"CUDA is available. Using GPU.\")\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed)  # For multi-GPU setups\n",
    "    # Additional settings for ensuring reproducibility on CUDA\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDA not available. Using CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-SXh5ZnZof9_",
   "metadata": {
    "id": "-SXh5ZnZof9_"
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67daca6a",
   "metadata": {
    "id": "67daca6a"
   },
   "source": [
    "## Matrix Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77602236",
   "metadata": {
    "id": "77602236"
   },
   "outputs": [],
   "source": [
    "class MatrixVectorizer:\n",
    "    \"\"\"\n",
    "    A class for transforming between matrices and vector representations.\n",
    "\n",
    "    This class provides methods to convert a symmetric matrix into a vector (vectorize)\n",
    "    and to reconstruct the matrix from its vector form (anti_vectorize), focusing on\n",
    "    vertical (column-based) traversal and handling of elements.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes the MatrixVectorizer instance.\n",
    "\n",
    "        The constructor currently does not perform any actions but is included for\n",
    "        potential future extensions where initialization parameters might be required.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def vectorize(matrix, include_diagonal=False):\n",
    "        \"\"\"\n",
    "        Converts a matrix into a vector by vertically extracting elements.\n",
    "\n",
    "        This method traverses the matrix column by column, collecting elements from the\n",
    "        upper triangle, and optionally includes the diagonal elements immediately below\n",
    "        the main diagonal based on the include_diagonal flag.\n",
    "\n",
    "        Parameters:\n",
    "        - matrix (numpy.ndarray): The matrix to be vectorized.\n",
    "        - include_diagonal (bool, optional): Flag to include diagonal elements in the vectorization.\n",
    "          Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "        - numpy.ndarray: The vectorized form of the matrix.\n",
    "        \"\"\"\n",
    "        # Determine the size of the matrix based on its first dimension\n",
    "        matrix_size = matrix.shape[0]\n",
    "\n",
    "        # Initialize an empty list to accumulate vector elements\n",
    "        vector_elements = []\n",
    "\n",
    "        # Iterate over columns and then rows to collect the relevant elements\n",
    "        for col in range(matrix_size):\n",
    "            for row in range(matrix_size):\n",
    "                # Skip diagonal elements if not including them\n",
    "                if row != col:\n",
    "                    if row < col:\n",
    "                        # Collect upper triangle elements\n",
    "                        vector_elements.append(matrix[row, col])\n",
    "                    elif include_diagonal and row == col + 1:\n",
    "                        # Optionally include the diagonal elements immediately below the diagonal\n",
    "                        vector_elements.append(matrix[row, col])\n",
    "\n",
    "        return np.array(vector_elements)\n",
    "\n",
    "    @staticmethod\n",
    "    def anti_vectorize(vector, matrix_size, include_diagonal=False):\n",
    "        \"\"\"\n",
    "        Reconstructs a matrix from its vector form, filling it vertically.\n",
    "\n",
    "        The method fills the matrix by reflecting vector elements into the upper triangle\n",
    "        and optionally including the diagonal elements based on the include_diagonal flag.\n",
    "\n",
    "        Parameters:\n",
    "        - vector (numpy.ndarray): The vector to be transformed into a matrix.\n",
    "        - matrix_size (int): The size of the square matrix to be reconstructed.\n",
    "        - include_diagonal (bool, optional): Flag to include diagonal elements in the reconstruction.\n",
    "          Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "        - numpy.ndarray: The reconstructed square matrix.\n",
    "        \"\"\"\n",
    "        # Initialize a square matrix of zeros with the specified size\n",
    "        matrix = np.zeros((matrix_size, matrix_size))\n",
    "\n",
    "        # Index to keep track of the current position in the vector\n",
    "        vector_idx = 0\n",
    "\n",
    "        # Fill the matrix by iterating over columns and then rows\n",
    "        for col in range(matrix_size):\n",
    "            for row in range(matrix_size):\n",
    "                # Skip diagonal elements if not including them\n",
    "                if row != col:\n",
    "                    if row < col:\n",
    "                        # Reflect vector elements into the upper triangle and its mirror in the lower triangle\n",
    "                        matrix[row, col] = vector[vector_idx]\n",
    "                        matrix[col, row] = vector[vector_idx]\n",
    "                        vector_idx += 1\n",
    "                    elif include_diagonal and row == col + 1:\n",
    "                        # Optionally fill the diagonal elements after completing each column\n",
    "                        matrix[row, col] = vector[vector_idx]\n",
    "                        matrix[col, row] = vector[vector_idx]\n",
    "                        vector_idx += 1\n",
    "\n",
    "        return matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc7e9a5",
   "metadata": {
    "id": "dfc7e9a5"
   },
   "source": [
    "## Turn .csv to .mat Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72cd1bd6",
   "metadata": {
    "id": "72cd1bd6"
   },
   "outputs": [],
   "source": [
    "# Modified from https://stackoverflow.com/questions/20818121/save-csv-to-mat-or-binary\n",
    "train = []\n",
    "test = []\n",
    "hr_train = []\n",
    "\n",
    "with open(path_lr_data) as f:\n",
    "    next(f)\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        rowData = [ float(elem) for elem in row ]\n",
    "        train.append(rowData)\n",
    "\n",
    "with open(path_lr_data_test) as f:\n",
    "    next(f)\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        rowData = [ float(elem) for elem in row ]\n",
    "        test.append(rowData)\n",
    "\n",
    "with open(path_hr_data) as f:\n",
    "    next(f)\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        rowData = [ float(elem) for elem in row ]\n",
    "        hr_train.append(rowData)\n",
    "lentrain = len(train)\n",
    "lentest = len(test)\n",
    "\n",
    "train = np.array(train)\n",
    "test = np.array(test)\n",
    "hr_train = np.array(hr_train)\n",
    "scipy.io.savemat('LR_data_160.mat', {'train':train,'test':test})\n",
    "scipy.io.savemat('HR_data_268.mat', {'train':hr_train})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ybHBNTF4YmEo",
   "metadata": {
    "id": "ybHBNTF4YmEo"
   },
   "source": [
    "## Loading data from Mat files (run it!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cWy0eHRYk9X",
   "metadata": {
    "id": "2cWy0eHRYk9X"
   },
   "outputs": [],
   "source": [
    "# Path to your .mat files\n",
    "\n",
    "lr_data_path = './LR_data_160.mat'\n",
    "hr_data_path = './HR_data_268.mat'\n",
    "\n",
    "# Load the data\n",
    "lr_data = loadmat(my_project_folder +lr_data_path)\n",
    "hr_data = loadmat(my_project_folder +hr_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5314c0d",
   "metadata": {
    "id": "c5314c0d"
   },
   "source": [
    "## Preprocessing (no need to run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd41f76",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "efd41f76",
    "outputId": "f8bbbef5-a6d7-4089-d29e-3c228ed6637e"
   },
   "outputs": [],
   "source": [
    "# Print the keys to see what variables are inside\n",
    "print(\"LR Data Keys:\", lr_data.keys())\n",
    "print(\"HR Data Keys:\", hr_data.keys())\n",
    "print()\n",
    "\n",
    "# Print basic information about 'LR' and 'HR' variables\n",
    "print(\"LR Data Type:\", type(lr_data['train']))\n",
    "print(\"LR Data Shape:\", lr_data['train'].shape)\n",
    "print()\n",
    "\n",
    "print(\"LR Data Type:\", type(lr_data['test']))\n",
    "print(\"LR Data Shape:\", lr_data['test'].shape)\n",
    "print()\n",
    "\n",
    "print(\"HR Data Type:\", type(hr_data['train']))\n",
    "print(\"HR Data Shape:\", hr_data['train'].shape)\n",
    "print()\n",
    "# If the data types are numpy arrays and not too large, you can print a small part of them\n",
    "# Adjust the slicing as needed to avoid printing too much data\n",
    "print(\"Sample from LR training Data:\\n\", lr_data['train'][:5])  # Adjust the index range as needed\n",
    "print()\n",
    "print(\"Sample from HR training Data:\\n\", hr_data['train'][:5])  # Adjust the index range as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3405f7c0",
   "metadata": {
    "id": "3405f7c0"
   },
   "outputs": [],
   "source": [
    "lr_array = np.concatenate((lr_data['train'], lr_data['test']), axis=0)\n",
    "hr_array = hr_data['train']\n",
    "\n",
    "# Replace all negative values with 0\n",
    "lr_array[lr_array < 0] = 0\n",
    "hr_array[hr_array < 0] = 0\n",
    "\n",
    "# Replace any 'NaN' values with 0\n",
    "np.nan_to_num(lr_array, copy=False)\n",
    "np.nan_to_num(hr_array, copy=False)\n",
    "\n",
    "assert np.min(lr_array) == 0.\n",
    "assert np.min(hr_array) == 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ebc38d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "32ebc38d",
    "outputId": "db75d2bf-7c66-49c9-bafa-4d8ccff823ae"
   },
   "outputs": [],
   "source": [
    "# Define a function to calculate statistics and return them in a dictionary\n",
    "def calculate_statistics(data):\n",
    "    statistics = {\n",
    "        'Mean': np.mean(data),\n",
    "        'Median': np.median(data),\n",
    "        'Standard Deviation': np.std(data),\n",
    "        'Min': np.min(data),\n",
    "        'Max': np.max(data)\n",
    "    }\n",
    "    return statistics\n",
    "\n",
    "# Calculate statistics for LR and HR data\n",
    "lr_stats = calculate_statistics(lr_array)\n",
    "hr_stats = calculate_statistics(hr_array)\n",
    "\n",
    "# Create a DataFrame to hold the statistics for comparison\n",
    "df_stats = pd.DataFrame({'LR Data': lr_stats, 'HR Data': hr_stats})\n",
    "\n",
    "# Round the numbers to four decimal places for better readability\n",
    "df_stats = df_stats.round(4)\n",
    "\n",
    "df_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c78963a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 573
    },
    "id": "3c78963a",
    "outputId": "11a1fead-bf22-4806-a677-38fb2ec7831d"
   },
   "outputs": [],
   "source": [
    "# Setting the Seaborn theme for nice aesthetics\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# Plotting histograms on the same figure for comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Making histograms semi-transparent with alpha and using a higher bin count for finer detail\n",
    "sns.histplot(lr_array.flatten(), bins=100, color='blue', alpha=0.5, label='LR Data')\n",
    "sns.histplot(hr_array.flatten(), bins=100, color='red', alpha=0.5, label='HR Data')\n",
    "\n",
    "# Adding titles and labels\n",
    "plt.title('Combined Distribution of LR and HR Data')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Adding a legend to differentiate between LR and HR data\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0686befc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 573
    },
    "id": "0686befc",
    "outputId": "50e198b8-eb1e-4d0f-fee3-b81d142986bb"
   },
   "outputs": [],
   "source": [
    "# Setting the Seaborn theme for aesthetics\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# Plotting histograms on the same figure for comparison, excluding zeros\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Making histograms semi-transparent with alpha and using a higher bin count for finer detail\n",
    "# We filter out the zeros using lr_array[lr_array > 0].flatten() and hr_array[hr_array > 0].flatten()\n",
    "sns.histplot(lr_array[lr_array > 0].flatten(), bins=100, color='blue', alpha=0.5, label='LR Data')\n",
    "sns.histplot(hr_array[hr_array > 0].flatten(), bins=100, color='red', alpha=0.5, label='HR Data')\n",
    "\n",
    "# Adding titles and labels\n",
    "plt.title('Combined Distribution of LR and HR Data (Excluding Zeros)')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Adding a legend to differentiate between LR and HR data\n",
    "plt.legend()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e6fe13",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 703
    },
    "id": "d9e6fe13",
    "outputId": "2de4adbd-5c24-4015-81e2-36278539a6f4"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 8))\n",
    "\n",
    "# Heatmap of a subset of LR data\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(lr_array, aspect='auto', cmap='viridis')  # Adjust subset size as needed\n",
    "plt.colorbar()\n",
    "plt.title('LR Data Heatmap')\n",
    "\n",
    "# Heatmap of a subset of HR data\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(hr_array, aspect='auto', cmap='viridis')  # Adjust subset size as needed\n",
    "plt.colorbar()\n",
    "plt.title('HR Data Heatmap')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e90fac",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 661
    },
    "id": "a2e90fac",
    "outputId": "04df3c0c-0a61-465e-ea27-5e080a3b3b4a"
   },
   "outputs": [],
   "source": [
    "# Choose which matrix to visualise\n",
    "i = 18\n",
    "\n",
    "# Now, we use the function for LR and HR data with the right flags\n",
    "lr_vector = lr_array[i, :]\n",
    "hr_vector = hr_array[i, :]\n",
    "\n",
    "# Now, we use the function for LR and HR data with the right flags\n",
    "lr_matrix = MatrixVectorizer.anti_vectorize(lr_vector, 160, include_diagonal=False)\n",
    "hr_matrix = MatrixVectorizer.anti_vectorize(hr_vector, 268, include_diagonal=False)\n",
    "\n",
    "# Plot the heatmaps\n",
    "plt.figure(figsize=(18, 8))\n",
    "\n",
    "# LR Data Heatmap\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.heatmap(lr_matrix, square=True, cmap='viridis')\n",
    "plt.title('LR Data Heatmap (160x160)')\n",
    "\n",
    "# HR Data Heatmap\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.heatmap(hr_matrix, square=True, cmap='viridis')\n",
    "plt.title('HR Data Heatmap (268x268)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5122a97e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5122a97e",
    "outputId": "badd8030-f675-4bff-ee4e-d1b11d8fc408"
   },
   "outputs": [],
   "source": [
    "# Toy vector for demonstration\n",
    "toy_vector = np.array([1, 2, 3, 4, 5, 6])  # This is a vectorized upper triangular part of a 4x4 matrix\n",
    "\n",
    "# The matrix_size is 4 for our toy example\n",
    "matrix_size = 4\n",
    "\n",
    "# We call the anti_vectorize function with include_diagonal=False since we are not including the diagonal\n",
    "toy_matrix = MatrixVectorizer.anti_vectorize(toy_vector, matrix_size, include_diagonal=False)\n",
    "\n",
    "toy_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701db3c6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "701db3c6",
    "outputId": "a00ae1fe-c6d7-4ba6-c93a-9fca8a1c9750"
   },
   "outputs": [],
   "source": [
    "vectorized_matrix = MatrixVectorizer.vectorize(toy_matrix, include_diagonal=False)\n",
    "\n",
    "vectorized_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p-Ot7boOC4Ii",
   "metadata": {
    "id": "p-Ot7boOC4Ii"
   },
   "source": [
    "## Anti-Vectorize the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "asG-IWXxDqk7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "asG-IWXxDqk7",
    "outputId": "72031767-6a88-446b-8c02-30a10acd7609"
   },
   "outputs": [],
   "source": [
    "# Anti vectorize the data\n",
    "\n",
    "X = np.zeros((lentrain,160,160))\n",
    "Y = np.zeros((lentrain,268,268))\n",
    "X_test=np.zeros((lentest,160,160))\n",
    "\n",
    "\n",
    "for i in range(lentrain):\n",
    "  X[i] = MatrixVectorizer.anti_vectorize(lr_data['train'][i], 160, include_diagonal=False)\n",
    "  Y[i] = MatrixVectorizer.anti_vectorize(hr_data['train'][i], 268, include_diagonal=False)\n",
    "\n",
    "for i in range(lentest):\n",
    "  X_test[i]=MatrixVectorizer.anti_vectorize(lr_data['test'][i], 160, include_diagonal=False)\n",
    "\n",
    "\n",
    "# Plot the heatmaps\n",
    "plt.figure(figsize=(18, 8))\n",
    "\n",
    "# LR Data Heatmap\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.heatmap(X[12], square=True, cmap='viridis')\n",
    "plt.title('LR Data Heatmap (160x160)')\n",
    "\n",
    "# HR Data Heatmap\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.heatmap(Y[12], square=True, cmap='viridis')\n",
    "plt.title('HR Data Heatmap (268x268)')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cTE_ioJ4pMZe",
   "metadata": {
    "id": "cTE_ioJ4pMZe"
   },
   "source": [
    "# Plotting, Model Class Definitions, Training and Testing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "z2dXQeJGlweT",
   "metadata": {
    "id": "z2dXQeJGlweT"
   },
   "source": [
    "## Plotting and Writing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "z-4zLQxIlzPn",
   "metadata": {
    "id": "z-4zLQxIlzPn"
   },
   "outputs": [],
   "source": [
    "def make_one_plot(mae, pcc, js_dis, avg_mae_pc, avg_mae_ec, avg_mae_bc,title):\n",
    "    \"\"\"\n",
    "    Create a barplot visualizing various error metrics for model evaluation.\n",
    "\n",
    "    Parameters:\n",
    "    - mae (float): Mean Absolute Error of the predicted matrices compared to ground truth.\n",
    "    - pcc (float): Pearson Correlation Coefficient between the vectorized predicted and ground truth matrices.\n",
    "    - js_dis (float): Jensen-Shannon Distance between vectorized predicted and ground truth matrices.\n",
    "    - avg_mae_pc (float): Average Mean Absolute Error for PageRank Centrality.\n",
    "    - avg_mae_ec (float): Average Mean Absolute Error for Eigenvector Centrality.\n",
    "    - avg_mae_bc (float): Average Mean Absolute Error for Betweenness Centrality.\n",
    "    - title (str): Title for the plot.\n",
    "\n",
    "    Returns:\n",
    "    - None: Displays the barplot.\n",
    "    \"\"\"\n",
    "\n",
    "    labels = ['MAE','PCC','JSD','MAE (PC)', 'MAE (EC)', 'MAE (BC)']\n",
    "    data = [mae, pcc, js_dis, avg_mae_pc, avg_mae_ec, avg_mae_bc]\n",
    "    colors = ['#F46867', '#70B163', '#6A67FF', '#F9C963', '#83FEFF', '#80FE5F']\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.title(title)\n",
    "\n",
    "    plt.bar(x=labels, height=data, color=colors)\n",
    "    plt.ylabel('Error')\n",
    "    plt.show()\n",
    "\n",
    "def plot_gan_loss(g_loss, d_loss, mae_loss, mae_loss_val=None):\n",
    "    \"\"\"\n",
    "    Plot the Generator, Discriminator, and Mean Absolute Error (MAE) losses\n",
    "    (training and validation) during training.\n",
    "\n",
    "    Parameters:\n",
    "    - g_loss (list): List of Generator loss values across epochs.\n",
    "    - d_loss (list): List of Discriminator loss values across epochs.\n",
    "    - mae_loss (list): List of Mean Absolute Error (MAE) values across epochs for the training data.\n",
    "    - mae_loss_val (list) :List of Mean Absolute Error (MAE) values across epochs for the validation data.\n",
    "\n",
    "    Returns:\n",
    "    - None: Displays the separate loss plots.\n",
    "    \"\"\"\n",
    "\n",
    "    # Plot Generator and Discriminator loss\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.title(\"Generator and Discriminator Loss During Training\")\n",
    "    plt.plot(g_loss, label=\"Generator\")\n",
    "    plt.plot(d_loss, label=\"Discriminator\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "\n",
    "    if mae_loss_val is not None:\n",
    "      # Plot Mean Absolute Error (MAE) loss of both training and testing throughout training\n",
    "      plt.subplot(1, 2, 2)\n",
    "      plt.title(\"Mean Absolute Error (MAE) Loss During Training\")\n",
    "      plt.plot(mae_loss, label=\"Training MAE\",color='blue')\n",
    "      plt.plot(mae_loss_val, label=\"Validation MAE\",color='purple')\n",
    "\n",
    "      plt.xlabel(\"Epoch\")\n",
    "      plt.ylabel(\"Loss\")\n",
    "      plt.legend()\n",
    "\n",
    "    else:\n",
    "      # Plot Mean Absolute Error (MAE) loss without Validation\n",
    "      plt.subplot(1, 2, 2)\n",
    "      plt.title(\"Mean Absolute Error (MAE) Loss During Training\")\n",
    "      plt.plot(mae_loss, label=\"MAE\")\n",
    "      plt.xlabel(\"Epoch\")\n",
    "      plt.ylabel(\"Loss\")\n",
    "      plt.legend()\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def calculate_metrics(pred_matrices, gt_matrices, print_losses=False, plot=False):\n",
    "  \"\"\"\n",
    "  Calculate various metrics to evaluate the predictions.\n",
    "\n",
    "  Parameters:\n",
    "  - pred_matrices (list of 2D arrays): List of predicted adjacency matrices.\n",
    "  - gt_matrices (list of 2D arrays): List of ground truth adjacency matrices.\n",
    "  - print_losses (bool): If True, print calculated metrics. Default is False.\n",
    "  - plot (bool): If True, plot barplots of metrics. Default is False.\n",
    "\n",
    "  Returns:\n",
    "  - mae (float): Mean Absolute Error of the predicted matrices compared to ground truth.\n",
    "  - pcc (float): Pearson Correlation Coefficient between the vectorized predicted and ground truth matrices.\n",
    "  - avg_mae_bc (float): Average Mean Absolute Error for Betweenness Centrality.\n",
    "  - avg_mae_ec (float): Average Mean Absolute Error for Eigenvector Centrality.\n",
    "  - avg_mae_pc (float): Average Mean Absolute Error for PageRank Centrality.\n",
    "  - js_dis (float): Jensen-Shannon Distance between vectorized predicted and ground truth matrices.\n",
    "  \"\"\"\n",
    "\n",
    "  num_test_samples = len(pred_matrices)\n",
    "\n",
    "  # Initialize lists to store MAEs for each centrality measure\n",
    "  mae_bc = []\n",
    "  mae_ec = []\n",
    "  mae_pc = []\n",
    "\n",
    "  # Iterate over each test sample\n",
    "  for i  in tqdm.tqdm(range(num_test_samples)):\n",
    "      # Convert adjacency matrices to NetworkX graphs\n",
    "      pred_graph = nx.from_numpy_array(pred_matrices[i], edge_attr=\"weight\")\n",
    "      gt_graph = nx.from_numpy_array(gt_matrices[i], edge_attr=\"weight\")\n",
    "\n",
    "      # Compute centrality measures\n",
    "      pred_bc = nx.betweenness_centrality(pred_graph, weight=\"weight\")\n",
    "      pred_ec = nx.eigenvector_centrality(pred_graph, weight=\"weight\")\n",
    "      pred_pc = nx.pagerank(pred_graph, weight=\"weight\")\n",
    "\n",
    "      gt_bc = nx.betweenness_centrality(gt_graph, weight=\"weight\")\n",
    "      gt_ec = nx.eigenvector_centrality(gt_graph, weight=\"weight\")\n",
    "      gt_pc = nx.pagerank(gt_graph, weight=\"weight\")\n",
    "\n",
    "      # Convert centrality dictionaries to lists\n",
    "      pred_bc_values = list(pred_bc.values())\n",
    "      pred_ec_values = list(pred_ec.values())\n",
    "      pred_pc_values = list(pred_pc.values())\n",
    "\n",
    "      gt_bc_values = list(gt_bc.values())\n",
    "      gt_ec_values = list(gt_ec.values())\n",
    "      gt_pc_values = list(gt_pc.values())\n",
    "\n",
    "      # Compute MAEs\n",
    "      mae_bc.append(mean_absolute_error(pred_bc_values, gt_bc_values))\n",
    "      mae_ec.append(mean_absolute_error(pred_ec_values, gt_ec_values))\n",
    "      mae_pc.append(mean_absolute_error(pred_pc_values, gt_pc_values))\n",
    "\n",
    "  # Compute average MAEs\n",
    "  avg_mae_bc = sum(mae_bc) / len(mae_bc)\n",
    "  avg_mae_ec = sum(mae_ec) / len(mae_ec)\n",
    "  avg_mae_pc = sum(mae_pc) / len(mae_pc)\n",
    "\n",
    "  # vectorize and flatten\n",
    "  pred_1d = []\n",
    "  gt_1d = []\n",
    "  for i in range(pred_matrices.shape[0]):\n",
    "    pred_1d.extend(MatrixVectorizer.vectorize(pred_matrices[i]).flatten())\n",
    "    gt_1d.extend(MatrixVectorizer.vectorize(gt_matrices[i]).flatten())\n",
    "\n",
    "  mae = mean_absolute_error(pred_1d, gt_1d)\n",
    "  pcc = pearsonr(pred_1d, gt_1d)[0]\n",
    "  js_dis = jensenshannon(pred_1d, gt_1d)\n",
    "\n",
    "  if print_losses:\n",
    "    print(\"MAE: \", mae)\n",
    "    print(\"PCC: \", pcc)\n",
    "    print(\"Jensen-Shannon Distance: \", js_dis)\n",
    "    print(\"Average MAE betweenness centrality:\", avg_mae_bc)\n",
    "    print(\"Average MAE eigenvector centrality:\", avg_mae_ec)\n",
    "    print(\"Average MAE PageRank centrality:\", avg_mae_pc)\n",
    "\n",
    "  # Plot barplots if requested\n",
    "  if plot:\n",
    "    labels = ['MAE','PCC','JSD','MAE (PC)', 'MAE (EC)', 'MAE (BC)']\n",
    "    data = [mae, pcc, js_dis, avg_mae_pc, avg_mae_ec, avg_mae_bc]\n",
    "    colors = ['#F46867', '#70B163', '#6A67FF', '#F9C963', '#83FEFF', '#80FE5F']\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.barplot(x=labels, y=data, palette=colors)\n",
    "    plt.ylabel('Error')\n",
    "    plt.show()\n",
    "  return mae, pcc, avg_mae_bc, avg_mae_ec, avg_mae_pc, js_dis\n",
    "\n",
    "def make_all_plots(mae, pcc, avg_mae_bc, avg_mae_ec, avg_mae_pc, js_dis):\n",
    "  \"\"\"\n",
    "  Create subplots displaying error metrics across different folds and their average.\n",
    "\n",
    "  Parameters:\n",
    "  - mae (list of floats): List of Mean Absolute Errors for each fold.\n",
    "  - pcc (list of floats): List of Pearson Correlation Coefficients for each fold.\n",
    "  - avg_mae_bc (list of floats): List of Average Mean Absolute Errors for Betweenness Centrality for each fold.\n",
    "  - avg_mae_ec (list of floats): List of Average Mean Absolute Errors for Eigenvector Centrality for each fold.\n",
    "  - avg_mae_pc (list of floats): List of Average Mean Absolute Errors for PageRank Centrality for each fold.\n",
    "  - js_dis (list of floats): List of Jensen-Shannon Distances for each fold.\n",
    "\n",
    "  Returns:\n",
    "  - None: Displays the subplots.\n",
    "  \"\"\"\n",
    "  # Create a figure with 4x2 subplots\n",
    "  fig, axes = plt.subplots(nrows=4, ncols=2, figsize=(10, 14))\n",
    "\n",
    "  colors = ['#F46867', '#70B163', '#6A67FF', '#F9C963', '#83FEFF', '#80FE5F']\n",
    "  labels = ['MAE','PCC','JSD','MAE (PC)', 'MAE (EC)', 'MAE (BC)']\n",
    "\n",
    "  # Plot the data on each subplot\n",
    "  data_1 = [mae[0], pcc[0], js_dis[0], avg_mae_pc[0], avg_mae_ec[0], avg_mae_bc[0]]\n",
    "  data_2 = [mae[1], pcc[1], js_dis[1], avg_mae_pc[1], avg_mae_ec[1], avg_mae_bc[1]]\n",
    "  data_3 = [mae[2], pcc[2], js_dis[2], avg_mae_pc[2], avg_mae_ec[2], avg_mae_bc[2]]\n",
    "  data_4 = [mae[3], pcc[3], js_dis[3], avg_mae_pc[3], avg_mae_ec[3], avg_mae_bc[3]]\n",
    "  errors = [np.std(mae[0:3]), np.std(pcc[0:3]), np.std(js_dis[0:3]),\n",
    "            np.std(avg_mae_pc[0:3]), np.std(avg_mae_ec[0:3]), np.std(avg_mae_bc[0:3])]\n",
    "\n",
    "  axes[0, 0].bar(x=labels[:3], height=data_1[:3], color=colors[:3])\n",
    "  axes[0, 0].set_title('Fold 1')\n",
    "\n",
    "  axes[0, 1].bar(x=labels[3:], height=data_1[3:], color=colors[3:])\n",
    "  axes[0, 1].set_title('Fold 1')\n",
    "\n",
    "  axes[1, 0].bar(x=labels[:3], height=data_2[0:3], color=colors[:3])\n",
    "  axes[1, 0].set_title('Fold 2')\n",
    "\n",
    "  axes[1, 1].bar(x=labels[3:], height=data_2[3:], color=colors[3:])\n",
    "  axes[1, 1].set_title('Fold 2')\n",
    "\n",
    "  axes[2, 0].bar(x=labels[:3], height=data_3[0:3], color=colors[:3])\n",
    "  axes[2, 0].set_title('Fold 3')\n",
    "\n",
    "  axes[2, 1].bar(x=labels[3:], height=data_3[3:], color=colors[3:])\n",
    "  axes[2, 1].set_title('Fold 3')\n",
    "\n",
    "  axes[3, 0].bar(x=labels[:3], height=data_4[:3], yerr=errors[:3], color=colors[:3], capsize=5)\n",
    "  axes[3, 0].set_title('Avg. Across Folds')\n",
    "\n",
    "  axes[3, 1].bar(x=labels[3:], height=data_4[3:], yerr=errors[3:], color=colors[3:], capsize=5)\n",
    "  axes[3, 1].set_title('Avg. Across Folds')\n",
    "\n",
    "  # Adjust layout for better spacing\n",
    "  plt.tight_layout()\n",
    "\n",
    "  # Show the plots\n",
    "  plt.show()\n",
    "\n",
    "def make_csv(data_to_print,filename):\n",
    "  \"\"\"\n",
    "  Create a CSV file from the given data and save it with the specified filename.\n",
    "\n",
    "  Parameters:\n",
    "  - data_to_print (list): List of data arrays to be printed to the CSV file.\n",
    "  - filename (str): The name of the CSV file to be created.\n",
    "\n",
    "  Notes:\n",
    "  - The data should be provided in the form of a list, where each element is an array.\n",
    "  - The function creates a CSV file with two columns: 'ID' and 'Predicted'.\n",
    "  - The 'ID' column represents the index of each element in the flattened arrays.\n",
    "  - The 'Predicted' column contains the flattened values from the input arrays.\n",
    "  \"\"\"\n",
    "\n",
    "  # Prepare data for DataFrame\n",
    "  data = {\n",
    "      'ID': [],\n",
    "      'Predicted': []\n",
    "  }\n",
    "\n",
    "\n",
    "  # Flatten each array and create rows for each element\n",
    "  id=1\n",
    "  for array in data_to_print:\n",
    "      #flattened = array.flatten()\n",
    "      flattened = array\n",
    "      for value in flattened:\n",
    "          data['ID'].append(id)\n",
    "          data['Predicted'].append(value)\n",
    "          id=id+1\n",
    "\n",
    "  # Create a DataFrame\n",
    "  df = pd.DataFrame(data)\n",
    "\n",
    "  # Write the DataFrame to an Excel file\n",
    "  df.to_csv(filename, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70a15aa",
   "metadata": {
    "id": "a70a15aa"
   },
   "source": [
    "## Model class definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dSlxlvmW0cO3",
   "metadata": {
    "id": "dSlxlvmW0cO3"
   },
   "source": [
    "### Helpful operational functions such as: pad_HR_adj, normalize adjacency and weight variable glorot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "i11hJR5L0bB7",
   "metadata": {
    "id": "i11hJR5L0bB7"
   },
   "outputs": [],
   "source": [
    "def pad_HR_adj(label, split):\n",
    "\n",
    "    label = torch.nn.functional.pad(label, (split, split, split, split), mode=\"constant\").to(device)\n",
    "    identity = torch.eye(label.shape[1]).to(device)\n",
    "    return label + identity\n",
    "\n",
    "def unpad(data, split):\n",
    "    if data.dim() == 2:\n",
    "        idx_0 = data.shape[0]-split\n",
    "        idx_1 = data.shape[1]-split\n",
    "        train = data[split:idx_0, split:idx_1]\n",
    "        return train\n",
    "    idx_0 = data.shape[1]-split\n",
    "    idx_1 = data.shape[2]-split\n",
    "    train = data[:, split:idx_0, split:idx_1]\n",
    "    return train\n",
    "\n",
    "def normalize_adj_torch(mx):\n",
    "    rowsum = mx.sum(-1)\n",
    "    r_inv_sqrt = torch.pow(rowsum, -0.5).flatten(start_dim=1)\n",
    "    r_inv_sqrt[torch.isinf(r_inv_sqrt)] = 0.\n",
    "    diag = torch.eye(r_inv_sqrt.shape[1]).to(device)\n",
    "    r_mat_inv_sqrt = r_inv_sqrt.unsqueeze(-1).expand(*r_inv_sqrt.shape, r_inv_sqrt.shape[1]) * diag\n",
    "    mx = torch.matmul(mx, r_mat_inv_sqrt)\n",
    "    mx = torch.transpose(mx, -2, -1)\n",
    "    mx = torch.matmul(mx, r_mat_inv_sqrt)\n",
    "    return mx\n",
    "\n",
    "def weight_variable_glorot(output_dim):\n",
    "\n",
    "    input_dim = output_dim\n",
    "    init_range = np.sqrt(6.0 / (input_dim + output_dim))\n",
    "    initial = np.random.uniform(-init_range, init_range,\n",
    "                                (input_dim, output_dim))\n",
    "\n",
    "    return initial\n",
    "\n",
    "def weight_variable_glorot_extended(input_dim,output_dim):\n",
    "\n",
    "    init_range = np.sqrt(6.0 / (input_dim + output_dim))\n",
    "    initial = np.random.uniform(-init_range, init_range,\n",
    "                                (input_dim, output_dim))\n",
    "\n",
    "    return initial\n",
    "\n",
    "def augment(lr, hr, aug=None, noise=None, p=1.0):\n",
    "    \"\"\"\n",
    "    Augment the input arrays lr and/or hr by adding noise.\n",
    "\n",
    "    Parameters:\n",
    "    - lr (np.ndarray): Low-resolution array.\n",
    "    - hr (np.ndarray): High-resolution array.\n",
    "    - aug (str or None): Specifies which array(s) to augment ('lr', 'hr', or 'both' for both).\n",
    "    - noise (float or None): Standard deviation of the noise to be added.\n",
    "    - p (float): Probability of adding noise.\n",
    "\n",
    "    Returns:\n",
    "    Tuple of augmented lr and hr arrays.\n",
    "    \"\"\"\n",
    "\n",
    "    # Helper function to add noise to an array\n",
    "    def add_noise(arr, noise_std):\n",
    "        z = torch.normal(0, noise_std, arr.shape).to(device)\n",
    "        z = (z + z.transpose(2, 1))/2\n",
    "        return arr + z\n",
    "\n",
    "    # Augment lr if specified\n",
    "    if aug == 'lr':\n",
    "        if random.random() < p:\n",
    "            lr = add_noise(lr, noise)\n",
    "            lr[lr<0] = 0\n",
    "\n",
    "    # Augment hr if specified\n",
    "    elif aug == 'hr':\n",
    "        if random.random() < p:\n",
    "            hr = add_noise(hr, noise)\n",
    "            hr[hr<0] = 0\n",
    "\n",
    "    elif aug == 'both':\n",
    "        if random.random() < p:\n",
    "            lr = add_noise(lr, noise)\n",
    "            hr = add_noise(hr, noise)\n",
    "            lr[lr<0] = 0\n",
    "            hr[hr<0] = 0\n",
    "\n",
    "    return lr, hr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sbs-bqQy0riK",
   "metadata": {
    "id": "sbs-bqQy0riK"
   },
   "source": [
    "### GSR Layer and GraphConvolution definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22e2303",
   "metadata": {
    "id": "e22e2303"
   },
   "outputs": [],
   "source": [
    "\n",
    "class GSRLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, hr_dim):\n",
    "        super(GSRLayer, self).__init__()\n",
    "\n",
    "        self.weights = torch.from_numpy(\n",
    "            weight_variable_glorot(hr_dim)).type(torch.FloatTensor)\n",
    "        self.weights = torch.nn.Parameter(\n",
    "            data=self.weights, requires_grad=True)\n",
    "\n",
    "    def forward(self, A, X):\n",
    "        with torch.autograd.set_detect_anomaly(True):\n",
    "\n",
    "            lr = A\n",
    "            lr_dim = lr.shape[1]\n",
    "            f = X\n",
    "            # depreceated\n",
    "            #eig_val_lr, U_lr = torch.symeig(lr, eigenvectors=True, upper=True)\n",
    "\n",
    "            eig_val_lr, U_lr = torch.linalg.eigh(lr, UPLO='U')\n",
    "\n",
    "            # U_lr = torch.abs(U_lr)\n",
    "            eye_mat = torch.eye(lr_dim).type(torch.FloatTensor)\n",
    "            s_d = torch.cat((eye_mat, eye_mat), 0).to(device)\n",
    "\n",
    "            a = torch.matmul(self.weights, s_d)\n",
    "            b = torch.matmul(a, torch.transpose(U_lr, 1, 2))\n",
    "            f_d = torch.matmul(b, f)\n",
    "            f_d = torch.abs(f_d)\n",
    "            identity = torch.eye(f_d.shape[1]).to(device)\n",
    "            adj = f_d + identity\n",
    "\n",
    "            X = torch.matmul(adj, torch.transpose(adj, 1, 2))\n",
    "            X = (X + X.transpose(1, 2))/2\n",
    "            identity = torch.eye(X.shape[1]).to(device)\n",
    "            X += identity\n",
    "        return adj, torch.abs(X)\n",
    "\n",
    "class HGSRLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, lr_dim, inter_dim, hr_dim):\n",
    "        super(HGSRLayer, self).__init__()\n",
    "\n",
    "        self.inter_dim = inter_dim\n",
    "\n",
    "        self.weights_1 = torch.from_numpy(\n",
    "            weight_variable_glorot_extended(inter_dim,lr_dim)).type(torch.FloatTensor)\n",
    "        self.weights_1 = torch.nn.Parameter(\n",
    "            data=self.weights_1, requires_grad=True)\n",
    "\n",
    "        self.weights_2 = torch.from_numpy(\n",
    "            weight_variable_glorot_extended(hr_dim,inter_dim)).type(torch.FloatTensor)\n",
    "        self.weights_2 = torch.nn.Parameter(\n",
    "            data=self.weights_2, requires_grad=True)\n",
    "\n",
    "        self.weights_3 = torch.from_numpy(\n",
    "            weight_variable_glorot_extended(hr_dim,inter_dim)).type(torch.FloatTensor)\n",
    "        self.weights_3 = torch.nn.Parameter(\n",
    "            data=self.weights_3, requires_grad=True)\n",
    "\n",
    "        self.weights_4 = torch.from_numpy(\n",
    "            weight_variable_glorot_extended(inter_dim,hr_dim)).type(torch.FloatTensor)\n",
    "        self.weights_4 = torch.nn.Parameter(\n",
    "            data=self.weights_4, requires_grad=True)\n",
    "\n",
    "        self.gc1 = GraphConvolution(\n",
    "            inter_dim, inter_dim, 0, act=F.relu)\n",
    "        self.gc2 = GraphConvolution(\n",
    "            inter_dim, inter_dim, 0, act=F.relu)\n",
    "\n",
    "        # Initialise W_2, W_4 with zeros\n",
    "        self.weights_2.data.fill_(0)\n",
    "        self.weights_4.data.fill_(0)\n",
    "        self.weights_1.data.fill_(0)\n",
    "        self.weights_3.data.fill_(0)\n",
    "\n",
    "\n",
    "    def forward(self, A, X):\n",
    "        with torch.autograd.set_detect_anomaly(True):\n",
    "            # First weights\n",
    "            f_d = self.weights_1 @ (X @ self.weights_2 + X[:, :, :self.inter_dim])\n",
    "            f_d = torch.abs(f_d)\n",
    "            identity = torch.eye(f_d.shape[1], dtype=torch.float32).to(device)\n",
    "            adj = f_d + identity\n",
    "\n",
    "            X_I = torch.matmul(adj, adj.transpose(1, 2))\n",
    "            X_I = (X_I + X_I.transpose(1, 2))/2\n",
    "            identity = torch.eye(X_I.shape[1], dtype=torch.float32).to(device)\n",
    "            X_I = X_I + identity\n",
    "\n",
    "            self.X_I = self.gc1(X_I, adj)\n",
    "            self.X_I = self.gc2(X_I, adj)\n",
    "\n",
    "            f_d = self.weights_3 @ (X_I @ self.weights_4 + self.weights_1 @ X) # try W_5 in case it doesn't work\n",
    "            f_d = torch.abs(f_d)\n",
    "            identity = torch.eye(f_d.shape[1], dtype=torch.float32).to(device)\n",
    "            adj = f_d + identity\n",
    "\n",
    "            X = torch.matmul(adj, adj.transpose(1, 2))\n",
    "            X = (X + X.transpose(1, 2))/2\n",
    "            identity = torch.eye(X.shape[1], dtype=torch.float32).to(device)\n",
    "            X = X + identity\n",
    "\n",
    "        return adj, torch.abs(X)\n",
    "\n",
    "\n",
    "class GraphConvolution(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, dropout, act=F.relu):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.dropout = dropout\n",
    "        self.act = act\n",
    "        self.weight = torch.nn.Parameter(\n",
    "            torch.FloatTensor(in_features, out_features))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        torch.nn.init.xavier_uniform_(self.weight)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        input = F.dropout(input, self.dropout, self.training)\n",
    "        support = torch.matmul(input, self.weight)\n",
    "        output = torch.matmul(adj, support)\n",
    "        output = self.act(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2oQMPgLIw-k6",
   "metadata": {
    "id": "2oQMPgLIw-k6"
   },
   "source": [
    "### GraphUnet, GCN definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a9e132",
   "metadata": {
    "id": "25a9e132"
   },
   "outputs": [],
   "source": [
    "# Ops\n",
    "\n",
    "class GraphUnpool(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(GraphUnpool, self).__init__()\n",
    "\n",
    "    def forward(self, A, X, idx):\n",
    "        new_X = torch.zeros([A.shape[0], A.shape[1], X.shape[2]]).to(device)\n",
    "        extract = torch.arange(A.shape[0], dtype=torch.int).to(device)\n",
    "        new_X[:, idx][extract, extract] = X\n",
    "        return A, new_X\n",
    "\n",
    "class GraphPool(nn.Module):\n",
    "\n",
    "    def __init__(self, k, in_dim):\n",
    "        super(GraphPool, self).__init__()\n",
    "        self.k = k\n",
    "        self.proj = nn.Linear(in_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, A, X):\n",
    "        scores = self.proj(X)\n",
    "        # scores = torch.abs(scores)\n",
    "        scores = torch.squeeze(scores, dim=(1,2))\n",
    "        scores = self.sigmoid(scores/100)\n",
    "        num_nodes = A.shape[1]\n",
    "        values, idx = torch.topk(scores, int(self.k*num_nodes))\n",
    "        extract = torch.arange(A.shape[0], dtype=torch.int).to(device)\n",
    "\n",
    "        new_X = X[:, idx, :][extract, extract, :, :]\n",
    "        values = torch.unsqueeze(values, -1)\n",
    "        new_X = torch.mul(new_X, values)\n",
    "        A = A[:, idx, :][extract, extract, :, :]\n",
    "        A = A[:, :, idx][extract, :, extract, :]\n",
    "        return A, new_X, idx\n",
    "\n",
    "\n",
    "class GCN(nn.Module):\n",
    "\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(GCN, self).__init__()\n",
    "        self.proj = nn.Linear(in_dim, out_dim)\n",
    "        self.drop = nn.Dropout(p=0)\n",
    "\n",
    "    def forward(self, A, X):\n",
    "\n",
    "        X = self.drop(X)\n",
    "        X = torch.matmul(A, X)\n",
    "        X = self.proj(X)\n",
    "        return X\n",
    "\n",
    "\n",
    "class GraphUnet(nn.Module):\n",
    "\n",
    "    def __init__(self, ks, in_dim, out_dim, dim):\n",
    "        super(GraphUnet, self).__init__()\n",
    "        self.ks = ks\n",
    "\n",
    "        self.start_gcn = GCN(in_dim, dim).to(device)\n",
    "        self.bottom_gcn = GCN(dim, dim).to(device)\n",
    "        self.end_gcn = GCN(2*dim, out_dim).to(device)\n",
    "        self.down_gcns = []\n",
    "        self.up_gcns = []\n",
    "        self.pools = []\n",
    "        self.unpools = []\n",
    "        self.l_n = len(ks)\n",
    "        for i in range(self.l_n):\n",
    "            self.down_gcns.append(GCN(dim, dim).to(device))\n",
    "            self.up_gcns.append(GCN(dim, dim).to(device))\n",
    "            self.pools.append(GraphPool(ks[i], dim).to(device))\n",
    "            self.unpools.append(GraphUnpool().to(device))\n",
    "\n",
    "    def forward(self, A, X):\n",
    "        adj_ms = []\n",
    "        indices_list = []\n",
    "        down_outs = []\n",
    "        X = self.start_gcn(A, X)\n",
    "        start_gcn_outs = X\n",
    "        org_X = X\n",
    "        for i in range(self.l_n):\n",
    "\n",
    "            X = self.down_gcns[i](A, X)\n",
    "            adj_ms.append(A)\n",
    "            down_outs.append(X)\n",
    "            A, X, idx = self.pools[i](A, X)\n",
    "            indices_list.append(idx)\n",
    "        X = self.bottom_gcn(A, X)\n",
    "        for i in range(self.l_n):\n",
    "            up_idx = self.l_n - i - 1\n",
    "\n",
    "            A, idx = adj_ms[up_idx].to(device), indices_list[up_idx].to(device)\n",
    "            A, X = self.unpools[i](A, X, idx)\n",
    "            X = self.up_gcns[i](A, X)\n",
    "            X = X.add(down_outs[up_idx])\n",
    "        X = torch.cat([X, org_X], 2)\n",
    "        X = self.end_gcn(A, X)\n",
    "\n",
    "        return X, start_gcn_outs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0-QLiVbaxJ60",
   "metadata": {
    "id": "0-QLiVbaxJ60"
   },
   "source": [
    "### AGSRNet, dense and discriminator definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2981838",
   "metadata": {
    "id": "e2981838"
   },
   "outputs": [],
   "source": [
    "class AGSRNet(nn.Module):\n",
    "\n",
    "    def __init__(self, ks, args):\n",
    "        super(AGSRNet, self).__init__()\n",
    "\n",
    "        self.lr_dim = args.lr_dim\n",
    "        self.inter_dim = args.inter_dim\n",
    "        self.hr_dim = args.hr_dim\n",
    "        self.hidden_dim = args.hidden_dim\n",
    "\n",
    "        self.layer = HGSRLayer(self.lr_dim, self.inter_dim, self.hr_dim)\n",
    "\n",
    "        self.net = GraphUnet(ks, self.lr_dim, self.hr_dim, self.hr_dim)\n",
    "        self.gc1 = GraphConvolution(\n",
    "            self.hr_dim, self.hidden_dim, 0, act=F.relu)\n",
    "        self.gc2 = GraphConvolution(\n",
    "            self.hidden_dim, self.hr_dim, 0, act=F.relu)\n",
    "\n",
    "    def forward(self, lr, lr_dim, hr_dim):\n",
    "        with torch.autograd.set_detect_anomaly(True):\n",
    "\n",
    "            I = torch.eye(self.lr_dim).type(torch.FloatTensor)\n",
    "            A = normalize_adj_torch(lr).type(torch.FloatTensor)\n",
    "            A = A.to(device)\n",
    "            I = I.to(device)\n",
    "\n",
    "            self.net_outs, self.start_gcn_outs = self.net(A, I)\n",
    "\n",
    "            self.outputs, self.Z = self.layer(A, self.net_outs)\n",
    "\n",
    "            self.hidden1 = self.gc1(self.Z, self.outputs)\n",
    "            self.hidden2 = self.gc2(self.hidden1, self.outputs)\n",
    "            z = self.hidden2\n",
    "\n",
    "            z = (z + z.transpose(1, 2))/2\n",
    "            identity = torch.eye(z.shape[1]).to(device)\n",
    "            z += identity\n",
    "\n",
    "        return torch.abs(z), self.net_outs, self.start_gcn_outs, self.outputs\n",
    "\n",
    "\n",
    "class Dense(nn.Module):\n",
    "    def __init__(self, n1, n2, args):\n",
    "        super(Dense, self).__init__()\n",
    "        self.weights = torch.nn.Parameter(\n",
    "            torch.FloatTensor(n1, n2), requires_grad=True)\n",
    "        nn.init.normal_(self.weights, mean=args.mean_dense, std=args.std_dense)\n",
    "\n",
    "    def forward(self, x):\n",
    "        np.random.seed(1)\n",
    "        torch.manual_seed(1)\n",
    "\n",
    "        out = torch.matmul(x, self.weights)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(Discriminator, self).__init__()\n",
    "        # Add GCN and MLP in discriminator\n",
    "        self.hr_dim=args.hr_dim\n",
    "\n",
    "        self.gcn_1 = GCN(args.hr_dim, args.hr_dim).to(device)\n",
    "        self.relu_gcn_1 = nn.ReLU(inplace=False)\n",
    "\n",
    "        self.gcn_2 = GCN(args.hr_dim, args.hr_dim).to(device)\n",
    "        self.relu_gcn_2 = nn.ReLU(inplace=False)\n",
    "\n",
    "\n",
    "        self.dense_1 = Dense(args.hr_dim, args.hr_dim, args)\n",
    "        self.relu_1 = nn.ReLU(inplace=False)\n",
    "        self.dense_2 = Dense(args.hr_dim, args.hr_dim, args)\n",
    "        self.relu_2 = nn.ReLU(inplace=False)\n",
    "        self.dense_3 = Dense(args.hr_dim, 1, args)\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        np.random.seed(1)\n",
    "        torch.manual_seed(1)\n",
    "        # GCN part\n",
    "        I = torch.eye(self.hr_dim).type(torch.FloatTensor).repeat(inputs.shape[0], 1, 1)\n",
    "        A = normalize_adj_torch(inputs).type(torch.FloatTensor)\n",
    "        A = A.to(device)\n",
    "        I = I.to(device)\n",
    "        gcn_outs_1 =self.relu_gcn_1 (self.gcn_1(A,I))\n",
    "        gcn_outs_2 = self.relu_gcn_2(self.gcn_2(A,gcn_outs_1))\n",
    "\n",
    "\n",
    "        # dc_den1 = self.relu_1(self.dense_1(inputs))\n",
    "        # MLP Part\n",
    "        dc_den1 = self.relu_1(self.dense_1(gcn_outs_2))\n",
    "\n",
    "        dc_den2 = self.relu_2(self.dense_2(dc_den1))\n",
    "        output = dc_den2\n",
    "        output = self.dense_3(dc_den2)\n",
    "        output = self.sigmoid(output)\n",
    "\n",
    "        return torch.abs(output)\n",
    "\n",
    "def gaussian_noise_layer(input_layer, args):\n",
    "    z = torch.empty_like(input_layer)\n",
    "    noise = z.normal_(mean=args.mean_gaussian, std=args.std_gaussian)\n",
    "    z = torch.abs(input_layer + noise)\n",
    "\n",
    "    z = (z + z.transpose(1, 2))/2\n",
    "    identity = torch.eye(z.shape[1]).to(device)\n",
    "    return z + identity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515f63e5",
   "metadata": {
    "id": "515f63e5"
   },
   "source": [
    "## Training and Testing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9134389-541d-4f9b-bfe2-541d20734d8b",
   "metadata": {
    "id": "e9134389-541d-4f9b-bfe2-541d20734d8b"
   },
   "outputs": [],
   "source": [
    "def train(model, subjects_adj, subjects_labels, args, aug=None, batch_size=16, val_adj=None, val_labels=None):\n",
    "    \"\"\"\n",
    "    Train a given PyTorch model on a dataset of low-resolution and high-resolution adjacency matrices.\n",
    "\n",
    "    Parameters:\n",
    "    - model (torch.nn.Module): The PyTorch model to be trained.\n",
    "    - subjects_adj (list): List of numpy arrays representing the low-resolution adjacency matrices.\n",
    "    - subjects_labels (list): List of numpy arrays representing the corresponding high-resolution adjacency matrices.\n",
    "    - args (Namespace): Namespace containing training-related parameters.\n",
    "    - aug (str or None): Specifies which array(s) to augment ('lr', 'hr', or 'both' for both).\n",
    "    - batch_size (int): Training batch size\n",
    "\n",
    "    Returns:\n",
    "    - train_losses_D (list): List of discriminator loss values for each epoch during training.\n",
    "    - train_losses_G (list): List of generator loss values for each epoch during training.\n",
    "    - mae_losses (list): List of Mean Absolute Error (MAE) values for each epoch during training.\n",
    "    \"\"\"\n",
    "    criterion = nn.MSELoss()\n",
    "    bce_loss = nn.BCELoss()\n",
    "    netD = Discriminator(args).to(device)\n",
    "    optimizerG = optim.Adam(model.parameters(), lr=args.g_lr)\n",
    "    optimizerD = optim.Adam(netD.parameters(), lr=args.d_lr)\n",
    "\n",
    "    train_losses_G = []\n",
    "    train_losses_D = []\n",
    "    mae_losses = []\n",
    "    mae_losses_validation=[]\n",
    "\n",
    "    subjects_adj = torch.from_numpy(subjects_adj).type(torch.FloatTensor).to(device)\n",
    "    subjects_labels = torch.from_numpy(subjects_labels).type(torch.FloatTensor).to(device)\n",
    "\n",
    "    lr_addition = tuple()\n",
    "    hr_addition = tuple()\n",
    "\n",
    "    for epoch in range(args.epochs):\n",
    "      epoch_performance = []\n",
    "      # Track losses for plotting\n",
    "      train_loss_D = 0\n",
    "      train_loss_G = 0\n",
    "      total_mae_loss = 0\n",
    "\n",
    "      perm = torch.randperm(subjects_adj.shape[0])\n",
    "      lr_batches = torch.split(subjects_adj[perm], batch_size) + lr_addition\n",
    "      hr_batches = torch.split(subjects_labels[perm], batch_size) + hr_addition\n",
    "      counter=0\n",
    "      with tqdm.tqdm(zip(lr_batches, hr_batches), unit=\"batch\", total=len(lr_batches)) as tepoch:\n",
    "        for i, (lr, hr) in enumerate(tepoch):\n",
    "            # Data augmentation\n",
    "            lr, hr = augment(lr, hr, aug, args.noise, args.p)\n",
    "\n",
    "            optimizerD.zero_grad()\n",
    "            optimizerG.zero_grad()\n",
    "\n",
    "            # Set device of data\n",
    "            lr = lr.to(device)\n",
    "            hr = hr.to(device)\n",
    "\n",
    "            model_outputs, net_outs, start_gcn_outs, layer_outs = model(\n",
    "                lr, args.lr_dim, args.hr_dim)\n",
    "\n",
    "            mse_loss = args.lmbda * criterion(net_outs, start_gcn_outs) + criterion(model_outputs, hr)\n",
    "\n",
    "            # Loss for plots\n",
    "            mae_loss = F.l1_loss(model_outputs, hr)\n",
    "\n",
    "            if epoch % 4 == 0:\n",
    "                epoch_performance.append((i, mae_loss.item()))\n",
    "\n",
    "\n",
    "            # Process real data and fake data for discriminator\n",
    "            real_data = model_outputs.detach()\n",
    "            fake_data = gaussian_noise_layer(hr, args)\n",
    "\n",
    "            # Pass them through the discriminator\n",
    "            d_real = netD(real_data).to(device)\n",
    "            d_fake = netD(fake_data).to(device)\n",
    "\n",
    "            # Calculate discriminator loss\n",
    "            dc_loss_real = bce_loss(d_real, torch.ones(d_real.shape[0], args.hr_dim, 1).to(device))\n",
    "            dc_loss_fake = bce_loss(d_fake, torch.zeros(d_real.shape[0], args.hr_dim, 1).to(device))\n",
    "            dc_loss = dc_loss_real + dc_loss_fake\n",
    "\n",
    "            # Optimise the discriminator\n",
    "            dc_loss.backward()\n",
    "            optimizerD.step()\n",
    "\n",
    "            # Add noise to the generated image and pass it through the generator\n",
    "            # d_fake = netD(model_outputs).to(device)\n",
    "\n",
    "            # their initial d_fake\n",
    "            d_fake = netD(gaussian_noise_layer(hr, args)).to(device)\n",
    "\n",
    "            # Calculate the generator loss\n",
    "            gen_loss = bce_loss(d_fake, torch.ones(d_fake.shape[0], args.hr_dim, 1).to(device))\n",
    "            generator_loss = args.g_weight * gen_loss + mse_loss\n",
    "\n",
    "            # just adding - as we said in meeting\n",
    "            #generator_loss = -gen_loss + mse_loss\n",
    "\n",
    "            # Optimise the generator\n",
    "            generator_loss.backward()\n",
    "            optimizerG.step()\n",
    "\n",
    "            # Logging info\n",
    "            train_loss_D += dc_loss.item()\n",
    "            train_loss_G += generator_loss.item()\n",
    "            total_mae_loss += mae_loss.item()\n",
    "            counter+=1\n",
    "\n",
    "            # Logging\n",
    "            if i % 50 == 0:\n",
    "                tepoch.set_description(f\"Epoch {epoch}\")\n",
    "                tepoch.set_postfix(Loss_D=dc_loss.item(), Loss_G=generator_loss.item(), MAE=mae_loss.item())\n",
    "\n",
    "\n",
    "      if epoch % 4 == 0:\n",
    "        worst_performing_samples = sorted(epoch_performance, key=lambda x: x[1], reverse=True)\n",
    "        print(\"Worst-performing training samples last epoch (Index: MAE):\")\n",
    "        for index, mae in worst_performing_samples[:2]:  # Adjust the slice as needed\n",
    "            print(f\"Index {index}: MAE = {mae}\")\n",
    "        indices_to_duplicate = worst_performing_samples[:2]  # Example indices\n",
    "\n",
    "        # Duplicate each chosen sample 2 additional times\n",
    "        lr_addition = tuple(lr_batches[index] for index, _ in indices_to_duplicate for _ in range(2))\n",
    "        hr_addition = tuple(hr_batches[index] for index, _ in indices_to_duplicate for _ in range(2))\n",
    "\n",
    "      # train_losses_D.append(batch_size * train_loss_D / len(subjects_adj))\n",
    "      # train_losses_G.append(batch_size * train_loss_G / len(subjects_adj))\n",
    "\n",
    "      # mae_losses.append(batch_size * total_mae_loss / len(subjects_adj))\n",
    "      train_losses_D.append(train_loss_D / counter)\n",
    "      train_losses_G.append(train_loss_G / counter)\n",
    "      mae_losses.append(total_mae_loss / counter)\n",
    "\n",
    "\n",
    "      if ((val_adj is not None) and (val_labels is not None)):\n",
    "        _,validation_total_mae_loss=val_with_real(model, val_adj, val_labels, args)\n",
    "        mae_losses_validation.append(validation_total_mae_loss)\n",
    "\n",
    "    return train_losses_D, train_losses_G, mae_losses, mae_losses_validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890bc1ed-293a-4b37-8737-2891021041f7",
   "metadata": {
    "id": "890bc1ed-293a-4b37-8737-2891021041f7"
   },
   "source": [
    "### Validation and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2963a925-e320-4518-82f4-f4002d012e8a",
   "metadata": {
    "id": "2963a925-e320-4518-82f4-f4002d012e8a"
   },
   "outputs": [],
   "source": [
    "def val_with_real(model, val_adj, val_labels, args, to_csv=None):\n",
    "    \"\"\"\n",
    "    Evaluate a PyTorch model on validation data and compute relevant metrics.\n",
    "\n",
    "    Parameters:\n",
    "    - model (torch.nn.Module): The PyTorch model to be evaluated.\n",
    "    - val_adj (list): List of numpy arrays representing the low-resolution adjacency matrices.\n",
    "    - val_labels (list): List of numpy arrays representing the corresponding high-resolution adjacency matrices.\n",
    "    - args (Namespace): Namespace containing additional arguments for the evaluation.\n",
    "    - to_csv (str or None): If provided, the path to save the predictions in CSV format. Default is None.\n",
    "\n",
    "    Returns:\n",
    "    - preds_list (numpy.ndarray): Array of predictions for each evaluated pair of low and high-resolution matrices.\n",
    "    \"\"\"\n",
    "    val_error = []\n",
    "    preds_list = []\n",
    "    preds_vectorized_list = []\n",
    "\n",
    "    for lr, hr in zip(val_adj, val_labels):\n",
    "        all_zeros_lr = not np.any(lr)\n",
    "        all_zeros_hr = not np.any(hr)\n",
    "        if all_zeros_lr == False and all_zeros_hr == False:\n",
    "            lr = torch.from_numpy(lr).type(torch.FloatTensor)\n",
    "            np.fill_diagonal(hr, 1)\n",
    "            hr = torch.from_numpy(hr).type(torch.FloatTensor)\n",
    "\n",
    "            lr = lr.to(device)\n",
    "            hr = hr.to(device)\n",
    "\n",
    "            preds, a, b, c = model(lr[None, :, :], args.lr_dim, args.hr_dim)\n",
    "            preds = preds[0]\n",
    "            a = a[0]\n",
    "            b = b[0]\n",
    "            c = c[0]\n",
    "\n",
    "            # post-processing\n",
    "            preds[preds < 0] = 0\n",
    "\n",
    "            preds_list.append(preds.cpu().detach().numpy())\n",
    "            preds_vectorized_list.append(MatrixVectorizer.vectorize(preds.cpu().detach().numpy()))\n",
    "            mae_loss = F.l1_loss(preds, hr)\n",
    "\n",
    "            val_error.append(mae_loss.item())\n",
    "    actual_error=np.mean(val_error)\n",
    "    print(\"Validation error MAE: \",actual_error )\n",
    "\n",
    "    preds_list = np.asarray(preds_list)\n",
    "\n",
    "    if to_csv is not None:\n",
    "      make_csv(preds_vectorized_list,to_csv)\n",
    "    return preds_list,actual_error\n",
    "\n",
    "\n",
    "def test(model, test_adj, args, to_csv):\n",
    "    \"\"\"\n",
    "    Test a PyTorch model on a set of low-resolution adjacency matrices and generate predictions.\n",
    "\n",
    "    Parameters:\n",
    "    - model (torch.nn.Module): The PyTorch model to be tested.\n",
    "    - test_adj (list): List of numpy arrays representing the low-resolution adjacency matrices.\n",
    "    - args (Namespace): Namespace containing additional arguments for testing.\n",
    "    - to_csv (str or None): If provided, the path to save the predictions in CSV format. Default is None.\n",
    "\n",
    "    Returns:\n",
    "    - preds_list (numpy.ndarray): Array of vectorized predictions for each evaluated low-resolution matrix.\n",
    "    \"\"\"\n",
    "    preds_list = []\n",
    "    preds_vectorized_list = []\n",
    "\n",
    "    for lr in test_adj:\n",
    "        all_zeros_lr = not np.any(lr)\n",
    "        if all_zeros_lr == False :\n",
    "            lr = torch.from_numpy(lr).type(torch.FloatTensor)\n",
    "\n",
    "            lr = lr.to(device)\n",
    "            preds, a, b, c = model(lr[None, :, :], args.lr_dim, args.hr_dim)\n",
    "            preds = preds[0]\n",
    "            a = a[0]\n",
    "            b = b[0]\n",
    "            c = c[0]\n",
    "\n",
    "            # post-processing\n",
    "            preds[preds < 0] = 0\n",
    "\n",
    "            preds_list.append(preds.cpu().detach().numpy())\n",
    "            preds_vectorized_list.append(MatrixVectorizer.vectorize(preds.cpu().detach().numpy()))\n",
    "\n",
    "    preds_list = np.asarray(preds_list)\n",
    "\n",
    "    if to_csv is not None:\n",
    "      make_csv(preds_vectorized_list,to_csv)\n",
    "\n",
    "    return preds_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "W6oWDfxz09Bv",
   "metadata": {
    "id": "W6oWDfxz09Bv"
   },
   "source": [
    "# Argument class for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "J5tlv5CxzyCP",
   "metadata": {
    "id": "J5tlv5CxzyCP"
   },
   "outputs": [],
   "source": [
    "class Args:\n",
    "    \"\"\"\n",
    "    Container class for model training and configuration parameters.\n",
    "    These are the model's hyperparameters.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.epochs = 156\n",
    "\n",
    "        self.d_lr = 0.0001\n",
    "        self.g_lr = 0.0001\n",
    "\n",
    "        self.g_weight = 1.0\n",
    "        self.lmbda = 0.1\n",
    "        self.lr_dim = 160\n",
    "        self.inter_dim = 240\n",
    "        self.hr_dim = 268\n",
    "        self.hidden_dim = 268\n",
    "        self.padding = 26\n",
    "        self.mean_dense = 0.0\n",
    "        self.std_dense = 0.01\n",
    "        self.mean_gaussian = 0.0\n",
    "        self.std_gaussian = 0.1\n",
    "\n",
    "        self.noise = 0.005\n",
    "        self.p = 0.70\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8_PfC8Fwpr0w",
   "metadata": {
    "id": "8_PfC8Fwpr0w"
   },
   "source": [
    "# Run the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "IMunck3iwHVI",
   "metadata": {
    "id": "IMunck3iwHVI"
   },
   "source": [
    "## 3-Cross Validation Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Fymdl1l1qBL1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "id": "Fymdl1l1qBL1",
    "outputId": "f9cff2d2-0740-4aa2-cf70-04e2c33e7b59"
   },
   "outputs": [],
   "source": [
    "def run_cross_validation(X, Y, n_splits=3, plot=True, batch_size=4, aug='lr', plot_for_val_and_train=False):\n",
    "  \"\"\"\n",
    "  Perform k-fold cross-validation on a given model using the provided data\n",
    "  writes csv files for the validation sets and creates plot of the performance\n",
    "  metrics.\n",
    "\n",
    "  Parameters:\n",
    "  - X (numpy.ndarray): Input data (low-resolution adjacency matrices).\n",
    "  - Y (numpy.ndarray): Target data (high-resolution adjacency matrices).\n",
    "  - n_splits (int): Number of folds for cross-validation. Default is 3.\n",
    "  - plot (bool): If True, plot metrics across folds and their averages. Default is True.\n",
    "\n",
    "  Returns:\n",
    "  - None: Displays the cross-validation plots and creates the csv files.\n",
    "  \"\"\"\n",
    "\n",
    "  # Define the parameters\n",
    "  args = Args()\n",
    "  ks = [0.9, 0.7, 0.6, 0.5] # graphUnet params\n",
    "\n",
    "  mae_scores = []\n",
    "  pcc_scores = []\n",
    "  avg_mae_bc_scores = []\n",
    "  avg_mae_ec_scores = []\n",
    "  avg_mae_pc_scores = []\n",
    "  js_dis_scores = []\n",
    "\n",
    "  X_trai = []\n",
    "  with open(path_lr_data_tr) as f:\n",
    "    next(f)\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "      rowData = [ float(elem) for elem in row ]\n",
    "      X_trai.append(rowData)\n",
    "  X_trai = np.array(X_trai)\n",
    "  scipy.io.savemat('LR_data_X_160.mat', {'train':X_trai})\n",
    "  lr_data_path_X = './LR_data_X_160.mat'\n",
    "  X_trai = loadmat(my_project_folder +lr_data_path_X)\n",
    "  X_train = np.zeros((93,160,160))\n",
    "  for i in range(93):\n",
    "    X_train[i] = MatrixVectorizer.anti_vectorize(X_trai['train'][i], 160, include_diagonal=False)\n",
    "\n",
    "  Y_trai = []\n",
    "  with open(path_hr_data_tr) as f:\n",
    "    next(f)\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "      rowData = [ float(elem) for elem in row ]\n",
    "      Y_trai.append(rowData)\n",
    "  Y_trai = np.array(Y_trai)\n",
    "  scipy.io.savemat('HR_data_Y_268.mat', {'train':Y_trai})\n",
    "  hr_data_path_Y = './HR_data_Y_268.mat'\n",
    "  Y_trai = loadmat(my_project_folder +hr_data_path_Y)\n",
    "  Y_train = np.zeros((93,268,268))\n",
    "  for i in range(93):\n",
    "    Y_train[i] = MatrixVectorizer.anti_vectorize(Y_trai['train'][i], 268, include_diagonal=False)\n",
    "\n",
    "  X_valid = []\n",
    "  with open(path_lr_data_valid) as f:\n",
    "    next(f)\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "      rowData = [ float(elem) for elem in row ]\n",
    "      X_valid.append(rowData)\n",
    "  X_valid = np.array(X_valid)\n",
    "  scipy.io.savemat('LR_data_Xval_160.mat', {'train':X_valid})\n",
    "  lr_data_path_Xval = './LR_data_Xval_160.mat'\n",
    "  X_valid = loadmat(my_project_folder +lr_data_path_Xval)\n",
    "  X_val = np.zeros((40,160,160))\n",
    "  for i in range(40):\n",
    "    X_val[i] = MatrixVectorizer.anti_vectorize(X_valid['train'][i], 160, include_diagonal=False)\n",
    "\n",
    "  Y_valid = []\n",
    "  with open(path_hr_data_valid) as f:\n",
    "    next(f)\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "      rowData = [ float(elem) for elem in row ]\n",
    "      Y_valid.append(rowData)\n",
    "  Y_valid = np.array(Y_valid)\n",
    "  scipy.io.savemat('HR_data_Yval_268.mat', {'train':Y_valid})\n",
    "  hr_data_path_Y = './HR_data_Yval_268.mat'\n",
    "  Y_valid = loadmat(my_project_folder +hr_data_path_Y)\n",
    "  Y_val = np.zeros((40,268,268))\n",
    "  for i in range(40):\n",
    "    Y_val[i] = MatrixVectorizer.anti_vectorize(Y_valid['train'][i], 268, include_diagonal=False)\n",
    "\n",
    "  model = AGSRNet(ks, args).to(device)\n",
    "\n",
    "  if plot_for_val_and_train:\n",
    "    train_losses_D, train_losses_G, mae_losses, mae_losses_val = train(model, X_train, Y_train, args, batch_size=batch_size, aug=aug, val_adj=X_val,val_labels=Y_val)\n",
    "  else:\n",
    "    train_losses_D, train_losses_G, mae_losses, mae_losses_val = train(model, X_train, Y_train, args, batch_size=batch_size, aug=aug)\n",
    "\n",
    "  preds, _ = val_with_real(model, X_val, Y_val, args, to_csv=f\"predictions_fold_{i+1}.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119a996e-9ee1-4b0f-afbf-f2a57e990406",
   "metadata": {
    "id": "119a996e-9ee1-4b0f-afbf-f2a57e990406"
   },
   "source": [
    "## Executing the 3-Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zzxASGCwF9Ay",
   "metadata": {
    "id": "zzxASGCwF9Ay"
   },
   "outputs": [],
   "source": [
    "if option == 0:\n",
    "    run_cross_validation(X, Y, n_splits=3, plot=True, plot_for_val_and_train=True, batch_size=4, aug='lr')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2mXo3yR1q1b6",
   "metadata": {
    "id": "2mXo3yR1q1b6"
   },
   "source": [
    "### Model training with all the data and creating the submission file for Kaggle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c5aeb5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d4c5aeb5",
    "outputId": "1d363679-9225-47b1-cda3-23e4d202b2d7"
   },
   "outputs": [],
   "source": [
    "if option == 1:\n",
    "    # Define the parameters\n",
    "\n",
    "    args = Args()\n",
    "    ks = [0.9, 0.7, 0.6, 0.5] # graphUnet params\n",
    "\n",
    "    model = AGSRNet(ks, args).to(device)\n",
    "    print(model)\n",
    "\n",
    "    train_losses_D, train_losses_G, mae_losses,_ = train(model, X, Y, args, batch_size=4, aug='lr')\n",
    "    test_preds= test(model, X_test, args, \"output.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "y_CmDwrvU50f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 601
    },
    "id": "y_CmDwrvU50f",
    "outputId": "4948d7ae-7a49-400b-b68c-c3e1e86c5f3a"
   },
   "outputs": [],
   "source": [
    "# Plot losses\n",
    "plot_gan_loss(train_losses_G, train_losses_D, mae_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sUQWwvNrWsZY",
   "metadata": {
    "id": "sUQWwvNrWsZY"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr, structural_similarity as ssim\n",
    "import community.community_louvain as community_louvain\n",
    "import os\n",
    "def calculate_centralities(adj_matrix):\n",
    "    if adj_matrix.shape[0] != adj_matrix.shape[1]:\n",
    "        raise ValueError(f\"Adjacency matrix is not square: shape={adj_matrix.shape}\")\n",
    "    print(f\"Processing adjacency matrix of shape: {adj_matrix.shape}\")\n",
    "\n",
    "    G = nx.from_numpy_array(adj_matrix)\n",
    "    partition = community_louvain.best_partition(G)\n",
    "\n",
    "    # Calculate the participation coefficient with the partition\n",
    "    pc_dict = participation_coefficient(G, partition)\n",
    "\n",
    "    # Calculate averages of centrality measures\n",
    "    pr = nx.pagerank(G, alpha=0.9)\n",
    "    ec = nx.eigenvector_centrality_numpy(G, max_iter=100)\n",
    "    bc = nx.betweenness_centrality(G, normalized=True, endpoints=False)\n",
    "    ns = np.array(list(nx.degree_centrality(G).values())) * (len(G.nodes()) - 1)\n",
    "    acc = nx.average_clustering(G, weight=None)\n",
    "\n",
    "    # Average participation coefficient\n",
    "    pc_avg = np.mean(list(pc_dict.values()))\n",
    "\n",
    "    return {\n",
    "        'pr': np.mean(list(pr.values())),\n",
    "        'ec': np.mean(list(ec.values())),\n",
    "        'bc': np.mean(list(bc.values())),\n",
    "        'ns': ns,\n",
    "        'pc': pc_avg,\n",
    "        'acc': acc\n",
    "    }\n",
    "\n",
    "def participation_coefficient(G, partition):\n",
    "    # Initialize dictionary for participation coefficients\n",
    "    pc_dict = {}\n",
    "\n",
    "    # Calculate participation coefficient for each node\n",
    "    for node in G.nodes():\n",
    "        node_degree = G.degree(node)\n",
    "        if node_degree == 0:\n",
    "            pc_dict[node] = 0.0\n",
    "        else:\n",
    "            # Count within-module connections\n",
    "            within_module_degree = sum(1 for neighbor in G[node] if partition[neighbor] == partition[node])\n",
    "            # Calculate participation coefficient\n",
    "            pc_dict[node] = 1 - (within_module_degree / node_degree) ** 2\n",
    "\n",
    "    return pc_dict\n",
    "\n",
    "def evaluate_all(true_hr_matrices, predicted_hr_matrices, output_path=path_eval_matrics):\n",
    "    print(true_hr_matrices.shape)\n",
    "    print(predicted_hr_matrices.shape)\n",
    "\n",
    "    num_subjects = true_hr_matrices.shape[0]\n",
    "    results = []\n",
    "\n",
    "    for i in range(num_subjects):\n",
    "        true_matrix = true_hr_matrices[i, :, :]\n",
    "        pred_matrix = predicted_hr_matrices[i, :, :]\n",
    "\n",
    "        print(f\"Evaluating subject {i+1} with matrix shapes: true={true_matrix.shape}, pred={pred_matrix.shape}\")\n",
    "\n",
    "        if true_matrix.shape != pred_matrix.shape or true_matrix.shape[0] != true_matrix.shape[1]:\n",
    "            print(f\"Error: Matrix shape mismatch or not square for subject {i+1}: true={true_matrix.shape}, pred={pred_matrix.shape}\")\n",
    "            continue\n",
    "\n",
    "        metrics = {\n",
    "            'ID': i + 1,\n",
    "            'MAE': mean_absolute_error(true_matrix.flatten(), pred_matrix.flatten()),\n",
    "            'PCC': pearsonr(true_matrix.flatten(), pred_matrix.flatten())[0],\n",
    "            'JSD': jensenshannon(true_matrix.flatten(), pred_matrix.flatten()),\n",
    "        }\n",
    "\n",
    "        true_metrics = calculate_centralities(true_matrix)\n",
    "        pred_metrics = calculate_centralities(pred_matrix)\n",
    "\n",
    "        for key in ['NS', 'PR', 'EC', 'BC', 'PC', 'ACC']:\n",
    "            metrics[f'MAE in {key}'] = mean_absolute_error([true_metrics[key.lower()]], [pred_metrics[key.lower()]])\n",
    "\n",
    "        results.append(metrics)\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    if not df.empty:\n",
    "        # Check if the file exists to decide whether to write headers\n",
    "        file_exists = os.path.isfile(output_path)\n",
    "\n",
    "        df.to_csv(output_path, mode='a', header=not file_exists, index=False)\n",
    "        print(f\"Results appended to {output_path}.\")\n",
    "    else:\n",
    "        print(\"No data to save.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1XIG02N_mGNZ",
   "metadata": {
    "id": "1XIG02N_mGNZ"
   },
   "outputs": [],
   "source": [
    "Y_test = []\n",
    "with open(path_hr_data_test) as f:\n",
    "    next(f)\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        rowData = [ float(elem) for elem in row ]\n",
    "        Y_test.append(rowData)\n",
    "Y_test = np.array(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "C0NUNGQknOTD",
   "metadata": {
    "id": "C0NUNGQknOTD"
   },
   "outputs": [],
   "source": [
    "scipy.io.savemat('HR_data_true_268.mat', {'train':Y_test})\n",
    "hr_data_true_path = './HR_data_true_268.mat'\n",
    "Y_t = loadmat(my_project_folder +hr_data_true_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "okFwgtjVm30e",
   "metadata": {
    "id": "okFwgtjVm30e"
   },
   "outputs": [],
   "source": [
    "Y_true = np.zeros((lentest,268,268))\n",
    "for i in range(lentest):\n",
    "  Y_true[i] = MatrixVectorizer.anti_vectorize(Y_t['train'][i], 268, include_diagonal=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pxcM3ynnWtrr",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pxcM3ynnWtrr",
    "outputId": "efe1ef07-053c-44ca-af81-a4983fb5be27"
   },
   "outputs": [],
   "source": [
    "metrics = evaluate_all(\n",
    "    Y_true, test_preds\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
