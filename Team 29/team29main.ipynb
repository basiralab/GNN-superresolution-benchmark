{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-03-14T08:52:58.202192Z",
     "iopub.status.busy": "2024-03-14T08:52:58.201624Z",
     "iopub.status.idle": "2024-03-14T08:52:58.231772Z",
     "shell.execute_reply": "2024-03-14T08:52:58.230817Z",
     "shell.execute_reply.started": "2024-03-14T08:52:58.202160Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is Dr. Rekik's implementation from her paper in 2021:\n",
    "\n",
    "https://www.sciencedirect.com/science/article/pii/S1361841521001304"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T08:52:58.234240Z",
     "iopub.status.busy": "2024-03-14T08:52:58.233858Z",
     "iopub.status.idle": "2024-03-14T08:53:03.643201Z",
     "shell.execute_reply": "2024-03-14T08:53:03.642390Z",
     "shell.execute_reply.started": "2024-03-14T08:52:58.234207Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import scipy.io\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "from evaluation import *\n",
    "from utils import *\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T08:53:03.644956Z",
     "iopub.status.busy": "2024-03-14T08:53:03.644183Z",
     "iopub.status.idle": "2024-03-14T08:53:03.653689Z",
     "shell.execute_reply": "2024-03-14T08:53:03.652728Z",
     "shell.execute_reply.started": "2024-03-14T08:53:03.644927Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Set a fixed random seed for reproducibility across multiple libraries\n",
    "import random\n",
    "random_seed = 42\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T08:53:03.656331Z",
     "iopub.status.busy": "2024-03-14T08:53:03.655984Z",
     "iopub.status.idle": "2024-03-14T08:53:03.668070Z",
     "shell.execute_reply": "2024-03-14T08:53:03.667047Z",
     "shell.execute_reply.started": "2024-03-14T08:53:03.656305Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class MatrixVectorizer:\n",
    "    \"\"\"\n",
    "    A class for transforming between matrices and vector representations.\n",
    "    \n",
    "    This class provides methods to convert a symmetric matrix into a vector (vectorize)\n",
    "    and to reconstruct the matrix from its vector form (anti_vectorize), focusing on \n",
    "    vertical (column-based) traversal and handling of elements.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes the MatrixVectorizer instance.\n",
    "        \n",
    "        The constructor currently does not perform any actions but is included for \n",
    "        potential future extensions where initialization parameters might be required.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def vectorize(matrix, include_diagonal=False):\n",
    "        \"\"\"\n",
    "        Converts a matrix into a vector by vertically extracting elements.\n",
    "        \n",
    "        This method traverses the matrix column by column, collecting elements from the\n",
    "        upper triangle, and optionally includes the diagonal elements immediately below\n",
    "        the main diagonal based on the include_diagonal flag.\n",
    "        \n",
    "        Parameters:\n",
    "        - matrix (numpy.ndarray): The matrix to be vectorized.\n",
    "        - include_diagonal (bool, optional): Flag to include diagonal elements in the vectorization.\n",
    "          Defaults to False.\n",
    "        \n",
    "        Returns:\n",
    "        - numpy.ndarray: The vectorized form of the matrix.\n",
    "        \"\"\"\n",
    "        # Determine the size of the matrix based on its first dimension\n",
    "        matrix_size = matrix.shape[0]\n",
    "\n",
    "        # Initialize an empty list to accumulate vector elements\n",
    "        vector_elements = []\n",
    "\n",
    "        # Iterate over columns and then rows to collect the relevant elements\n",
    "        for col in range(matrix_size):\n",
    "            for row in range(matrix_size):\n",
    "                # Skip diagonal elements if not including them\n",
    "                if row != col:  \n",
    "                    if row < col:\n",
    "                        # Collect upper triangle elements\n",
    "                        vector_elements.append(matrix[row, col])\n",
    "                    elif include_diagonal and row == col + 1:\n",
    "                        # Optionally include the diagonal elements immediately below the diagonal\n",
    "                        vector_elements.append(matrix[row, col])\n",
    "\n",
    "        return np.array(vector_elements)\n",
    "\n",
    "    @staticmethod\n",
    "    def anti_vectorize(vector, matrix_size, include_diagonal=False):\n",
    "        \"\"\"\n",
    "        Reconstructs a matrix from its vector form, filling it vertically.\n",
    "        \n",
    "        The method fills the matrix by reflecting vector elements into the upper triangle\n",
    "        and optionally including the diagonal elements based on the include_diagonal flag.\n",
    "        \n",
    "        Parameters:\n",
    "        - vector (numpy.ndarray): The vector to be transformed into a matrix.\n",
    "        - matrix_size (int): The size of the square matrix to be reconstructed.\n",
    "        - include_diagonal (bool, optional): Flag to include diagonal elements in the reconstruction.\n",
    "          Defaults to False.\n",
    "        \n",
    "        Returns:\n",
    "        - numpy.ndarray: The reconstructed square matrix.\n",
    "        \"\"\"\n",
    "        # Initialize a square matrix of zeros with the specified size\n",
    "        matrix = np.zeros((matrix_size, matrix_size))\n",
    "\n",
    "        # Index to keep track of the current position in the vector\n",
    "        vector_idx = 0\n",
    "\n",
    "        # Fill the matrix by iterating over columns and then rows\n",
    "        for col in range(matrix_size):\n",
    "            for row in range(matrix_size):\n",
    "                # Skip diagonal elements if not including them\n",
    "                if row != col:  \n",
    "                    if row < col:\n",
    "                        # Reflect vector elements into the upper triangle and its mirror in the lower triangle\n",
    "                        matrix[row, col] = vector[vector_idx]\n",
    "                        matrix[col, row] = vector[vector_idx]\n",
    "                        vector_idx += 1\n",
    "                    elif include_diagonal and row == col + 1:\n",
    "                        # Optionally fill the diagonal elements after completing each column\n",
    "                        matrix[row, col] = vector[vector_idx]\n",
    "                        matrix[col, row] = vector[vector_idx]\n",
    "                        vector_idx += 1\n",
    "\n",
    "        return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T08:53:03.673380Z",
     "iopub.status.busy": "2024-03-14T08:53:03.672644Z",
     "iopub.status.idle": "2024-03-14T08:53:11.192870Z",
     "shell.execute_reply": "2024-03-14T08:53:11.191823Z",
     "shell.execute_reply.started": "2024-03-14T08:53:03.673354Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Path to your .mat files\n",
    "lr_data_path = '../data/lr_train.csv'\n",
    "hr_data_path = '../data/hr_train.csv'\n",
    "lr_data_test_path = '../data/lr_test.csv'\n",
    "\n",
    "# Load the data\n",
    "lr_data = pd.read_csv(lr_data_path)\n",
    "hr_data = pd.read_csv(hr_data_path)\n",
    "\n",
    "# Print basic information about 'LR' and 'HR' variables\n",
    "print(\"LR Data Shape:\", lr_data.shape)\n",
    "print(\"HR Data Shape:\", hr_data.shape)\n",
    "\n",
    "# If the data frames are not too large, you can print a small part of them\n",
    "print(\"Sample from LR Data:\")\n",
    "print(lr_data.head())\n",
    "\n",
    "print(\"Sample from HR Data:\")\n",
    "print(hr_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T08:53:11.194576Z",
     "iopub.status.busy": "2024-03-14T08:53:11.194200Z",
     "iopub.status.idle": "2024-03-14T08:53:11.216068Z",
     "shell.execute_reply": "2024-03-14T08:53:11.214901Z",
     "shell.execute_reply.started": "2024-03-14T08:53:11.194542Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define a function to calculate statistics and return them in a dictionary\n",
    "def calculate_statistics(data):\n",
    "    statistics = {\n",
    "        'Mean': np.mean(data),\n",
    "        'Median': np.median(data),\n",
    "        'Standard Deviation': np.std(data),\n",
    "        'Min': np.min(data),\n",
    "        'Max': np.max(data)\n",
    "    }\n",
    "    return statistics\n",
    "\n",
    "# Extract the first line of LR and HR data as numpy arrays\n",
    "lr_array = lr_data.iloc[0].to_numpy()\n",
    "hr_array = hr_data.iloc[0].to_numpy()\n",
    "\n",
    "# Calculate statistics for LR and HR data\n",
    "lr_stats = calculate_statistics(lr_array)\n",
    "hr_stats = calculate_statistics(hr_array)\n",
    "\n",
    "# Create a DataFrame to hold the statistics for comparison\n",
    "df_stats = pd.DataFrame({'LR Data': lr_stats, 'HR Data': hr_stats})\n",
    "\n",
    "# Round the numbers to four decimal places for better readability\n",
    "df_stats = df_stats.round(4)\n",
    "\n",
    "df_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T08:53:11.219030Z",
     "iopub.status.busy": "2024-03-14T08:53:11.217652Z",
     "iopub.status.idle": "2024-03-14T08:53:11.963719Z",
     "shell.execute_reply": "2024-03-14T08:53:11.962690Z",
     "shell.execute_reply.started": "2024-03-14T08:53:11.218993Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Setting the Seaborn theme for nice aesthetics\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# Plotting histograms on the same figure for comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Making histograms semi-transparent with alpha and using a higher bin count for finer detail\n",
    "sns.histplot(lr_array, bins=100, color='blue', alpha=0.5, label='LR Data')\n",
    "sns.histplot(hr_array, bins=100, color='red', alpha=0.5, label='HR Data')\n",
    "\n",
    "# Adding titles and labels\n",
    "plt.title('Combined Distribution of LR and HR Data')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Adding a legend to differentiate between LR and HR data\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T08:53:11.965510Z",
     "iopub.status.busy": "2024-03-14T08:53:11.965142Z",
     "iopub.status.idle": "2024-03-14T08:53:12.678797Z",
     "shell.execute_reply": "2024-03-14T08:53:12.677650Z",
     "shell.execute_reply.started": "2024-03-14T08:53:11.965479Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Setting the Seaborn theme for aesthetics\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# Plotting histograms on the same figure for comparison, excluding zeros\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Making histograms semi-transparent with alpha and using a higher bin count for finer detail\n",
    "# We filter out the zeros using lr_array[lr_array > 0].flatten() and hr_array[hr_array > 0].flatten()\n",
    "sns.histplot(lr_array[lr_array > 0].flatten(), bins=100, color='blue', alpha=0.5, label='LR Data')\n",
    "sns.histplot(hr_array[hr_array > 0].flatten(), bins=100, color='red', alpha=0.5, label='HR Data')\n",
    "\n",
    "# Adding titles and labels\n",
    "plt.title('Combined Distribution of LR and HR Data (Excluding Zeros)')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Adding a legend to differentiate between LR and HR data\n",
    "plt.legend()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T08:53:12.682695Z",
     "iopub.status.busy": "2024-03-14T08:53:12.682349Z",
     "iopub.status.idle": "2024-03-14T08:53:13.710724Z",
     "shell.execute_reply": "2024-03-14T08:53:13.709617Z",
     "shell.execute_reply.started": "2024-03-14T08:53:12.682653Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "lr_matrix = MatrixVectorizer.anti_vectorize(lr_array, 160)\n",
    "hr_matrix = MatrixVectorizer.anti_vectorize(hr_array, 268)\n",
    "\n",
    "plt.figure(figsize=(20, 8))\n",
    "\n",
    "# Heatmap of a subset of LR data\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(lr_matrix, aspect='auto', cmap='viridis')  # Adjust subset size as needed\n",
    "plt.colorbar()\n",
    "plt.title('LR Data Heatmap')\n",
    "\n",
    "# Heatmap of a subset of HR data\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(hr_matrix, aspect='auto', cmap='viridis')  # Adjust subset size as needed\n",
    "plt.colorbar()\n",
    "plt.title('HR Data Heatmap')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T08:53:13.713194Z",
     "iopub.status.busy": "2024-03-14T08:53:13.712489Z",
     "iopub.status.idle": "2024-03-14T08:53:13.872737Z",
     "shell.execute_reply": "2024-03-14T08:53:13.871535Z",
     "shell.execute_reply.started": "2024-03-14T08:53:13.713158Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def pad_HR_adj(label, split):\n",
    "    \"\"\"\n",
    "    Pads a high-resolution (HR) adjacency matrix with zeros on all sides.\n",
    "\n",
    "    Parameters:\n",
    "    - label (torch.Tensor): The HR adjacency matrix to be padded.\n",
    "    - split (int): The number of zeros to add to each side of the matrix.\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: The padded HR adjacency matrix.\n",
    "    \"\"\"\n",
    "    padded_label = F.pad(label, pad=(split, split, split, split), mode=\"constant\", value=0)\n",
    "    return padded_label\n",
    "\n",
    "\n",
    "def normalize_adj_torch(mx):\n",
    "    rowsum = mx.sum(1)\n",
    "    r_inv_sqrt = torch.pow(rowsum, -0.5).flatten()\n",
    "    r_inv_sqrt[torch.isinf(r_inv_sqrt)] = 0.\n",
    "    r_mat_inv_sqrt = torch.diag(r_inv_sqrt)\n",
    "    mx = torch.matmul(mx, r_mat_inv_sqrt)\n",
    "    mx = torch.transpose(mx, 0, 1)\n",
    "    mx = torch.matmul(mx, r_mat_inv_sqrt)\n",
    "    return mx\n",
    "\n",
    "\n",
    "def unpad(data, split):\n",
    "    idx_0 = data.shape[0]-split\n",
    "    idx_1 = data.shape[1]-split\n",
    "    # print(idx_0,idx_1)\n",
    "    train = data[split:idx_0, split:idx_1]\n",
    "    return train\n",
    "\n",
    "\n",
    "def extract_data(subject, session_str, parcellation_str, subjects_roi):\n",
    "    folder_path = os.path.join(\n",
    "        path, str(subject), session_str, parcellation_str)\n",
    "    roi_data = scipy.io.loadmat(os.path.join(folder_path, roi_str))\n",
    "    roi = roi_data['r']\n",
    "\n",
    "    # Replacing NaN values\n",
    "    col_mean = np.nanmean(roi, axis=0)\n",
    "    inds = np.where(np.isnan(roi))\n",
    "    roi[inds] = 1\n",
    "\n",
    "    # Taking the absolute values of the matrix\n",
    "    roi = np.absolute(roi, dtype=np.float32)\n",
    "\n",
    "    if parcellation_str == 'shen_268':\n",
    "        roi = np.reshape(roi, (1, 268, 268))\n",
    "    else:\n",
    "        roi = np.reshape(roi, (1, 160, 160))\n",
    "\n",
    "    if subject == 25629:\n",
    "        subjects_roi = roi\n",
    "    else:\n",
    "        subjects_roi = np.concatenate((subjects_roi, roi), axis=0)\n",
    "\n",
    "    return subjects_roi\n",
    "\n",
    "\n",
    "def load_data(start_value, end_value):\n",
    "\n",
    "    subjects_label = np.zeros((1, 268, 268))\n",
    "    subjects_adj = np.zeros((1, 160, 160))\n",
    "\n",
    "    for subject in range(start_value, end_value):\n",
    "        subject_path = os.path.join(path, str(subject))\n",
    "\n",
    "        if 'session_1' in os.listdir(subject_path):\n",
    "\n",
    "            subjects_label = extract_data(\n",
    "                subject, 'session_1', 'shen_268', subjects_label)\n",
    "            subjects_adj = extract_data(\n",
    "                subject, 'session_1', 'Dosenbach_160', subjects_adj)\n",
    "\n",
    "    return subjects_adj, subjects_label\n",
    "\n",
    "\n",
    "def data():\n",
    "    subjects_adj, subjects_labels = load_data(25629, 25830)\n",
    "    test_adj_1, test_labels_1 = load_data(25831, 25863)\n",
    "    test_adj_2, test_labels_2 = load_data(30701, 30757)\n",
    "    test_adj = np.concatenate((test_adj_1, test_adj_2), axis=0)\n",
    "    test_labels = np.concatenate((test_labels_1, test_labels_2), axis=0)\n",
    "    return subjects_adj, subjects_labels, test_adj, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T08:53:13.874864Z",
     "iopub.status.busy": "2024-03-14T08:53:13.874464Z",
     "iopub.status.idle": "2024-03-14T08:53:13.907167Z",
     "shell.execute_reply": "2024-03-14T08:53:13.906212Z",
     "shell.execute_reply.started": "2024-03-14T08:53:13.874835Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def weight_variable_glorot(output_dim):\n",
    "\n",
    "    input_dim = output_dim\n",
    "    init_range = np.sqrt(6.0 / (input_dim + output_dim))\n",
    "    initial = np.random.uniform(-init_range, init_range,\n",
    "                                (input_dim, output_dim))\n",
    "\n",
    "    return initial\n",
    "\n",
    "class GraphUnpool(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(GraphUnpool, self).__init__()\n",
    "\n",
    "    def forward(self, A, X, idx):\n",
    "        new_X = torch.zeros([A.shape[0], X.shape[1]])\n",
    "        new_X[idx] = X\n",
    "        return A, new_X\n",
    "\n",
    "\n",
    "class GraphPool(nn.Module):\n",
    "\n",
    "    def __init__(self, k, in_dim):\n",
    "        super(GraphPool, self).__init__()\n",
    "        self.k = k\n",
    "        self.proj = nn.Linear(in_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, A, X):\n",
    "        scores = self.proj(X)\n",
    "        # scores = torch.abs(scores)\n",
    "        scores = torch.squeeze(scores)\n",
    "        scores = self.sigmoid(scores/100)\n",
    "        num_nodes = A.shape[0]\n",
    "        values, idx = torch.topk(scores, int(self.k*num_nodes))\n",
    "        new_X = X[idx, :]\n",
    "        values = torch.unsqueeze(values, -1)\n",
    "        new_X = torch.mul(new_X, values)\n",
    "        A = A[idx, :]\n",
    "        A = A[:, idx]\n",
    "        return A, new_X, idx\n",
    "\n",
    "\n",
    "class GCN(nn.Module):\n",
    "\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(GCN, self).__init__()\n",
    "        self.proj = nn.Linear(in_dim, out_dim)\n",
    "        self.drop = nn.Dropout(p=0)\n",
    "\n",
    "    def forward(self, A, X):\n",
    "        X = self.drop(X)\n",
    "        X = torch.matmul(A, X)\n",
    "        X = self.proj(X)\n",
    "        return X\n",
    "    \n",
    "class GCNGraphSage(nn.Module):\n",
    "\n",
    "    def __init__(self, in_dim, out_dim, sampling=None, k=50):\n",
    "        super(GCNGraphSage, self).__init__()\n",
    "        self.proj = nn.Linear(in_dim, out_dim)\n",
    "        self.drop = nn.Dropout(p=0)\n",
    "        self.sampling = sampling\n",
    "        self.k = k\n",
    "        \n",
    "    def selectKRandom(self, adj):\n",
    "        adj = adj.detach().numpy()\n",
    "        rows, cols = adj.shape\n",
    "        new_adj = np.zeros_like(adj)\n",
    "        for i in range(rows):\n",
    "            indices = np.random.choice(range(cols), size=self.k, replace=False)\n",
    "            new_adj[i][indices] = row[indices]\n",
    "        return torch.tensor(new_adj)\n",
    "    \n",
    "    def selectKDeterministic(self, adj):\n",
    "        adj = adj.detach().numpy()\n",
    "        rows, cols = adj.shape\n",
    "        new_adj = np.zeros_like(adj)\n",
    "        for i in range(rows):\n",
    "            row = adj[i]\n",
    "            indices = np.argsort(arr)[-self.k:][::-1]\n",
    "            new_adj[i][indices] = row[indices]\n",
    "        return torch.tensor(new_adj)\n",
    "    \n",
    "    def selectKProbabilistic(self, adj):\n",
    "        adj = adj.detach().numpy()\n",
    "        rows, cols = adj.shape\n",
    "        new_adj = np.zeros_like(adj)\n",
    "        for i in range(rows):\n",
    "            row = adj[i]\n",
    "            probabilities = row / np.sum(row) \n",
    "            indices = random.choices(range(cols), probabilities, k=self.k)\n",
    "            new_adj[i][indices] = row[indices]\n",
    "        return torch.tensor(new_adj)\n",
    "\n",
    "    def forward(self, A, X):\n",
    "        if self.sampling == \"random\":\n",
    "            A = self.selectKRandom(A)\n",
    "        elif self.sampling == \"deterministic\":\n",
    "            A = self.selectKDeterministic(A)\n",
    "        elif self.sampling == \"probabilistic\":\n",
    "            A = self.selectKProbabilistic(A)\n",
    "        X = self.drop(X)\n",
    "        X = torch.matmul(A, X)\n",
    "        X = self.proj(X)\n",
    "        return X\n",
    "    \n",
    "class Dense(nn.Module):\n",
    "    def __init__(self, n1, n2, args):\n",
    "        super(Dense, self).__init__()\n",
    "        self.weights = torch.nn.Parameter(\n",
    "            torch.FloatTensor(n1, n2), requires_grad=True)\n",
    "        nn.init.normal_(self.weights, mean=args.mean_dense, std=args.std_dense)\n",
    "\n",
    "    def forward(self, x):\n",
    "        np.random.seed(1)\n",
    "        torch.manual_seed(1)\n",
    "\n",
    "        out = torch.mm(x, self.weights)\n",
    "        return out\n",
    "    \n",
    "class GIN(nn.Module):\n",
    "\n",
    "    def __init__(self, in_dim, out_dim, hid_dim):\n",
    "        super(GIN, self).__init__()\n",
    "        self.dense_1 = nn.Linear(in_dim, hid_dim)\n",
    "        self.relu_1 = nn.ReLU(inplace=False)\n",
    "        self.dense_2 = nn.Linear(hid_dim, out_dim)\n",
    "\n",
    "    def forward(self, A, X):\n",
    "        X = torch.matmul(A, X)\n",
    "        X = self.dense_1(X)\n",
    "        X = self.relu_1(X)\n",
    "        X = self.dense_2(X)\n",
    "        return X\n",
    "\n",
    "class GraphUnet(nn.Module):\n",
    "\n",
    "    def __init__(self, ks, in_dim, out_dim, dim=320):\n",
    "        super(GraphUnet, self).__init__()\n",
    "        self.ks = ks\n",
    "\n",
    "        self.start_gcn = GIN(in_dim, dim, dim)\n",
    "        self.bottom_gcn = GIN(dim, dim, dim)\n",
    "        self.end_gcn = GIN(2*dim, out_dim, dim)\n",
    "        self.down_gcns = []\n",
    "        self.up_gcns = []\n",
    "        self.pools = []\n",
    "        self.unpools = []\n",
    "        self.l_n = len(ks)\n",
    "        for i in range(self.l_n):\n",
    "            self.down_gcns.append(GIN(dim, dim, dim))\n",
    "            self.up_gcns.append(GIN(dim, dim, dim))\n",
    "            self.pools.append(GraphPool(ks[i], dim))\n",
    "            self.unpools.append(GraphUnpool())\n",
    "\n",
    "    def forward(self, A, X):\n",
    "        adj_ms = []\n",
    "        indices_list = []\n",
    "        down_outs = []\n",
    "        X = self.start_gcn(A, X)\n",
    "        start_gcn_outs = X\n",
    "        org_X = X\n",
    "        for i in range(self.l_n):\n",
    "\n",
    "            X = self.down_gcns[i](A, X)\n",
    "            adj_ms.append(A)\n",
    "            down_outs.append(X)\n",
    "            A, X, idx = self.pools[i](A, X)\n",
    "            indices_list.append(idx)\n",
    "        X = self.bottom_gcn(A, X)\n",
    "        for i in range(self.l_n):\n",
    "            up_idx = self.l_n - i - 1\n",
    "\n",
    "            A, idx = adj_ms[up_idx], indices_list[up_idx]\n",
    "            A, X = self.unpools[i](A, X, idx)\n",
    "            X = self.up_gcns[i](A, X)\n",
    "            X = X.add(down_outs[up_idx])\n",
    "        X = torch.cat([X, org_X], 1)\n",
    "        X = self.end_gcn(A, X)\n",
    "\n",
    "        return X, start_gcn_outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T08:53:13.909206Z",
     "iopub.status.busy": "2024-03-14T08:53:13.908890Z",
     "iopub.status.idle": "2024-03-14T08:53:13.931825Z",
     "shell.execute_reply": "2024-03-14T08:53:13.930634Z",
     "shell.execute_reply.started": "2024-03-14T08:53:13.909181Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class GSRLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, hr_dim):\n",
    "        super(GSRLayer, self).__init__()\n",
    "\n",
    "        self.weights = torch.from_numpy(\n",
    "            weight_variable_glorot(hr_dim)).type(torch.FloatTensor)\n",
    "        self.weights = torch.nn.Parameter(\n",
    "            data=self.weights, requires_grad=True)\n",
    "\n",
    "    def forward(self, A, X):\n",
    "        with torch.autograd.set_detect_anomaly(True):\n",
    "\n",
    "            lr = A\n",
    "            lr_dim = lr.shape[0]\n",
    "            f = X\n",
    "            #eig_val_lr, U_lr = torch.symeig(lr, eigenvectors=True, upper=True)\n",
    "            eig_val_lr, U_lr = torch.linalg.eigh(lr, UPLO='U')\n",
    "\n",
    "            # U_lr = torch.abs(U_lr)\n",
    "            eye_mat = torch.eye(lr_dim).type(torch.FloatTensor)\n",
    "            s_d = torch.cat((eye_mat, eye_mat), 0)\n",
    "\n",
    "            a = torch.matmul(self.weights, s_d)\n",
    "            b = torch.matmul(a, torch.t(U_lr))\n",
    "            f_d = torch.matmul(b, f)\n",
    "            f_d = torch.abs(f_d)\n",
    "            f_d = f_d.fill_diagonal_(1)\n",
    "            adj = f_d\n",
    "\n",
    "            X = torch.mm(adj, adj.t())\n",
    "            X = (X + X.t())/2\n",
    "            X = X.fill_diagonal_(1)\n",
    "        return adj, torch.abs(X)\n",
    "\n",
    "\n",
    "class GraphConvolution(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, dropout, act=F.relu):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.dropout = dropout\n",
    "        self.act = act\n",
    "        self.weight = torch.nn.Parameter(\n",
    "            torch.FloatTensor(in_features, out_features))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        torch.nn.init.xavier_uniform_(self.weight)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        input = F.dropout(input, self.dropout, self.training)\n",
    "        support = torch.mm(input, self.weight)\n",
    "        output = torch.mm(adj, support)\n",
    "        output = self.act(output)\n",
    "        return output\n",
    "    \n",
    "class GraphSageConvolution(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features, dropout, act=F.relu, sampling=None, k=50):\n",
    "        super(GraphSageConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.dropout = dropout\n",
    "        self.act = act\n",
    "        self.weight = torch.nn.Parameter(\n",
    "            torch.FloatTensor(in_features, out_features))\n",
    "        self.reset_parameters()\n",
    "        self.sampling = sampling\n",
    "        self.k = 50\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        torch.nn.init.xavier_uniform_(self.weight)\n",
    "        \n",
    "    def selectKRandom(self, adj):\n",
    "        adj = adj.detach().numpy()\n",
    "        rows, cols = adj.shape\n",
    "        new_adj = np.zeros_like(adj)\n",
    "        for i in range(rows):\n",
    "            indices = np.random.choice(range(cols), size=self.k, replace=False)\n",
    "            new_adj[i][indices] = row[indices]\n",
    "        return torch.tensor(new_adj)\n",
    "    \n",
    "    def selectKDeterministic(self, adj):\n",
    "        adj = adj.detach().numpy()\n",
    "        rows, cols = adj.shape\n",
    "        new_adj = np.zeros_like(adj)\n",
    "        for i in range(rows):\n",
    "            row = adj[i]\n",
    "            indices = np.argsort(arr)[-self.k:][::-1]\n",
    "            new_adj[i][indices] = row[indices]\n",
    "        return torch.tensor(new_adj)\n",
    "    \n",
    "    def selectKProbabilistic(self, adj):\n",
    "        adj = adj.detach().numpy()\n",
    "        rows, cols = adj.shape\n",
    "        new_adj = np.zeros_like(adj)\n",
    "        for i in range(rows):\n",
    "            row = adj[i]\n",
    "            probabilities = row / np.sum(row) \n",
    "            indices = random.choices(range(cols), probabilities, k=self.k)\n",
    "            new_adj[i][indices] = row[indices]\n",
    "        return torch.tensor(new_adj)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        if self.sampling == \"random\":\n",
    "            adj = self.selectKRandom(adj)\n",
    "        elif self.sampling == \"deterministic\":\n",
    "            adj = self.selectKDeterministic(adj)\n",
    "        elif self.sampling == \"probabilistic\":\n",
    "            adj = self.selectKProbabilistic(adj)\n",
    "        input = F.dropout(input, self.dropout, self.training)\n",
    "        support = torch.mm(input, self.weight)\n",
    "        output = torch.mm(adj, support)\n",
    "        output = self.act(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T08:53:13.933433Z",
     "iopub.status.busy": "2024-03-14T08:53:13.933103Z",
     "iopub.status.idle": "2024-03-14T08:53:13.950758Z",
     "shell.execute_reply": "2024-03-14T08:53:13.949511Z",
     "shell.execute_reply.started": "2024-03-14T08:53:13.933407Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class AGSRNet(nn.Module):\n",
    "\n",
    "    def __init__(self, ks, args):\n",
    "        super(AGSRNet, self).__init__()\n",
    "\n",
    "        self.lr_dim = args.lr_dim\n",
    "        self.hr_dim = args.hr_dim\n",
    "        self.hidden_dim = args.hidden_dim\n",
    "        self.layer = GSRLayer(self.hr_dim)\n",
    "        self.net = GraphUnet(ks, self.lr_dim, self.hr_dim)\n",
    "#         self.gc1 = GraphConvolution(\n",
    "#             self.hr_dim, self.hidden_dim, 0, act=F.relu)\n",
    "#         self.gc2 = GraphConvolution(\n",
    "#             self.hidden_dim, self.hr_dim, 0, act=F.relu)\n",
    "        self.gc1 = GraphSageConvolution(\n",
    "            self.hr_dim, self.hidden_dim, 0, act=F.relu,\n",
    "            sampling=\"probabilistic\", k=args.k)\n",
    "        self.gc2 = GraphSageConvolution(\n",
    "            self.hidden_dim, self.hr_dim, 0, act=F.relu,\n",
    "            sampling=\"probabilistic\", k=args.k)\n",
    "\n",
    "    def forward(self, lr, lr_dim, hr_dim):\n",
    "        with torch.autograd.set_detect_anomaly(True):\n",
    "\n",
    "            I = torch.eye(self.lr_dim).type(torch.FloatTensor)\n",
    "            A = normalize_adj_torch(lr).type(torch.FloatTensor)\n",
    "\n",
    "            self.net_outs, self.start_gcn_outs = self.net(A, I)\n",
    "\n",
    "            self.outputs, self.Z = self.layer(A, self.net_outs)\n",
    "\n",
    "            self.hidden1 = self.gc1(self.Z, self.outputs)\n",
    "            self.hidden2 = self.gc2(self.hidden1, self.outputs)\n",
    "            z = self.hidden2\n",
    "\n",
    "            z = (z + z.t())/2\n",
    "            z = z.fill_diagonal_(1)\n",
    "\n",
    "        return torch.abs(z), self.net_outs, self.start_gcn_outs, self.outputs\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.dense_1 = Dense(args.hr_dim, args.hr_dim, args)\n",
    "        self.relu_1 = nn.ReLU(inplace=False)\n",
    "        self.dense_2 = Dense(args.hr_dim, args.hr_dim, args)\n",
    "        self.relu_2 = nn.ReLU(inplace=False)\n",
    "        self.dense_3 = Dense(args.hr_dim, 1, args)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        np.random.seed(1)\n",
    "        torch.manual_seed(1)\n",
    "        dc_den1 = self.relu_1(self.dense_1(inputs))\n",
    "        dc_den2 = self.relu_2(self.dense_2(dc_den1))\n",
    "        output = dc_den2\n",
    "        output = self.dense_3(dc_den2)\n",
    "        output = self.sigmoid(output)\n",
    "        return torch.abs(output)\n",
    "\n",
    "def gaussian_noise_layer(input_layer, args):\n",
    "    z = torch.empty_like(input_layer)\n",
    "    noise = z.normal_(mean=args.mean_gaussian, std=args.std_gaussian)\n",
    "    z = torch.abs(input_layer + noise)\n",
    "\n",
    "    z = (z + z.t())/2\n",
    "    z = z.fill_diagonal_(1)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T08:53:13.952940Z",
     "iopub.status.busy": "2024-03-14T08:53:13.952561Z",
     "iopub.status.idle": "2024-03-14T08:53:14.117424Z",
     "shell.execute_reply": "2024-03-14T08:53:14.116441Z",
     "shell.execute_reply.started": "2024-03-14T08:53:13.952911Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "criterion = nn.MSELoss()\n",
    "# criterion = mean_absolute_error\n",
    "\n",
    "def train(model, subjects_adj, subjects_labels, args):\n",
    "\n",
    "    bce_loss = nn.BCELoss()\n",
    "    netD = Discriminator(args)\n",
    "    print(netD)\n",
    "    optimizerG = optim.Adam(model.parameters(), lr=args.lr)\n",
    "    optimizerD = optim.Adam(netD.parameters(), lr=args.lr)\n",
    "\n",
    "    all_epochs_loss = []\n",
    "    for epoch in range(args.epochs):\n",
    "        with torch.autograd.set_detect_anomaly(True):\n",
    "            epoch_loss = []\n",
    "            epoch_error = []\n",
    "            for lr, hr in zip(subjects_adj, subjects_labels):\n",
    "                optimizerD.zero_grad()\n",
    "                optimizerG.zero_grad()\n",
    "\n",
    "                hr = pad_HR_adj(hr, args.padding)\n",
    "                lr = torch.from_numpy(lr).type(torch.FloatTensor)\n",
    "                padded_hr = torch.from_numpy(hr).type(torch.FloatTensor)\n",
    "\n",
    "                #eig_val_hr, U_hr = torch.symeig(padded_hr, eigenvectors=True, upper=True)\n",
    "                eig_val_hr, U_hr = torch.linalg.eigh(padded_hr, UPLO='U')\n",
    "\n",
    "                model_outputs, net_outs, start_gcn_outs, layer_outs = model(\n",
    "                    lr, args.lr_dim, args.hr_dim)\n",
    "\n",
    "                mse_loss = args.lmbda * criterion(net_outs, start_gcn_outs) + criterion(\n",
    "                    model.layer.weights, U_hr) + criterion(model_outputs, padded_hr)\n",
    "\n",
    "                error = criterion(model_outputs, padded_hr)\n",
    "                real_data = model_outputs.detach()\n",
    "                fake_data = gaussian_noise_layer(padded_hr, args)\n",
    "\n",
    "                d_real = netD(real_data)\n",
    "                d_fake = netD(fake_data)\n",
    "\n",
    "                dc_loss_real = bce_loss(d_real, torch.ones(args.hr_dim, 1))\n",
    "                dc_loss_fake = bce_loss(d_fake, torch.zeros(args.hr_dim, 1))\n",
    "                dc_loss = dc_loss_real + dc_loss_fake\n",
    "\n",
    "                dc_loss.backward()\n",
    "                optimizerD.step()\n",
    "\n",
    "                d_fake = netD(gaussian_noise_layer(padded_hr, args))\n",
    "\n",
    "                gen_loss = bce_loss(d_fake, torch.ones(args.hr_dim, 1))\n",
    "                generator_loss = gen_loss + mse_loss\n",
    "                generator_loss.backward()\n",
    "                optimizerG.step()\n",
    "\n",
    "                epoch_loss.append(generator_loss.item())\n",
    "                epoch_error.append(error.item())\n",
    "\n",
    "            print(\"Epoch: \", epoch, \"Loss: \", np.mean(epoch_loss),\n",
    "                  \"Error: \", np.mean(epoch_error)*100, \"%\")\n",
    "            all_epochs_loss.append(np.mean(epoch_loss))\n",
    "            # test(model, lr_val_matrices, hr_val_matrices, args)\n",
    "\n",
    "\n",
    "def test(model, test_adj, test_labels, args):\n",
    "\n",
    "    g_t = []\n",
    "    test_error = []\n",
    "    preds_list = []\n",
    "\n",
    "    # i = 0\n",
    "\n",
    "    for lr, hr in zip(test_adj, test_labels):\n",
    "        all_zeros_lr = not np.any(lr)\n",
    "        all_zeros_hr = not np.any(hr)\n",
    "        if all_zeros_lr == False and all_zeros_hr == False:\n",
    "            lr = torch.from_numpy(lr).type(torch.FloatTensor)\n",
    "            np.fill_diagonal(hr, 1)\n",
    "#             hr = pad_HR_adj(hr, args.padding)\n",
    "            hr = torch.from_numpy(hr).type(torch.FloatTensor)\n",
    "            preds, a, b, c = model(lr, args.lr_dim, args.hr_dim)\n",
    "            preds = unpad(preds, args.padding)\n",
    "\n",
    "            preds_list.append(preds.flatten().detach().numpy())\n",
    "#             error = criterion(preds, hr)\n",
    "            error = mean_absolute_error(preds.flatten().detach().numpy(), hr.flatten().detach().numpy())\n",
    "            g_t.append(hr.flatten())\n",
    "#             print(error.item())\n",
    "            test_error.append(error.item())\n",
    "            # i += 1\n",
    "\n",
    "    print(\"Test error MAE: \", np.mean(test_error))\n",
    "    return np.mean(test_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "def train(model, subjects_adj, subjects_labels, args):\n",
    "\n",
    "    bce_loss = nn.BCELoss()\n",
    "    netD = Discriminator(args)\n",
    "    optimizerG = optim.Adam(model.parameters(), lr=args.lr)\n",
    "    optimizerD = optim.Adam(netD.parameters(), lr=args.lr)\n",
    "\n",
    "    all_epochs_loss = []\n",
    "    for epoch in range(args.epochs):\n",
    "        with torch.autograd.set_detect_anomaly(True):\n",
    "            epoch_loss = []\n",
    "            epoch_error = []\n",
    "            for lr, hr in zip(subjects_adj, subjects_labels):\n",
    "                optimizerD.zero_grad()\n",
    "                optimizerG.zero_grad()\n",
    "                \n",
    "                # Pad HR adjacency matrix and ensure it is a PyTorch tensor\n",
    "                hr_padded = pad_HR_adj(hr, args.padding)\n",
    "                padded_hr = hr_padded.type(torch.FloatTensor)\n",
    "    \n",
    "\n",
    "                # Use torch.linalg.eigh() instead of deprecated torch.symeig()\n",
    "                eig_val_hr, U_hr = torch.linalg.eigh(padded_hr, UPLO='U')\n",
    "\n",
    "                model_outputs, net_outs, start_gcn_outs, layer_outs = model(\n",
    "                    lr, args.lr_dim, args.hr_dim)\n",
    "\n",
    "                mse_loss = args.lmbda * criterion(net_outs, start_gcn_outs) + criterion(\n",
    "                    model.layer.weights, U_hr) + criterion(model_outputs, padded_hr)\n",
    "\n",
    "                error = criterion(model_outputs, padded_hr)\n",
    "                real_data = model_outputs.detach()\n",
    "                fake_data = gaussian_noise_layer(padded_hr, args)\n",
    "\n",
    "                d_real = netD(real_data)\n",
    "                d_fake = netD(fake_data)\n",
    "\n",
    "                dc_loss_real = bce_loss(d_real, torch.ones(args.hr_dim, 1))\n",
    "                dc_loss_fake = bce_loss(d_fake, torch.zeros(args.hr_dim, 1))\n",
    "                dc_loss = dc_loss_real + dc_loss_fake\n",
    "\n",
    "                dc_loss.backward()\n",
    "                optimizerD.step()\n",
    "\n",
    "                d_fake = netD(gaussian_noise_layer(padded_hr, args))\n",
    "\n",
    "                gen_loss = bce_loss(d_fake, torch.ones(args.hr_dim, 1))\n",
    "                generator_loss = gen_loss + mse_loss\n",
    "                generator_loss.backward()\n",
    "                optimizerG.step()\n",
    "\n",
    "                epoch_loss.append(generator_loss.item())\n",
    "                epoch_error.append(error.item())\n",
    "\n",
    "            print(\"Epoch: \", epoch, \"Loss: \", np.mean(epoch_loss),\n",
    "                  \"Error: \", np.mean(epoch_error)*100, \"%\", flush=True) \n",
    "            all_epochs_loss.append(np.mean(epoch_loss))\n",
    "    return all_epochs_loss\n",
    "\n",
    "\n",
    "def train_validation(model, subjects_adj, subjects_labels, val_adj, val_labels, args):\n",
    "\n",
    "    bce_loss = nn.BCELoss()\n",
    "    netD = Discriminator(args)\n",
    "    optimizerG = optim.Adam(model.parameters(), lr=args.lr)\n",
    "    optimizerD = optim.Adam(netD.parameters(), lr=args.lr)\n",
    "\n",
    "    all_epochs_loss = []\n",
    "    all_epochs_val_loss = []\n",
    "\n",
    "    # Early stopping parameters\n",
    "    best_val_mae = float('inf')\n",
    "    patience = 10\n",
    "    counter = 0\n",
    "    for epoch in range(args.epochs):\n",
    "        with torch.autograd.set_detect_anomaly(True):\n",
    "            epoch_loss = []\n",
    "            epoch_error = []\n",
    "            for lr, hr in zip(subjects_adj, subjects_labels):\n",
    "                optimizerD.zero_grad()\n",
    "                optimizerG.zero_grad()\n",
    "                \n",
    "                # Pad HR adjacency matrix and ensure it is a PyTorch tensor\n",
    "                hr_padded = pad_HR_adj(hr, args.padding)\n",
    "                padded_hr = hr_padded.type(torch.FloatTensor)\n",
    "    \n",
    "\n",
    "                # Use torch.linalg.eigh() instead of deprecated torch.symeig()\n",
    "                eig_val_hr, U_hr = torch.linalg.eigh(padded_hr, UPLO='U')\n",
    "\n",
    "                model_outputs, net_outs, start_gcn_outs, layer_outs = model(\n",
    "                    lr, args.lr_dim, args.hr_dim)\n",
    "\n",
    "                mse_loss = args.lmbda * criterion(net_outs, start_gcn_outs) + criterion(\n",
    "                    model.layer.weights, U_hr) + criterion(model_outputs, padded_hr)\n",
    "\n",
    "                error = criterion(model_outputs, padded_hr)\n",
    "                real_data = model_outputs.detach()\n",
    "                fake_data = gaussian_noise_layer(padded_hr, args)\n",
    "\n",
    "                d_real = netD(real_data)\n",
    "                d_fake = netD(fake_data)\n",
    "\n",
    "                dc_loss_real = bce_loss(d_real, torch.ones(args.hr_dim, 1))\n",
    "                dc_loss_fake = bce_loss(d_fake, torch.zeros(args.hr_dim, 1))\n",
    "                dc_loss = dc_loss_real + dc_loss_fake\n",
    "\n",
    "                dc_loss.backward()\n",
    "                optimizerD.step()\n",
    "\n",
    "                d_fake = netD(gaussian_noise_layer(padded_hr, args))\n",
    "\n",
    "                gen_loss = bce_loss(d_fake, torch.ones(args.hr_dim, 1))\n",
    "                generator_loss = gen_loss + mse_loss\n",
    "                generator_loss.backward()\n",
    "                optimizerG.step()\n",
    "\n",
    "                epoch_loss.append(generator_loss.item())\n",
    "                epoch_error.append(error.item())\n",
    "\n",
    "            all_epochs_loss.append(np.mean(epoch_loss))\n",
    "\n",
    "            val_mae = test(model, val_adj, val_labels, args) # Test on validation set\n",
    "            all_epochs_val_loss.append(val_mae)\n",
    "\n",
    "            print(\"Epoch: \", epoch, \"Loss: \", np.mean(epoch_loss),\n",
    "                  \"Error: \", np.mean(epoch_error)*100, \"%\", \"Validation MAE: \", val_mae, flush=True)  # Error now represents MAE\n",
    "            if val_mae < best_val_mae:\n",
    "                best_val_mae = val_mae\n",
    "                counter = 0\n",
    "            else:\n",
    "                counter += 1\n",
    "                if counter >= patience and epoch > 100:\n",
    "                    print(\"Early stopping at epoch \", epoch)\n",
    "                    return all_epochs_loss, all_epochs_val_loss\n",
    "    return all_epochs_loss, all_epochs_val_loss\n",
    "\n",
    "\n",
    "def test(model, test_adj, test_labels, args):\n",
    "\n",
    "    g_t = []\n",
    "    test_error = []\n",
    "    preds_list = []\n",
    "\n",
    "    for lr, hr in zip(test_adj, test_labels):\n",
    "        #all_zeros_lr = not np.any(lr)\n",
    "        all_zeros_lr = not torch.any(lr)\n",
    "        #all_zeros_hr = not np.any(hr)\n",
    "        all_zeros_hr = not torch.any(hr)\n",
    "        if all_zeros_lr == False and all_zeros_hr == False:\n",
    "            #lr = torch.from_numpy(lr).type(torch.FloatTensor)\n",
    "            #np.fill_diagonal(hr, 1)\n",
    "            hr = hr.fill_diagonal_(1)\n",
    "\n",
    "            hr = pad_HR_adj(hr, args.padding)\n",
    "            preds, a, b, c = model(lr, args.lr_dim, args.hr_dim)\n",
    "\n",
    "            preds_list.append(preds.flatten().detach().numpy())\n",
    "            error = criterion(preds, hr)\n",
    "            g_t.append(hr.flatten())\n",
    "            test_error.append(error.item())\n",
    "\n",
    "    print(\"Test error MAE: \", np.mean(test_error), flush=True)  # Changed MSE to MAE in print statement\n",
    "    return np.mean(test_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T08:53:14.126359Z",
     "iopub.status.busy": "2024-03-14T08:53:14.126045Z",
     "iopub.status.idle": "2024-03-14T08:53:14.146073Z",
     "shell.execute_reply": "2024-03-14T08:53:14.144893Z",
     "shell.execute_reply.started": "2024-03-14T08:53:14.126335Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def save_predictions(vectorized_predictions, fold_num=None):\n",
    "    melted = np.array(vectorized_predictions).flatten()\n",
    "    predictions_df = pd.DataFrame({'ID': np.arange(1, len(melted) + 1), 'Predicted': melted})\n",
    "    if not fold_num:\n",
    "        predictions_df.to_csv(f\"predictions_test.csv\", index=False)\n",
    "    else:\n",
    "        predictions_df.to_csv(f\"predictions_fold_{fold_num}.csv\", index=False)\n",
    "\n",
    "def evaluate_predictions(pred_matrices, gt_matrices):\n",
    "    \"\"\"\n",
    "    Evaluate predictions using various metrics.\n",
    "\n",
    "    Args:\n",
    "    pred_matrices (numpy array): Predicted high-resolution matrices.\n",
    "    gt_matrices (numpy array): Ground truth high-resolution matrices.\n",
    "    Both arguements of the form:\n",
    "     [[ Vectorized form of high-resolution matrix 1 ],\n",
    "      [ Vectorized form of high-resolution matrix 2 ],\n",
    "      ...\n",
    "      [ Vectorized form of high-resolution matrix n ]]\n",
    "\n",
    "    Returns:\n",
    "    dict: Dictionary containing evaluation measures.\n",
    "    \"\"\"\n",
    "    # Initialize lists to store MAEs for each centrality measure\n",
    "    mae_bc = []\n",
    "    mae_ec = []\n",
    "    mae_pc = []\n",
    "\n",
    "    num_test_samples = len(pred_matrices)\n",
    "    \n",
    "    for i in range(num_test_samples):\n",
    "        print(i)\n",
    "        pred = matrix_vectorizer.anti_vectorize(pred_matrices[i], 268)\n",
    "        gt = matrix_vectorizer.anti_vectorize(gt_matrices[i], 268)\n",
    "\n",
    "       # Convert adjacency matrices to NetworkX graphs\n",
    "        pred_graph = nx.from_numpy_array(pred, edge_attr=\"weight\")\n",
    "        gt_graph = nx.from_numpy_array(gt, edge_attr=\"weight\")\n",
    "\n",
    "        # Compute centrality measures\n",
    "        pred_bc = nx.betweenness_centrality(pred_graph, weight=\"weight\")\n",
    "        pred_ec = nx.eigenvector_centrality(pred_graph, weight=\"weight\")\n",
    "        pred_pc = nx.pagerank(pred_graph, weight=\"weight\")\n",
    "\n",
    "        gt_bc = nx.betweenness_centrality(gt_graph, weight=\"weight\")\n",
    "        gt_ec = nx.eigenvector_centrality(gt_graph, weight=\"weight\")\n",
    "        gt_pc = nx.pagerank(gt_graph, weight=\"weight\")\n",
    "\n",
    "        # Convert centrality dictionaries to lists\n",
    "        pred_bc_values = list(pred_bc.values())\n",
    "        pred_ec_values = list(pred_ec.values())\n",
    "        pred_pc_values = list(pred_pc.values())\n",
    "\n",
    "        gt_bc_values = list(gt_bc.values())\n",
    "        gt_ec_values = list(gt_ec.values())\n",
    "        gt_pc_values = list(gt_pc.values())\n",
    "\n",
    "        # Compute MAEs\n",
    "        mae_bc.append(mean_absolute_error(pred_bc_values, gt_bc_values))\n",
    "        mae_ec.append(mean_absolute_error(pred_ec_values, gt_ec_values))\n",
    "        mae_pc.append(mean_absolute_error(pred_pc_values, gt_pc_values))\n",
    "\n",
    "    # Compute average MAEs\n",
    "    avg_mae_bc = sum(mae_bc) / len(mae_bc)\n",
    "    avg_mae_ec = sum(mae_ec) / len(mae_ec)\n",
    "    avg_mae_pc = sum(mae_pc) / len(mae_pc)\n",
    "\n",
    "    # Concatenate flattened matrices\n",
    "    pred_1d = np.concatenate(pred_matrices)\n",
    "    gt_1d = np.concatenate(gt_matrices)\n",
    "\n",
    "    # Compute metrics\n",
    "    mae = mean_absolute_error(pred_1d, gt_1d)\n",
    "    pcc = pearsonr(pred_1d, gt_1d)[0]\n",
    "    js_dis = jensenshannon(pred_1d, gt_1d)\n",
    "\n",
    "    # Construct dictionary of evaluation measures\n",
    "    measures = {\n",
    "        \"MAE\": mae,\n",
    "        \"PCC\": pcc,\n",
    "        \"JSD\": js_dis,\n",
    "        \"MAE (PC)\": avg_mae_pc,\n",
    "        \"MAE (BC)\": avg_mae_bc,\n",
    "        \"MAE (EC)\": avg_mae_ec\n",
    "    }\n",
    "\n",
    "    return measures\n",
    "    \n",
    "def plot_evaluation_measures(measures):\n",
    "    \"\"\"\n",
    "    Measures is a list of dictionary where the dictionary holds each measure\n",
    "    And each dictionary corresponds to 1 fold of cross validation\n",
    "    \"\"\"\n",
    "    folds = len(measures) # should be 3\n",
    "    keys = list(measures[0].keys())\n",
    "    #TODO: feel free to change these colours\n",
    "    colours = [\"red\",\"orange\",\"yellow\",\"cyan\",\"blue\",\"purple\"]\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "    \n",
    "    for i in range(folds):\n",
    "        ax = axes[i // 2, i % 2]\n",
    "        ax.bar(range(len(keys)), list(measures[i].values()), color=colours)\n",
    "        ax.set_xticks(range(len(keys)), keys)\n",
    "        ax.set_title(f\"Fold {i+1}\")\n",
    "\n",
    "    # Calculate mean and standard deviation over folds\n",
    "    values = np.array([[d[key] for key in keys] for d in measures])\n",
    "    mean = np.mean(values, axis=0)\n",
    "    std_dev = np.std(values, axis=0)\n",
    "\n",
    "    ax = axes[1, 1]\n",
    "    ax.bar(range(len(keys)), mean, color=colours)\n",
    "    # Plot the error bars\n",
    "    ax.errorbar(range(len(keys)), mean, yerr=std_dev, fmt='none', capsize=5, elinewidth=1, markeredgewidth=1, ecolor='k')\n",
    "    ax.set_xticks(range(len(keys)), keys)\n",
    "    ax.set_title(\"Average across folds\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "#     plt.savefig(\"metrics_plot1.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T08:53:14.148957Z",
     "iopub.status.busy": "2024-03-14T08:53:14.147544Z",
     "iopub.status.idle": "2024-03-14T08:53:14.159823Z",
     "shell.execute_reply": "2024-03-14T08:53:14.159012Z",
     "shell.execute_reply.started": "2024-03-14T08:53:14.148919Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def plot_evaluation_measures2(measures):\n",
    "    \"\"\"\n",
    "    Measures is a list of dictionaries where each dictionary holds each measure.\n",
    "    Each dictionary corresponds to 1 fold of cross-validation.\n",
    "    \"\"\"\n",
    "\n",
    "    keys = list(measures[0].keys())\n",
    "    folds = len(measures)  # should be 3\n",
    "\n",
    "    # TODO: feel free to change these colors\n",
    "    colors = [\"red\", \"orange\", \"yellow\", \"cyan\", \"blue\", \"purple\"]\n",
    "\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(12, 8))\n",
    "\n",
    "    for metric_idx, metric in enumerate(keys):\n",
    "        \n",
    "        ax = axes[metric_idx // 3, metric_idx % 3]\n",
    "        values = [d[metric] for d in measures]\n",
    "\n",
    "        # Plot the bars for each fold\n",
    "        x = np.arange(folds)\n",
    "        ax.bar(x, values, color=colors[metric_idx])\n",
    "        \n",
    "        # Calculate mean and std.dev\n",
    "        mean = np.mean(values)\n",
    "        std_dev = np.std(values)\n",
    "        ax.bar(folds, mean, color=colors[metric_idx])\n",
    "        ax.errorbar(folds, mean, yerr=std_dev, fmt='none', capsize=5, elinewidth=1, markeredgewidth=1, ecolor='k')\n",
    "\n",
    "        ax.set_xticks(np.arange(folds + 1))\n",
    "        tick_labels = [f\"Fold {i + 1}\" for i in range(folds)]\n",
    "        tick_labels.append(\"Avg\")\n",
    "        ax.set_xticklabels(tick_labels)\n",
    "        ax.set_xlabel('Folds')\n",
    "        ax.set_title(metric)\n",
    "\n",
    "    plt.tight_layout()\n",
    "#     plt.savefig(\"metrics_plot2.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RandomCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLIT_1_LR_PATH = 'RandomCV/Train/Fold1/lr_split_1.csv'\n",
    "SPLIT_1_HR_PATH = 'RandomCV/Train/Fold1/hr_split_1.csv'\n",
    "SPLIT_2_LR_PATH = 'RandomCV/Train/Fold2/lr_split_2.csv'\n",
    "SPLIT_2_HR_PATH = 'RandomCV/Train/Fold2/hr_split_2.csv'\n",
    "SPLIT_3_LR_PATH = 'RandomCV/Train/Fold3/lr_split_3.csv'\n",
    "SPLIT_3_HR_PATH = 'RandomCV/Train/Fold3/hr_split_3.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_output(args, test_adj, model):\n",
    "    outputs = []\n",
    "    model.eval()\n",
    "\n",
    "    for lr_graph in test_adj:\n",
    "            #output = model(lr_graph)\n",
    "            model_outputs, net_outs, start_gcn_outs, layer_outs = model(lr_graph, args.lr_dim, args.hr_dim)\n",
    "            #model_outputs = unpad(model_outputs, padding)\n",
    "            #unpad and refactorize this\n",
    "            idx_0 = model_outputs.shape[0]-26\n",
    "            idx_1 = model_outputs.shape[1]-26\n",
    "            model_outputs = model_outputs[26:idx_0, 26:idx_1]\n",
    "            # append clipped outputs clipped between 0 and 1\n",
    "            outputs.append(model_outputs.detach().numpy())\n",
    "\n",
    "    outputs = np.array(outputs)\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T08:53:20.174420Z",
     "iopub.status.busy": "2024-03-14T08:53:20.174066Z",
     "iopub.status.idle": "2024-03-14T08:53:20.181003Z",
     "shell.execute_reply": "2024-03-14T08:53:20.179862Z",
     "shell.execute_reply.started": "2024-03-14T08:53:20.174390Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self, **kwargs):\n",
    "        for key, value in kwargs.items():\n",
    "            setattr(self, key, value)\n",
    "# from paper\n",
    "args = Args(\n",
    "    epochs=400, #down from 200\n",
    "    lr=0.0001,\n",
    "    lmbda=0.1,\n",
    "    lr_dim=160,\n",
    "    hr_dim=320,\n",
    "    hidden_dim=320,\n",
    "    padding=26,\n",
    "    mean_dense=0,\n",
    "    std_dense=0.01,\n",
    "    mean_gaussian=0,\n",
    "    std_gaussian=0.1,\n",
    "    k = 50\n",
    ")\n",
    "\n",
    "ks = [0.9, 0.7, 0.6, 0.5]\n",
    "matrix_vectorizer = MatrixVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T08:53:20.183008Z",
     "iopub.status.busy": "2024-03-14T08:53:20.182586Z",
     "iopub.status.idle": "2024-03-14T12:39:24.338339Z",
     "shell.execute_reply": "2024-03-14T12:39:24.337094Z",
     "shell.execute_reply.started": "2024-03-14T08:53:20.182974Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Set seeds\n",
    "SEED = 42\n",
    "GET_METRICS = True\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Load Data\n",
    "split_1_adj, split_1_ground_truth = load_matrix_data(SPLIT_1_LR_PATH, SPLIT_1_HR_PATH, 93)\n",
    "split_2_adj, split_2_ground_truth = load_matrix_data(SPLIT_2_LR_PATH, SPLIT_2_HR_PATH, 93)\n",
    "split_3_adj, split_3_ground_truth = load_matrix_data(SPLIT_3_LR_PATH, SPLIT_3_HR_PATH, 93)\n",
    "\n",
    "train_losses_all_with_val = []\n",
    "val_losses_all = []\n",
    "train_losses_all_no_val = []\n",
    "fold_results = []\n",
    "\n",
    "# Run 3-fold CV\n",
    "for i in range(3):\n",
    "    print(f\"Fold {i+1}:\")\n",
    "    \n",
    "    # Determine train, validation, and test splits\n",
    "    if i == 0:\n",
    "        train_adj = torch.cat((split_2_adj[:-20], split_3_adj[:-20]), dim=0)\n",
    "        train_ground_truth = torch.cat((split_2_ground_truth[:-20], split_3_ground_truth[:-20]), dim=0)\n",
    "        val_adj = torch.cat((split_2_adj[-20:], split_3_adj[-20:]), dim=0)\n",
    "        val_ground_truth = torch.cat((split_2_ground_truth[-20:], split_3_ground_truth[-20:]), dim=0)\n",
    "        test_adj = split_1_adj\n",
    "        test_ground_truth = split_1_ground_truth\n",
    "    elif i == 1:\n",
    "        train_adj = torch.cat((split_1_adj[:-20], split_3_adj[:-20]), dim=0)\n",
    "        train_ground_truth = torch.cat((split_1_ground_truth[:-20], split_3_ground_truth[:-20]), dim=0)\n",
    "        val_adj = torch.cat((split_1_adj[-20:], split_3_adj[-20:]), dim=0)\n",
    "        val_ground_truth = torch.cat((split_1_ground_truth[-20:], split_3_ground_truth[-20:]), dim=0)\n",
    "        test_adj = split_2_adj\n",
    "        test_ground_truth = split_2_ground_truth\n",
    "    else:\n",
    "        train_adj = torch.cat((split_1_adj[:-20], split_2_adj[:-20]), dim=0)\n",
    "        train_ground_truth = torch.cat((split_1_ground_truth[:-20], split_2_ground_truth[:-20]), dim=0)\n",
    "        val_adj = torch.cat((split_1_adj[-20:], split_2_adj[-20:]), dim=0)\n",
    "        val_ground_truth = torch.cat((split_1_ground_truth[-20:], split_2_ground_truth[-20:]), dim=0)\n",
    "        test_adj = split_3_adj\n",
    "        test_ground_truth = split_3_ground_truth\n",
    "    \n",
    "    # Find early stopping epoch\n",
    "    model = AGSRNet(ks, args)\n",
    "    lr = 0.0001\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    train_losses, val_losses = train_validation(model, train_adj, train_ground_truth, val_adj, val_ground_truth, args)\n",
    "    train_losses_all_with_val.append(train_losses)\n",
    "    val_losses_all.append(val_losses)\n",
    "    num_epochs = len(train_losses)\n",
    "    \n",
    "    # Retrain model on full training set (without validation)\n",
    "    full_train_adj = torch.cat((train_adj, val_adj), dim=0)\n",
    "    full_train_ground_truth = torch.cat((train_ground_truth, val_ground_truth), dim=0)\n",
    "    \n",
    "    model = AGSRNet(ks, args)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    args.epochs = num_epochs\n",
    "    train_losses = train(model, full_train_adj, full_train_ground_truth, args)\n",
    "    train_losses_all_no_val.append(train_losses)\n",
    "    \n",
    "    # Get metrics for the left-out fold\n",
    "    test_outputs = compute_output(args, test_adj, model)\n",
    "    metrics = evaluate_all(test_ground_truth.detach().numpy(), test_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create plots for each fold\n",
    "for i in range(3):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    #plt.plot(train_losses_all[i], label='Training Loss')\n",
    "    plt.plot(val_losses_all[i], label='Validation MAE')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f'Fold {i+1} - Validation MAE')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create plots for each fold\n",
    "for i in range(3):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses_all_with_val[i], label='Training Loss (with validation)')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f'Fold {i+1} - Training Loss')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create plots for each fold\n",
    "for i in range(3):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses_all_no_val[i], label='Training Loss (No Validation)')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f'Fold {i+1} - Training Loss')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "identity_df = pd.read_csv('29-randomCV_old.csv', index_col=0, header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "identity_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add row averaging and std columns except for the first column and top row\n",
    "identity_df.loc['mean'] = identity_df.mean()\n",
    "identity_df.loc['std'] = identity_df.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "identity_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dataframe to a csv file\n",
    "identity_df.to_csv('29-randomCV.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ClusterCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLIT_1_LR_PATH = 'Cluster-CV/Fold1/lr_clusterA.csv'\n",
    "SPLIT_1_HR_PATH = 'Cluster-CV/Fold1/hr_clusterA.csv'\n",
    "SPLIT_2_LR_PATH = 'Cluster-CV/Fold2/lr_clusterB.csv'\n",
    "SPLIT_2_HR_PATH = 'Cluster-CV/Fold2/hr_clusterB.csv'\n",
    "SPLIT_3_LR_PATH = 'Cluster-CV/Fold3/lr_clusterC.csv'\n",
    "SPLIT_3_HR_PATH = 'Cluster-CV/Fold3/hr_clusterC.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_output(args, test_adj, model):\n",
    "    outputs = []\n",
    "    model.eval()\n",
    "\n",
    "    for lr_graph in test_adj:\n",
    "            #output = model(lr_graph)\n",
    "            model_outputs, net_outs, start_gcn_outs, layer_outs = model(lr_graph, args.lr_dim, args.hr_dim)\n",
    "            #model_outputs = unpad(model_outputs, padding)\n",
    "            #unpad and refactorize this\n",
    "            idx_0 = model_outputs.shape[0]-26\n",
    "            idx_1 = model_outputs.shape[1]-26\n",
    "            model_outputs = model_outputs[26:idx_0, 26:idx_1]\n",
    "            # append clipped outputs clipped between 0 and 1\n",
    "            outputs.append(model_outputs.detach().numpy())\n",
    "\n",
    "    outputs = np.array(outputs)\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self, **kwargs):\n",
    "        for key, value in kwargs.items():\n",
    "            setattr(self, key, value)\n",
    "# from paper\n",
    "args = Args(\n",
    "    epochs=400, #down from 200\n",
    "    lr=0.0001,\n",
    "    lmbda=0.1,\n",
    "    lr_dim=160,\n",
    "    hr_dim=320,\n",
    "    hidden_dim=320,\n",
    "    padding=26,\n",
    "    mean_dense=0,\n",
    "    std_dense=0.01,\n",
    "    mean_gaussian=0,\n",
    "    std_gaussian=0.1,\n",
    "    k = 50\n",
    ")\n",
    "\n",
    "ks = [0.9, 0.7, 0.6, 0.5]\n",
    "matrix_vectorizer = MatrixVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seeds\n",
    "SEED = 42\n",
    "GET_METRICS = True\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Load Data\n",
    "split_1_adj, split_1_ground_truth = load_matrix_data(SPLIT_1_LR_PATH, SPLIT_1_HR_PATH, 103)\n",
    "split_2_adj, split_2_ground_truth = load_matrix_data(SPLIT_2_LR_PATH, SPLIT_2_HR_PATH, 103)\n",
    "split_3_adj, split_3_ground_truth = load_matrix_data(SPLIT_3_LR_PATH, SPLIT_3_HR_PATH, 76)\n",
    "\n",
    "train_losses_all_with_val = []\n",
    "val_losses_all = []\n",
    "train_losses_all_no_val = []\n",
    "fold_results = []\n",
    "\n",
    "# Run 3-fold CV\n",
    "for i in range(3):\n",
    "    print(f\"Fold {i+1}:\")\n",
    "    \n",
    "    # Determine train, validation, and test splits\n",
    "    if i == 0:\n",
    "        train_adj = torch.cat((split_2_adj[:-20], split_3_adj[:-20]), dim=0)\n",
    "        train_ground_truth = torch.cat((split_2_ground_truth[:-20], split_3_ground_truth[:-20]), dim=0)\n",
    "        val_adj = torch.cat((split_2_adj[-20:], split_3_adj[-20:]), dim=0)\n",
    "        val_ground_truth = torch.cat((split_2_ground_truth[-20:], split_3_ground_truth[-20:]), dim=0)\n",
    "        test_adj = split_1_adj\n",
    "        test_ground_truth = split_1_ground_truth\n",
    "    elif i == 1:\n",
    "        train_adj = torch.cat((split_1_adj[:-20], split_3_adj[:-20]), dim=0)\n",
    "        train_ground_truth = torch.cat((split_1_ground_truth[:-20], split_3_ground_truth[:-20]), dim=0)\n",
    "        val_adj = torch.cat((split_1_adj[-20:], split_3_adj[-20:]), dim=0)\n",
    "        val_ground_truth = torch.cat((split_1_ground_truth[-20:], split_3_ground_truth[-20:]), dim=0)\n",
    "        test_adj = split_2_adj\n",
    "        test_ground_truth = split_2_ground_truth\n",
    "    else:\n",
    "        train_adj = torch.cat((split_1_adj[:-20], split_2_adj[:-20]), dim=0)\n",
    "        train_ground_truth = torch.cat((split_1_ground_truth[:-20], split_2_ground_truth[:-20]), dim=0)\n",
    "        val_adj = torch.cat((split_1_adj[-20:], split_2_adj[-20:]), dim=0)\n",
    "        val_ground_truth = torch.cat((split_1_ground_truth[-20:], split_2_ground_truth[-20:]), dim=0)\n",
    "        test_adj = split_3_adj\n",
    "        test_ground_truth = split_3_ground_truth\n",
    "    \n",
    "    # Find early stopping epoch\n",
    "    model = AGSRNet(ks, args)\n",
    "    lr = 0.0001\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    train_losses, val_losses = train_validation(model, train_adj, train_ground_truth, val_adj, val_ground_truth, args)\n",
    "    train_losses_all_with_val.append(train_losses)\n",
    "    val_losses_all.append(val_losses)\n",
    "    num_epochs = len(train_losses)\n",
    "    \n",
    "    # Retrain model on full training set (without validation)\n",
    "    full_train_adj = torch.cat((train_adj, val_adj), dim=0)\n",
    "    full_train_ground_truth = torch.cat((train_ground_truth, val_ground_truth), dim=0)\n",
    "    \n",
    "    model = AGSRNet(ks, args)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    args.epochs = num_epochs\n",
    "    train_losses = train(model, full_train_adj, full_train_ground_truth, args)\n",
    "    train_losses_all_no_val.append(train_losses)\n",
    "    \n",
    "    # Get metrics for the left-out fold\n",
    "    test_outputs = compute_output(args, test_adj, model)\n",
    "    metrics = evaluate_all(test_ground_truth.detach().numpy(), test_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create plots for each fold\n",
    "for i in range(3):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    #plt.plot(train_losses_all[i], label='Training Loss')\n",
    "    plt.plot(val_losses_all[i], label='Validation MAE')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f'Fold {i+1} - Validation MAE')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create plots for each fold\n",
    "for i in range(3):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses_all_with_val[i], label='Training Loss (with validation)')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f'Fold {i+1} - Training Loss')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create plots for each fold\n",
    "for i in range(3):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses_all_no_val[i], label='Training Loss (No Validation)')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f'Fold {i+1} - Training Loss')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "identity_df = pd.read_csv('ID-randomCV.csv', index_col=0, header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "identity_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add row averaging and std columns except for the first column and top row\n",
    "identity_df.loc['mean'] = identity_df.mean()\n",
    "identity_df.loc['std'] = identity_df.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "identity_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dataframe to a csv file\n",
    "identity_df.to_csv('clusterCV.csv')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 7783828,
     "sourceId": 71243,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30646,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
