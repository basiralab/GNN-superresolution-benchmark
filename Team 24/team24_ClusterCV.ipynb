{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Zgt2s2BVQIv"
   },
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 28864,
     "status": "ok",
     "timestamp": 1716511332350,
     "user": {
      "displayName": "Wyal Xon",
      "userId": "14101716046877690904"
     },
     "user_tz": -60
    },
    "id": "FW7kekrOAFOt",
    "outputId": "c398fcb4-9e06-4903-f2de-94a4481f944c"
   },
   "outputs": [],
   "source": [
    "# Ran this notebook in google colab\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Add this in a Google Colab cell to install the correct version of Pytorch Geometric.\n",
    "import torch\n",
    "\n",
    "def format_pytorch_version(version):\n",
    "  return version.split('+')[0]\n",
    "\n",
    "TORCH_version = torch.__version__\n",
    "TORCH = format_pytorch_version(TORCH_version)\n",
    "\n",
    "def format_cuda_version(version):\n",
    "  return 'cu' + version.replace('.', '')\n",
    "\n",
    "CUDA_version = torch.version.cuda\n",
    "CUDA = format_cuda_version(CUDA_version)\n",
    "\n",
    "!pip install torch-scatter -f https://data.pyg.org/whl/torch-{TORCH}+{CUDA}.html\n",
    "!pip install torch-sparse -f https://data.pyg.org/whl/torch-{TORCH}+{CUDA}.html\n",
    "!pip install torch-cluster -f https://data.pyg.org/whl/torch-{TORCH}+{CUDA}.html\n",
    "!pip install torch-spline-conv -f https://data.pyg.org/whl/torch-{TORCH}+{CUDA}.html\n",
    "!pip install torch-geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 626
    },
    "executionInfo": {
     "elapsed": 28552,
     "status": "ok",
     "timestamp": 1716511360892,
     "user": {
      "displayName": "Wyal Xon",
      "userId": "14101716046877690904"
     },
     "user_tz": -60
    },
    "id": "vsmWLet8ilIR",
    "outputId": "737558f6-bc25-47a1-ad8a-9e29f9a8166a"
   },
   "outputs": [],
   "source": [
    "!pip uninstall -y networkx\n",
    "!pip uninstall -y python-louvain\n",
    "!pip uninstall -y community\n",
    "!pip install python-louvain\n",
    "!pip install networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 203
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1716511360892,
     "user": {
      "displayName": "Wyal Xon",
      "userId": "14101716046877690904"
     },
     "user_tz": -60
    },
    "id": "13wgd1WZiobR",
    "outputId": "90bc9c95-2565-4a1a-953f-061ca73a530f"
   },
   "outputs": [],
   "source": [
    "import community as community_louvain\n",
    "\n",
    "community_louvain.best_partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1716511360892,
     "user": {
      "displayName": "Wyal Xon",
      "userId": "14101716046877690904"
     },
     "user_tz": -60
    },
    "id": "MenMsoQV9Scv"
   },
   "outputs": [],
   "source": [
    "# Added imports\n",
    "import sys\n",
    "import os\n",
    "py_file_location = \"/content\"\n",
    "sys.path.append(os.path.abspath(py_file_location))\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "import torch\n",
    "import networkx as nx\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.utils.convert import from_networkx, to_networkx\n",
    "\n",
    "from MatrixVectorizer import MatrixVectorizer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr, structural_similarity as ssim\n",
    "import community as community_louvain\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fyUtwxNzVU8_"
   },
   "source": [
    "# Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1716511361327,
     "user": {
      "displayName": "Wyal Xon",
      "userId": "14101716046877690904"
     },
     "user_tz": -60
    },
    "id": "2yyeEEghIcOp"
   },
   "outputs": [],
   "source": [
    "def add_self_connections(A):\n",
    "    \"\"\"\n",
    "    Add self-connections to the adjacency matrix.\n",
    "\n",
    "    This function adds an identity matrix to the adjacency matrix `A`, which\n",
    "    effectively adds a self-loop to each node in the graph. This is a common\n",
    "    preprocessing step in graph neural network implementations.\n",
    "\n",
    "    Parameters:\n",
    "    A (np.ndarray): The adjacency matrix to modify.\n",
    "\n",
    "    Returns:\n",
    "    np.ndarray: The adjacency matrix with self-connections added.\n",
    "    \"\"\"\n",
    "    I = torch.Tensor(np.eye(A.shape[0]))\n",
    "    return A + I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1716511361327,
     "user": {
      "displayName": "Wyal Xon",
      "userId": "14101716046877690904"
     },
     "user_tz": -60
    },
    "id": "v6S4Tsx_QUNo"
   },
   "outputs": [],
   "source": [
    "def compute_ground_truth_metrics(gt_matrices):\n",
    "    num_test_samples = gt_matrices.shape[0]\n",
    "\n",
    "    # Initialize list to store metrics\n",
    "    gt_metrics = []\n",
    "    # Iterate over each test sample\n",
    "    for i in range(num_test_samples):\n",
    "        # Convert adjacency matrices to NetworkX graphs\n",
    "        gt_graph = nx.from_numpy_array(MatrixVectorizer.anti_vectorize(gt_matrices[i], 268), edge_attr=\"weight\")\n",
    "\n",
    "        # Compute centrality measures\n",
    "        gt_bc = nx.betweenness_centrality(gt_graph, weight=\"weight\")\n",
    "        gt_ec = nx.eigenvector_centrality(gt_graph, weight=\"weight\")\n",
    "        gt_pc = nx.pagerank(gt_graph, weight=\"weight\")\n",
    "\n",
    "        gt_metrics.append((gt_bc, gt_ec, gt_pc))\n",
    "\n",
    "    return gt_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1716511361327,
     "user": {
      "displayName": "Wyal Xon",
      "userId": "14101716046877690904"
     },
     "user_tz": -60
    },
    "id": "-Sm0BBLYGAKK"
   },
   "outputs": [],
   "source": [
    "def calculate_centralities(adj_matrix):\n",
    "    if adj_matrix.shape[0] != adj_matrix.shape[1]:\n",
    "        raise ValueError(f\"Adjacency matrix is not square: shape={adj_matrix.shape}\")\n",
    "    print(f\"Processing adjacency matrix of shape: {adj_matrix.shape}\")\n",
    "\n",
    "    G = nx.from_numpy_array(adj_matrix)\n",
    "    partition = community_louvain.best_partition(G)\n",
    "\n",
    "    # Calculate the participation coefficient with the partition\n",
    "    pc_dict = participation_coefficient(G, partition)\n",
    "\n",
    "    # Calculate averages of centrality measures\n",
    "    pr = nx.pagerank(G, alpha=0.9)\n",
    "    ec = nx.eigenvector_centrality_numpy(G, max_iter=100)\n",
    "    bc = nx.betweenness_centrality(G, normalized=True, endpoints=False)\n",
    "    ns = np.array(list(nx.degree_centrality(G).values())) * (len(G.nodes()) - 1)\n",
    "    acc = nx.average_clustering(G, weight=None)\n",
    "\n",
    "    # Average participation coefficient\n",
    "    pc_avg = np.mean(list(pc_dict.values()))\n",
    "\n",
    "    return {\n",
    "        'pr': np.mean(list(pr.values())),\n",
    "        'ec': np.mean(list(ec.values())),\n",
    "        'bc': np.mean(list(bc.values())),\n",
    "        'ns': ns,\n",
    "        'pc': pc_avg,\n",
    "        'acc': acc\n",
    "    }\n",
    "\n",
    "def participation_coefficient(G, partition):\n",
    "    # Initialize dictionary for participation coefficients\n",
    "    pc_dict = {}\n",
    "\n",
    "    # Calculate participation coefficient for each node\n",
    "    for node in G.nodes():\n",
    "        node_degree = G.degree(node)\n",
    "        if node_degree == 0:\n",
    "            pc_dict[node] = 0.0\n",
    "        else:\n",
    "            # Count within-module connections\n",
    "            within_module_degree = sum(1 for neighbor in G[node] if partition[neighbor] == partition[node])\n",
    "            # Calculate participation coefficient\n",
    "            pc_dict[node] = 1 - (within_module_degree / node_degree) ** 2\n",
    "\n",
    "    return pc_dict\n",
    "\n",
    "\n",
    "def evaluate_all(true_hr_matrices, predicted_hr_matrices, output_path='ID-randomCV.csv'):\n",
    "    print(true_hr_matrices.shape)\n",
    "    print(predicted_hr_matrices.shape)\n",
    "\n",
    "    num_subjects = true_hr_matrices.shape[0]\n",
    "    results = []\n",
    "\n",
    "    for i in range(num_subjects):\n",
    "        true_matrix = true_hr_matrices[i, :, :]\n",
    "        pred_matrix = predicted_hr_matrices[i, :, :]\n",
    "\n",
    "        print(f\"Evaluating subject {i+1} with matrix shapes: true={true_matrix.shape}, pred={pred_matrix.shape}\")\n",
    "\n",
    "        if true_matrix.shape != pred_matrix.shape or true_matrix.shape[0] != true_matrix.shape[1]:\n",
    "            print(f\"Error: Matrix shape mismatch or not square for subject {i+1}: true={true_matrix.shape}, pred={pred_matrix.shape}\")\n",
    "            continue\n",
    "\n",
    "        metrics = {\n",
    "            'ID': i + 1,\n",
    "            'MAE': mean_absolute_error(true_matrix.flatten(), pred_matrix.flatten()),\n",
    "            'PCC': pearsonr(true_matrix.flatten(), pred_matrix.flatten())[0],\n",
    "            'JSD': jensenshannon(true_matrix.flatten(), pred_matrix.flatten()),\n",
    "        }\n",
    "\n",
    "        true_metrics = calculate_centralities(true_matrix)\n",
    "        pred_metrics = calculate_centralities(pred_matrix)\n",
    "\n",
    "        for key in ['NS', 'PR', 'EC', 'BC', 'PC', 'ACC']:\n",
    "            metrics[f'MAE in {key}'] = mean_absolute_error([true_metrics[key.lower()]], [pred_metrics[key.lower()]])\n",
    "\n",
    "        results.append(metrics)\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    if not df.empty:\n",
    "        # Check if the file exists to decide whether to write headers\n",
    "        file_exists = os.path.isfile(output_path)\n",
    "\n",
    "        df.to_csv(output_path, mode='a', header=not file_exists, index=False)\n",
    "        print(f\"Results appended to {output_path}.\")\n",
    "    else:\n",
    "        print(\"No data to save.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "abFCseO0VXN1"
   },
   "source": [
    "# Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1716511361327,
     "user": {
      "displayName": "Wyal Xon",
      "userId": "14101716046877690904"
     },
     "user_tz": -60
    },
    "id": "cfKvrKn4ISkQ",
    "outputId": "73a6830d-0911-4d5f-8451-d3ffe10a765d"
   },
   "outputs": [],
   "source": [
    "# Set a fixed random seed for reproducibility across multiple libraries\n",
    "random_seed = 42\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "# Check for CUDA (GPU support) and set device accordingly\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"CUDA is available. Using GPU.\")\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed)  # For multi-GPU setups\n",
    "    # Additional settings for ensuring reproducibility on CUDA\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDA not available. Using CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SaU6FZ-1VaGL"
   },
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1716511361327,
     "user": {
      "displayName": "Wyal Xon",
      "userId": "14101716046877690904"
     },
     "user_tz": -60
    },
    "id": "dzbT0rNqV_u9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# lr_train_data = pd.read_csv(\"data/lr_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1716511361327,
     "user": {
      "displayName": "Wyal Xon",
      "userId": "14101716046877690904"
     },
     "user_tz": -60
    },
    "id": "90lSsJ_RVmYy"
   },
   "outputs": [],
   "source": [
    "# hr_train_data = pd.read_csv(\"data/hr_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1716511361327,
     "user": {
      "displayName": "Wyal Xon",
      "userId": "14101716046877690904"
     },
     "user_tz": -60
    },
    "id": "cBixTu0JGAKN"
   },
   "outputs": [],
   "source": [
    "def prepare_train_data_for_GSR(data, embedding_dim=160):\n",
    "  data_list = []\n",
    "  for i, g in enumerate(data.values):\n",
    "    adjacency_matrix = add_self_connections(torch.Tensor(MatrixVectorizer.anti_vectorize(g, embedding_dim)))\n",
    "\n",
    "    data_list.append(adjacency_matrix.numpy())\n",
    "\n",
    "  return np.array(data_list)\n",
    "\n",
    "# lr_data_list = prepare_train_data_for_GSR(lr_train_data, embedding_dim=160)\n",
    "# hr_data_list = prepare_train_data_for_GSR(hr_train_data, embedding_dim=268)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1716511361327,
     "user": {
      "displayName": "Wyal Xon",
      "userId": "14101716046877690904"
     },
     "user_tz": -60
    },
    "id": "TNbmiPDUQUNt"
   },
   "outputs": [],
   "source": [
    "# ground_truth_metrics = compute_ground_truth_metrics(hr_train_data.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1716511361328,
     "user": {
      "displayName": "Wyal Xon",
      "userId": "14101716046877690904"
     },
     "user_tz": -60
    },
    "id": "7WRs5yxwYxg0"
   },
   "outputs": [],
   "source": [
    "# MatrixVectorizer.anti_vectorize(lr_train_data.values[0], 160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1716511361328,
     "user": {
      "displayName": "Wyal Xon",
      "userId": "14101716046877690904"
     },
     "user_tz": -60
    },
    "id": "l5Xz7_QwIyMF"
   },
   "outputs": [],
   "source": [
    "# add_self_connections(torch.Tensor(MatrixVectorizer.anti_vectorize(lr_train_data.values[0], 160)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2PhMhgloGAKO"
   },
   "source": [
    "# Define function to get evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1716511361328,
     "user": {
      "displayName": "Wyal Xon",
      "userId": "14101716046877690904"
     },
     "user_tz": -60
    },
    "id": "ZysDQJbOGAKO"
   },
   "outputs": [],
   "source": [
    "def get_evaluation_metrics(pred_matrices, gt_matrices, computed_gt_metrics=None, graph_metrics=True):\n",
    "    \"\"\"\n",
    "    Calculate evaluation metrics for predicted and ground truth matrices.\n",
    "\n",
    "    Args:\n",
    "    pred_matrices (np.ndarray): Array of predicted adjacency matrices.\n",
    "    gt_matrices (np.ndarray): Array of ground truth adjacency matrices.\n",
    "    computed_gt_metrics (list, optional): List of precomputed ground truth centrality metrics.\n",
    "    Defaults to None.\n",
    "    graph_metrics (bool, optional): Flag to indicate whether to compute individual graph centrality metrics.\n",
    "    Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing the following evaluation metrics:\n",
    "        - Mean Absolute Error (MAE) between predicted and ground truth matrices.\n",
    "        - Pearson Correlation Coefficient (PCC) between predicted and ground truth matrices.\n",
    "        - Jensen-Shannon Distance (JSD) between predicted and ground truth matrices.\n",
    "        - Average MAE for betweenness centrality.\n",
    "        - Average MAE for eigenvector centrality.\n",
    "        - Average MAE for PageRank centrality.\n",
    "    \"\"\"\n",
    "\n",
    "    num_test_samples = gt_matrices.shape[0]\n",
    "    num_roi = 268\n",
    "    pred_1d_list = []\n",
    "    gt_1d_list = []\n",
    "    for i in range(num_test_samples):\n",
    "        # Vectorize matrices\n",
    "        pred_1d_list.append(MatrixVectorizer.vectorize(pred_matrices[i]))\n",
    "        gt_1d_list.append(MatrixVectorizer.vectorize(gt_matrices[i]))\n",
    "\n",
    "    # Concatenate flattened matrices\n",
    "    pred_1d = np.concatenate(pred_1d_list)\n",
    "    gt_1d = np.concatenate(gt_1d_list)\n",
    "\n",
    "    # Compute metrics\n",
    "    mae = mean_absolute_error(pred_1d, gt_1d)\n",
    "    pcc = pearsonr(pred_1d, gt_1d)[0]\n",
    "    js_dis = jensenshannon(pred_1d, gt_1d)\n",
    "\n",
    "    print(\"MAE: \", mae)\n",
    "    print(\"PCC: \", pcc)\n",
    "    print(\"Jensen-Shannon Distance: \", js_dis)\n",
    "\n",
    "    # If individual metrics are not requested, return the global metrics only to save time\n",
    "    if not graph_metrics:\n",
    "        return mae, pcc, js_dis\n",
    "\n",
    "    # Initialize lists to store MAEs for each centrality measure\n",
    "    mae_bc = []\n",
    "    mae_ec = []\n",
    "    mae_pc = []\n",
    "\n",
    "    # Iterate over each test sample\n",
    "    for i in range(num_test_samples):\n",
    "        # Convert adjacency matrices to NetworkX graphs\n",
    "        pred_graph = nx.from_numpy_array(pred_matrices[i], edge_attr=\"weight\")\n",
    "\n",
    "        # Compute centrality measures\n",
    "        pred_bc = nx.betweenness_centrality(pred_graph, weight=\"weight\")\n",
    "        pred_ec = nx.eigenvector_centrality(pred_graph, weight=\"weight\")\n",
    "        pred_pc = nx.pagerank(pred_graph, weight=\"weight\")\n",
    "\n",
    "        if computed_gt_metrics is None:\n",
    "            gt_graph = nx.from_numpy_array(gt_matrices[i], edge_attr=\"weight\")\n",
    "            gt_bc = nx.betweenness_centrality(gt_graph, weight=\"weight\")\n",
    "            gt_ec = nx.eigenvector_centrality(gt_graph, weight=\"weight\")\n",
    "            gt_pc = nx.pagerank(gt_graph, weight=\"weight\")\n",
    "        else:\n",
    "            gt_bc, gt_ec, gt_pc = computed_gt_metrics[i]\n",
    "\n",
    "        # Convert centrality dictionaries to lists\n",
    "        pred_bc_values = list(pred_bc.values())\n",
    "        pred_ec_values = list(pred_ec.values())\n",
    "        pred_pc_values = list(pred_pc.values())\n",
    "\n",
    "        gt_bc_values = list(gt_bc.values())\n",
    "        gt_ec_values = list(gt_ec.values())\n",
    "        gt_pc_values = list(gt_pc.values())\n",
    "\n",
    "        # Compute MAEs\n",
    "        mae_bc.append(mean_absolute_error(pred_bc_values, gt_bc_values))\n",
    "        mae_ec.append(mean_absolute_error(pred_ec_values, gt_ec_values))\n",
    "        mae_pc.append(mean_absolute_error(pred_pc_values, gt_pc_values))\n",
    "\n",
    "    # Compute average MAEs\n",
    "    avg_mae_bc = sum(mae_bc) / len(mae_bc)\n",
    "    avg_mae_ec = sum(mae_ec) / len(mae_ec)\n",
    "    avg_mae_pc = sum(mae_pc) / len(mae_pc)\n",
    "\n",
    "    print(\"Average MAE betweenness centrality:\", avg_mae_bc)\n",
    "    print(\"Average MAE eigenvector centrality:\", avg_mae_ec)\n",
    "    print(\"Average MAE PageRank centrality:\", avg_mae_pc)\n",
    "\n",
    "    return mae, pcc, js_dis, avg_mae_bc, avg_mae_ec, avg_mae_pc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yb7JDnLKdMie"
   },
   "source": [
    "# Create submodels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BF7n72OgGAKO"
   },
   "source": [
    "In this section we define all the building blocks of the Graph2Graph model and then use them to create the final architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2rqLasHodOd3"
   },
   "source": [
    "## Part 1: Define GraphUNet and GSRLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1716511361328,
     "user": {
      "displayName": "Wyal Xon",
      "userId": "14101716046877690904"
     },
     "user_tz": -60
    },
    "id": "EzVbj3v7GAKO"
   },
   "outputs": [],
   "source": [
    "def weight_variable_glorot(output_dim):\n",
    "    \"\"\"\n",
    "    Initialize weights using Glorot initialization.\n",
    "\n",
    "    Args:\n",
    "    output_dim (int): The dimension of the output.\n",
    "\n",
    "    Returns:\n",
    "    np.ndarray: An array of weights initialized using Glorot initialization.\n",
    "    \"\"\"\n",
    "\n",
    "    input_dim = output_dim\n",
    "    init_range = np.sqrt(6.0 / (input_dim + output_dim))\n",
    "    initial = np.random.uniform(-init_range, init_range,\n",
    "                                (input_dim, output_dim))\n",
    "\n",
    "    return initial\n",
    "\n",
    "def normalize_adj_torch(mx):\n",
    "    \"\"\"\n",
    "    Normalize the input adjacency matrix using symmetric normalization.\n",
    "\n",
    "    Args:\n",
    "    mx (torch.Tensor): Input adjacency matrix.\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: Normalized adjacency matrix.\n",
    "    \"\"\"\n",
    "    rowsum = mx.sum(1)\n",
    "    r_inv_sqrt = torch.pow(rowsum, -0.5).flatten()\n",
    "    r_inv_sqrt[torch.isinf(r_inv_sqrt)] = 0.\n",
    "    r_mat_inv_sqrt = torch.diag(r_inv_sqrt)\n",
    "    mx = torch.matmul(mx, r_mat_inv_sqrt)\n",
    "    mx = torch.transpose(mx, 0, 1)\n",
    "    mx = torch.matmul(mx, r_mat_inv_sqrt)\n",
    "    return mx\n",
    "\n",
    "class GSRLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Graph Super-Resolution Layer.\n",
    "\n",
    "    Args:\n",
    "    hr_dim (int): The dimension of the high-resolution input.\n",
    "\n",
    "    Attributes:\n",
    "    weights (torch.nn.Parameter): Learnable weights for the layer.\n",
    "\n",
    "    Methods:\n",
    "    forward(A, X, device=None): Performs the forward pass of the layer.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing the following:\n",
    "        - The normalized adjacency matrix.\n",
    "        - The processed high-resolution input.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hr_dim):\n",
    "        super(GSRLayer, self).__init__()\n",
    "\n",
    "        self.weights = torch.from_numpy(\n",
    "            weight_variable_glorot(hr_dim)).type(torch.FloatTensor)\n",
    "        self.weights = torch.nn.Parameter(\n",
    "            data=self.weights, requires_grad=True)\n",
    "\n",
    "    def forward(self, A, X, device=None):\n",
    "        \"\"\"\n",
    "        Perform the forward pass of the Graph Super-Resolution Layer.\n",
    "\n",
    "        Args:\n",
    "        A (torch.Tensor): Input adjacency matrix.\n",
    "        X (torch.Tensor): Input high-resolution data.\n",
    "        device (torch.device, optional): The device to be used for computation. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "        tuple: A tuple containing the following:\n",
    "            - The normalized adjacency matrix.\n",
    "            - The processed high-resolution input.\n",
    "        \"\"\"\n",
    "\n",
    "        lr = A\n",
    "        lr_dim = lr.shape[0]\n",
    "        f = X\n",
    "        eig_val_lr, U_lr = torch.linalg.eigh(lr, UPLO='U')\n",
    "\n",
    "        # U_lr = torch.abs(U_lr)\n",
    "        eye_mat = torch.eye(lr_dim, device=device).type(torch.FloatTensor)\n",
    "        s_d = torch.cat((eye_mat, eye_mat), 0)\n",
    "\n",
    "        if device is not None:\n",
    "            s_d = s_d.to(device)\n",
    "            U_lr = U_lr.to(device)\n",
    "\n",
    "        a = torch.matmul(self.weights, s_d)\n",
    "        b = torch.matmul(a, torch.t(U_lr))\n",
    "        f_d = torch.matmul(b, f)\n",
    "        f_d = torch.abs(f_d)\n",
    "        f_d = f_d.fill_diagonal_(1)\n",
    "        adj = normalize_adj_torch(f_d)\n",
    "\n",
    "        X = torch.mm(adj, adj.t())\n",
    "        X = (X + X.t())/2\n",
    "        X = X.fill_diagonal_(1)\n",
    "        return adj, torch.abs(X)\n",
    "\n",
    "class GraphConvolution(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, dropout, act=F.relu):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.dropout = dropout\n",
    "        self.act = act\n",
    "        self.weight = torch.nn.Parameter(\n",
    "            torch.FloatTensor(in_features, out_features))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        torch.nn.init.xavier_uniform_(self.weight)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        \"\"\"\n",
    "        Perform the forward pass of the GraphConvolution layer.\n",
    "\n",
    "        Args:\n",
    "        input (torch.Tensor): Input feature matrix.\n",
    "        adj (torch.Tensor): Input adjacency matrix.\n",
    "\n",
    "        Returns:\n",
    "        torch.Tensor: The output of the GraphConvolution layer.\n",
    "        \"\"\"\n",
    "        if self.training and self.dropout > 0:\n",
    "            input = F.dropout(input, self.dropout, self.training)\n",
    "        support = torch.mm(input, self.weight)\n",
    "        output = torch.mm(adj, support)\n",
    "        if self.act is not None:\n",
    "            output = self.act(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1716511361328,
     "user": {
      "displayName": "Wyal Xon",
      "userId": "14101716046877690904"
     },
     "user_tz": -60
    },
    "id": "O0dhKt3LGAKP"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class GraphUnpool(nn.Module):\n",
    "    \"\"\"\n",
    "    Graph Unpooling Layer.\n",
    "\n",
    "    Methods:\n",
    "    forward(A, X, idx, device=None): Performs the forward pass of the layer.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing the following:\n",
    "        - The original adjacency matrix.\n",
    "        - The unpooling result of the input feature matrix.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(GraphUnpool, self).__init__()\n",
    "\n",
    "    def forward(self, A, X, idx, device=None):\n",
    "        \"\"\"\n",
    "        Perform the forward pass of the Graph Unpooling Layer.\n",
    "\n",
    "        Args:\n",
    "        A (torch.Tensor): Input adjacency matrix.\n",
    "        X (torch.Tensor): Input feature matrix.\n",
    "        idx (torch.Tensor): Indices for unpooling.\n",
    "        device (torch.device, optional): The device to be used for computation. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "        tuple: A tuple containing the following:\n",
    "            - The original adjacency matrix.\n",
    "            - The unpooling result of the input feature matrix.\n",
    "        \"\"\"\n",
    "        new_X = torch.zeros([A.shape[0], X.shape[1]], device=device)\n",
    "        new_X[idx] = X\n",
    "        return A, new_X\n",
    "\n",
    "class GraphPool(nn.Module):\n",
    "    \"\"\"\n",
    "    Graph Pooling Layer.\n",
    "\n",
    "    Args:\n",
    "    k (float): The ratio of nodes to keep after pooling.\n",
    "    in_dim (int): The dimension of the input feature matrix.\n",
    "\n",
    "    Methods:\n",
    "    forward(A, X): Performs the forward pass of the layer.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing the following:\n",
    "        - The pooled adjacency matrix.\n",
    "        - The pooled feature matrix.\n",
    "        - The indices of the pooled nodes.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, k, in_dim):\n",
    "        super(GraphPool, self).__init__()\n",
    "        self.k = k\n",
    "        self.proj = nn.Linear(in_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, A, X):\n",
    "        \"\"\"\n",
    "        Perform the forward pass of the Graph Pooling Layer.\n",
    "\n",
    "        Args:\n",
    "        A (torch.Tensor): Input adjacency matrix.\n",
    "        X (torch.Tensor): Input feature matrix.\n",
    "\n",
    "        Returns:\n",
    "        tuple: A tuple containing the following:\n",
    "            - The pooled adjacency matrix.\n",
    "            - The pooled feature matrix.\n",
    "            - The indices of the pooled nodes.\n",
    "        \"\"\"\n",
    "        scores = self.proj(X)\n",
    "        # scores = torch.abs(scores)\n",
    "        scores = torch.squeeze(scores)\n",
    "        scores = self.sigmoid(scores/100)\n",
    "        num_nodes = A.shape[0]\n",
    "        values, idx = torch.topk(scores, int(self.k*num_nodes))\n",
    "        new_X = X[idx, :]\n",
    "        values = torch.unsqueeze(values, -1)\n",
    "        new_X = torch.mul(new_X, values)\n",
    "        A = A[idx, :]\n",
    "        A = A[:, idx]\n",
    "        return A, new_X, idx\n",
    "\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple Graph Convolutional Network (GCN) layer.\n",
    "\n",
    "    Args:\n",
    "    in_dim (int): The dimension of the input feature matrix.\n",
    "    out_dim (int): The dimension of the output feature matrix.\n",
    "\n",
    "    Methods:\n",
    "    forward(A, X, device=None): Performs the forward pass of the layer.\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: The output of the GCN layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(GCN, self).__init__()\n",
    "        self.proj = nn.Linear(in_dim, out_dim)\n",
    "        self.drop = nn.Dropout(p=0)\n",
    "\n",
    "    def forward(self, A, X, device=None):\n",
    "        \"\"\"\n",
    "        Perform the forward pass of the Graph Convolutional Network (GCN) layer.\n",
    "\n",
    "        Args:\n",
    "        A (torch.Tensor): Input adjacency matrix.\n",
    "        X (torch.Tensor): Input feature matrix.\n",
    "        device (torch.device, optional): The device to be used for computation. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "        torch.Tensor: The output of the GCN layer.\n",
    "        \"\"\"\n",
    "        if device is not None:\n",
    "            A = A.to(device)\n",
    "            X = X.to(device)\n",
    "        X = self.drop(X)\n",
    "        X = torch.matmul(A, X)\n",
    "        X = self.proj(X)\n",
    "        return X\n",
    "\n",
    "\n",
    "class GraphUnet(nn.Module):\n",
    "    \"\"\"\n",
    "    GraphUnet model for graph data.\n",
    "\n",
    "    Args:\n",
    "    ks (list): List of ratios for graph pooling.\n",
    "    in_dim (int): The dimension of the input feature matrix.\n",
    "    out_dim (int): The dimension of the output feature matrix.\n",
    "    dim (int): The dimension parameter. Defaults to 320.\n",
    "\n",
    "    Methods:\n",
    "    forward(A, X, device=None): Performs the forward pass of the model.\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: The output of the GraphUnet model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ks, in_dim, out_dim, dim=320):\n",
    "        super(GraphUnet, self).__init__()\n",
    "        self.ks = ks\n",
    "\n",
    "        self.down_gcns = nn.ModuleList()\n",
    "        self.up_gcns = nn.ModuleList()\n",
    "        self.pools = nn.ModuleList()\n",
    "        self.unpools = nn.ModuleList()\n",
    "        self.l_n = len(ks)\n",
    "\n",
    "        self.down_gcns.append(GCN(in_dim, dim))\n",
    "        for i in range(self.l_n):\n",
    "            self.down_gcns.append(GCN(dim, dim))\n",
    "            self.pools.append(GraphPool(ks[i], dim))\n",
    "\n",
    "        for i in range(self.l_n):\n",
    "            self.up_gcns.append(GCN(dim, dim))\n",
    "            self.unpools.append(GraphUnpool())\n",
    "        self.up_gcns.append(GCN(dim*2, out_dim))\n",
    "\n",
    "    def forward(self, A, X, device=None):\n",
    "        \"\"\"\n",
    "        Perform the forward pass of the GraphUnet model.\n",
    "\n",
    "        Args:\n",
    "        A (torch.Tensor): Input adjacency matrix.\n",
    "        X (torch.Tensor): Input feature matrix.\n",
    "        device (torch.device, optional): The device to be used for computation. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "        torch.Tensor: The output of the GraphUnet model.\n",
    "        \"\"\"\n",
    "        X = self.down_gcns[0](A, X, device=device)\n",
    "        start_gcn_outs = X\n",
    "        org_X = X\n",
    "\n",
    "        adj_ms = [A]\n",
    "        indices_list = []\n",
    "        down_outs = [X]\n",
    "\n",
    "        for i in range(1, self.l_n+1):\n",
    "\n",
    "            A, X, idx = self.pools[i-1](A, X)\n",
    "            X = self.down_gcns[i](A, X, device=device)\n",
    "            if i < self.l_n:\n",
    "                adj_ms.append(A)\n",
    "                down_outs.append(X)\n",
    "            indices_list.append(idx)\n",
    "\n",
    "        for i in range(self.l_n):\n",
    "            up_idx = self.l_n - i - 1\n",
    "\n",
    "            A, idx = adj_ms[up_idx], indices_list[up_idx]\n",
    "            A, X = self.unpools[i](A, X, idx, device=device)\n",
    "            X = self.up_gcns[i](A, X)\n",
    "            X = X.add(down_outs[up_idx])\n",
    "        X = torch.cat([X, org_X], 1)\n",
    "        X = self.up_gcns[-1](A, X, device=device)\n",
    "\n",
    "        return X, start_gcn_outs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hm8V0KEQkWgD"
   },
   "source": [
    "## Part 2: Create Graph Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1716511361328,
     "user": {
      "displayName": "Wyal Xon",
      "userId": "14101716046877690904"
     },
     "user_tz": -60
    },
    "id": "LJ9sPLw5GAKP"
   },
   "outputs": [],
   "source": [
    "class GraphDiscriminator(nn.Module):\n",
    "    \"\"\"\n",
    "    Graph Discriminator model.\n",
    "\n",
    "    Args:\n",
    "    args (dict): Dictionary of arguments.\n",
    "\n",
    "    Methods:\n",
    "    forward(inputs, adj): Performs the forward pass of the model.\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: The output of the Graph Discriminator model.\n",
    "    \"\"\"\n",
    "    def __init__(self, args):\n",
    "        super(GraphDiscriminator, self).__init__()\n",
    "        self.hr_dim = args[\"hr_dim\"]\n",
    "        self.hidden_dim = args[\"hidden_dim\"]\n",
    "        self.gc1 = GraphConvolution(\n",
    "            self.hr_dim, self.hidden_dim, 0.1, act=None\n",
    "        )\n",
    "        self.act1 = nn.LeakyReLU(negative_slope=0.2, inplace=False)\n",
    "        self.gc2 = GraphConvolution(\n",
    "            self.hidden_dim, 1, 0.1, act=None\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs, adj):\n",
    "        \"\"\"\n",
    "        Perform the forward pass of the Graph Discriminator model.\n",
    "\n",
    "        Args:\n",
    "        inputs (torch.Tensor): Input data.\n",
    "        adj (torch.Tensor): Input adjacency matrix.\n",
    "\n",
    "        Returns:\n",
    "        torch.Tensor: The output of the Graph Discriminator model.\n",
    "        \"\"\"\n",
    "        z = self.gc1(inputs, adj)\n",
    "        z = self.act1(z)\n",
    "        z = self.gc2(z, adj)\n",
    "\n",
    "        return torch.abs(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0DzcmRWWKKT4"
   },
   "source": [
    "# Our model: Graph2Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1716511361328,
     "user": {
      "displayName": "Wyal Xon",
      "userId": "14101716046877690904"
     },
     "user_tz": -60
    },
    "id": "JpVy_Et7GAKP"
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "def normalize_adj_torch(mx):\n",
    "    \"\"\"\n",
    "    Normalize the input adjacency matrix using symmetric normalization.\n",
    "\n",
    "    Args:\n",
    "    mx (torch.Tensor): Input adjacency matrix.\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: Normalized adjacency matrix.\n",
    "    \"\"\"\n",
    "    rowsum = mx.sum(1)\n",
    "    r_inv_sqrt = torch.pow(rowsum, -0.5).flatten()\n",
    "    r_inv_sqrt[torch.isinf(r_inv_sqrt)] = 0.\n",
    "    r_mat_inv_sqrt = torch.diag(r_inv_sqrt)\n",
    "    mx = torch.matmul(mx, r_mat_inv_sqrt)\n",
    "    mx = torch.transpose(mx, 0, 1)\n",
    "    mx = torch.matmul(mx, r_mat_inv_sqrt)\n",
    "    return mx\n",
    "\n",
    "def unpad(data, split):\n",
    "    \"\"\"\n",
    "    Unpad the input data matrix.\n",
    "\n",
    "    Args:\n",
    "    data (numpy.ndarray): Input data matrix.\n",
    "    split (int): Number of rows and columns to remove from each side.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: Unpadded data matrix.\n",
    "    \"\"\"\n",
    "    idx_0 = data.shape[0]-split\n",
    "    idx_1 = data.shape[1]-split\n",
    "    train = data[split:idx_0, split:idx_1]\n",
    "    return train\n",
    "\n",
    "def gaussian_noise_layer(input_layer, args):\n",
    "    \"\"\"\n",
    "    Add Gaussian noise to the input layer.\n",
    "\n",
    "    Args:\n",
    "    input_layer (torch.Tensor): Input data matrix.\n",
    "    args (dict): Dictionary of arguments containing mean and standard deviation for the noise.\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: Input layer with added Gaussian noise.\n",
    "    \"\"\"\n",
    "    z = torch.empty_like(input_layer)\n",
    "    noise = z.normal_(mean=args[\"mean_gaussian\"], std=args[\"std_gaussian\"])\n",
    "    z = torch.abs(input_layer + noise)\n",
    "\n",
    "    z = (z + z.t())/2\n",
    "    z = z.fill_diagonal_(1)\n",
    "    return z\n",
    "\n",
    "class Graph2Graph(nn.Module):\n",
    "    \"\"\"\n",
    "    Graph2Graph model for graph data.\n",
    "\n",
    "    Args:\n",
    "    ks (list): List of ratios for graph pooling.\n",
    "    args (dict): Dictionary of arguments.\n",
    "\n",
    "    Methods:\n",
    "    forward(lr, lr_dim, hr_dim, device=None): Performs the forward pass of the model.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing the following:\n",
    "        - The processed adjacency matrix.\n",
    "        - The output of the GraphUnet model.\n",
    "        - The start output of the GraphConvolution layer.\n",
    "        - The final output of the model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ks, args):\n",
    "        super(Graph2Graph, self).__init__()\n",
    "\n",
    "        self.lr_dim = args[\"lr_dim\"]\n",
    "        self.hr_dim = args[\"hr_dim\"]\n",
    "        self.hidden_dim = args[\"hidden_dim\"]\n",
    "        self.layer = GSRLayer(self.hr_dim)\n",
    "        self.net = GraphUnet(ks, self.lr_dim, self.hr_dim)\n",
    "        self.gc1 = GraphConvolution(\n",
    "            self.hr_dim, self.hidden_dim, 0.2, act=partial(F.leaky_relu, negative_slope=0.2)\n",
    "        )\n",
    "        self.gc2 = GraphConvolution(\n",
    "            self.hidden_dim, self.hr_dim, 0.2, act=partial(F.leaky_relu, negative_slope=0.2)\n",
    "        )\n",
    "\n",
    "    def forward(self, lr, lr_dim, hr_dim, device=None):\n",
    "        \"\"\"\n",
    "        Perform the forward pass of the Graph2Graph model.\n",
    "\n",
    "        Args:\n",
    "        lr (torch.Tensor): Low-resolution input data.\n",
    "        lr_dim (int): Dimension of the low-resolution input.\n",
    "        hr_dim (int): Dimension of the high-resolution input.\n",
    "        device (torch.device, optional): The device to be used for computation. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "        tuple: A tuple containing the following:\n",
    "            - The processed adjacency matrix.\n",
    "            - The output of the GraphUnet model.\n",
    "            - The start output of the GraphConvolution layer.\n",
    "            - The final output of the model.\n",
    "        \"\"\"\n",
    "        I = torch.eye(self.lr_dim, device=device).type(torch.FloatTensor)\n",
    "        A = normalize_adj_torch(lr).type(torch.FloatTensor)\n",
    "\n",
    "        if device is not None:\n",
    "            A = A.to(device)\n",
    "\n",
    "        self.net_outs, self.start_gcn_outs = self.net(A, I, device=device)\n",
    "\n",
    "        self.outputs, self.Z = self.layer(A, self.net_outs, device=device)\n",
    "\n",
    "        z = self.gc1(self.Z, self.outputs)\n",
    "        z = self.gc2(z, self.outputs)\n",
    "\n",
    "        z = (z + z.t())/2\n",
    "        z = z.fill_diagonal_(1)\n",
    "\n",
    "        return torch.abs(z), self.net_outs, self.start_gcn_outs, self.outputs\n",
    "\n",
    "    def predict(self, lr_data, lr_dim, hr_dim, padding, device=None):\n",
    "        \"\"\"\n",
    "        Generate predictions for the given low-resolution data.\n",
    "\n",
    "        Args:\n",
    "        lr_data (list): List of low-resolution input data.\n",
    "        lr_dim (int): Dimension of the low-resolution input.\n",
    "        hr_dim (int): Dimension of the high-resolution input.\n",
    "        padding (int): Number of rows and columns to remove from each side.\n",
    "        device (torch.device, optional): The device to be used for computation. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "        list: List of predicted high-resolution data.\n",
    "        \"\"\"\n",
    "        predictions = []\n",
    "        for lr in lr_data:\n",
    "            lr = torch.from_numpy(lr).type(torch.FloatTensor)\n",
    "            if device is not None:\n",
    "                lr = lr.to(device)\n",
    "            preds = self.forward(lr, lr_dim, hr_dim, device=device)[0]\n",
    "            preds[preds < 0] = 0\n",
    "            preds = unpad(preds, padding).detach().cpu().numpy()\n",
    "            predictions.append(preds)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "flQY0MYwViF7"
   },
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GfM9WLVQGAKQ"
   },
   "source": [
    "In this section we define the hyperparameters used the Graph2Graph model and define the training function of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1716511361328,
     "user": {
      "displayName": "Wyal Xon",
      "userId": "14101716046877690904"
     },
     "user_tz": -60
    },
    "id": "E2DH9E5ZGAKQ"
   },
   "outputs": [],
   "source": [
    "args = {\n",
    "    'epochs': 80,\n",
    "    'lr': 0.001,\n",
    "    'lmbda': 0.1,\n",
    "    'lr_dim': 160,\n",
    "    'hr_dim': 320,\n",
    "    'hidden_dim': 320,\n",
    "    'padding': 26,\n",
    "    'mean_dense': 0.0,\n",
    "    'std_dense': 0.01,\n",
    "    'mean_gaussian': 0.0,\n",
    "    'std_gaussian': 0.1,\n",
    "    \"dropout_rate\": 0.01,\n",
    "    'beta': 1,\n",
    "}\n",
    "\n",
    "ks = [0.9, 0.7, 0.6, 0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1716511361328,
     "user": {
      "displayName": "Wyal Xon",
      "userId": "14101716046877690904"
     },
     "user_tz": -60
    },
    "id": "nAYckuheGAKQ"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def pad_HR_adj(label, split):\n",
    "    \"\"\"\n",
    "    Pad the high-resolution adjacency matrix.\n",
    "\n",
    "    Args:\n",
    "    label (numpy.ndarray): High-resolution adjacency matrix.\n",
    "    split (int): Number of rows and columns to pad on each side.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: Padded high-resolution adjacency matrix.\n",
    "    \"\"\"\n",
    "    label = np.pad(label, ((split, split), (split, split)), mode=\"constant\")\n",
    "    np.fill_diagonal(label, 1)\n",
    "    return label\n",
    "\n",
    "def train_g2g(model, subjects_adj, subjects_labels, args, val_adj=None, val_labels=None, device=None):\n",
    "    \"\"\"\n",
    "    Train the Graph2Graph model using the provided data and parameters.\n",
    "\n",
    "    Args:\n",
    "    model (Graph2Graph): The Graph2Graph model to be trained.\n",
    "    subjects_adj (list): List of adjacency matrices for training subjects.\n",
    "    subjects_labels (list): List of label matrices for training subjects.\n",
    "    args (dict): Dictionary of training arguments and hyperparameters.\n",
    "    val_adj (list, optional): List of adjacency matrices for validation subjects. Defaults to None.\n",
    "    val_labels (list, optional): List of label matrices for validation subjects. Defaults to None.\n",
    "    device (torch.device, optional): The device to be used for computation. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "    list or tuple: List of training evaluation metrics or tuple containing training\n",
    "    and validation evaluation metrics.\n",
    "    \"\"\"\n",
    "    criterion = nn.L1Loss()\n",
    "    bce_loss = nn.BCEWithLogitsLoss()\n",
    "    netD = GraphDiscriminator(args)\n",
    "    if device is not None:\n",
    "        model = model.to(device)\n",
    "        netD = netD.to(device)\n",
    "    optimizerG = optim.Adam(model.parameters(), lr=args[\"lr\"])\n",
    "    optimizerD = optim.Adam(netD.parameters(), lr=args[\"lr\"])\n",
    "    schedulerG = optim.lr_scheduler.ReduceLROnPlateau(optimizerG, 'min', patience=3, factor=0.5)\n",
    "    schedulerD = optim.lr_scheduler.ReduceLROnPlateau(optimizerD, 'min', patience=3, factor=0.5)\n",
    "\n",
    "    all_epochs_loss = []\n",
    "    train_evaluation_metrics = []\n",
    "    val_evaluation_metrics = []\n",
    "    best_epoch = -1\n",
    "    best_val_mae = 1e10\n",
    "    val_mae_coll = []\n",
    "    patience_epochs = 10\n",
    "\n",
    "    for epoch in range(args[\"epochs\"]):\n",
    "        epoch_loss = []\n",
    "        epoch_error = []\n",
    "        g_loss = []\n",
    "        d_loss = []\n",
    "        model.train()\n",
    "\n",
    "        # Shuffle the data so that the order of subjects is different in each epoch\n",
    "        order = list(range(len(subjects_adj)))\n",
    "        random.shuffle(order)\n",
    "        for index in order:\n",
    "            lr = subjects_adj[index]\n",
    "            hr = subjects_labels[index]\n",
    "\n",
    "            optimizerD.zero_grad()\n",
    "            optimizerG.zero_grad()\n",
    "\n",
    "            padded_hr = pad_HR_adj(hr, args[\"padding\"])\n",
    "            lr = torch.from_numpy(lr).type(torch.FloatTensor)\n",
    "            padded_hr = torch.from_numpy(padded_hr).type(torch.FloatTensor)\n",
    "\n",
    "            # Add noise to input data\n",
    "            noise_scales = 0.05 * torch.rand_like(lr)\n",
    "            lr = lr + noise_scales * lr\n",
    "            lr = torch.clip(lr, min=0, max=1)\n",
    "            # Drop some of the nodes\n",
    "            if args[\"dropout_rate\"] > 0:\n",
    "                mask = torch.bernoulli(torch.full((lr.shape[0],), args[\"dropout_rate\"])).bool()\n",
    "                lr[mask, :] = 0\n",
    "                lr[:, mask] = 0\n",
    "                lr[mask, mask] = 1\n",
    "\n",
    "            if device is not None:\n",
    "                lr = lr.to(device)\n",
    "                padded_hr = padded_hr.to(device)\n",
    "\n",
    "            # Eigen decomposition needed for the loss function\n",
    "            eig_val_hr, U_hr = torch.linalg.eigh(\n",
    "                padded_hr, UPLO='U')\n",
    "\n",
    "            # Forward pass through the model\n",
    "            model_outputs, net_outs, start_gcn_outs, layer_outs = model(\n",
    "                lr, args[\"lr_dim\"], args[\"hr_dim\"], device=device)\n",
    "\n",
    "            recon_loss = args[\"lmbda\"] * criterion(net_outs, start_gcn_outs)\n",
    "            eig_loss = criterion(model.layer.weights, U_hr)\n",
    "            hr_loss = criterion(model_outputs, padded_hr)\n",
    "\n",
    "            mse_loss = recon_loss + eig_loss + hr_loss\n",
    "\n",
    "            error = criterion(model_outputs, padded_hr)\n",
    "            real_data = model_outputs.detach()\n",
    "            fake_data = torch.clip(gaussian_noise_layer(padded_hr, args), min=0, max=1)\n",
    "            fake_adj = gaussian_noise_layer(layer_outs, args)\n",
    "\n",
    "            if device is not None:\n",
    "                real_data = real_data.to(device)\n",
    "                fake_data = fake_data.to(device)\n",
    "\n",
    "            # Forward pass through the discriminator\n",
    "            d_real = netD(real_data, fake_adj)\n",
    "            d_fake = netD(fake_data, fake_adj)\n",
    "\n",
    "            # Smooth labels\n",
    "            real_labels = torch.rand(args[\"hr_dim\"], 1, device=device) * 0.3 + 0.7\n",
    "            fake_labels = torch.rand(args[\"hr_dim\"], 1, device=device) * 0.3\n",
    "\n",
    "            # Swap real and fake labels with a small probability\n",
    "            if torch.rand(1) < 0.05:\n",
    "                real_labels, fake_labels = fake_labels, real_labels\n",
    "\n",
    "            # Compute losses for discriminator and optimise\n",
    "            dc_loss_real = bce_loss(d_real, real_labels)\n",
    "            dc_loss_fake = bce_loss(d_fake, fake_labels)\n",
    "            dc_loss = dc_loss_real + dc_loss_fake\n",
    "            d_loss.append(dc_loss.item())\n",
    "\n",
    "            dc_loss.backward(retain_graph=True)\n",
    "            optimizerD.step()\n",
    "\n",
    "            d_fake = netD(fake_data, fake_adj)\n",
    "\n",
    "            # Compute losses for generator and optimise\n",
    "            gen_loss = bce_loss(d_fake, real_labels)\n",
    "            g_loss.append(gen_loss.item())\n",
    "\n",
    "            generator_loss = gen_loss + args[\"beta\"] * mse_loss\n",
    "            generator_loss.backward()\n",
    "\n",
    "            optimizerG.step()\n",
    "            epoch_loss.append(generator_loss.item())\n",
    "\n",
    "            epoch_error.append(error.item())\n",
    "\n",
    "        print(\"Epoch: \", epoch, \"Loss: \", np.mean(epoch_loss),\n",
    "              \"G Loss: \", np.mean(g_loss), \"D Loss: \", np.mean(d_loss),\n",
    "                \"Error: \", np.mean(epoch_error)*100, \"%\")\n",
    "        all_epochs_loss.append(np.mean(epoch_loss))\n",
    "\n",
    "        # Compute and store evaluation metrics\n",
    "        model.eval()\n",
    "        print('Train evaluation metrics:')\n",
    "        predictions_train = model.predict(subjects_adj, args[\"lr_dim\"], args[\"hr_dim\"], args[\"padding\"], device)\n",
    "        train_evaluation_metrics.append(get_evaluation_metrics(predictions_train, subjects_labels, computed_gt_metrics=None, graph_metrics=False))\n",
    "        if val_adj is not None:\n",
    "            print('Val evaluation metrics:')\n",
    "            predictions_val = model.predict(val_adj, args[\"lr_dim\"], args[\"hr_dim\"], args[\"padding\"], device)\n",
    "            val_evaluation_metrics.append(get_evaluation_metrics(predictions_val, val_labels, computed_gt_metrics=None, graph_metrics=False))\n",
    "\n",
    "            val_mae, _, _ = get_evaluation_metrics(predictions_val, val_labels, computed_gt_metrics=None, graph_metrics=False)\n",
    "\n",
    "            schedulerD.step(val_evaluation_metrics[-1][0])\n",
    "            schedulerG.step(val_evaluation_metrics[-1][0])\n",
    "\n",
    "            val_mae_coll.append(val_mae)\n",
    "\n",
    "            if val_mae < best_val_mae:\n",
    "                best_epoch = int(epoch + 1)\n",
    "                best_val_mae = val_mae\n",
    "\n",
    "            if (epoch - (best_epoch - 1)) >= patience_epochs:\n",
    "                print(f\"stopped early at epoch {epoch}\")\n",
    "                break\n",
    "\n",
    "        else:\n",
    "            schedulerD.step(np.mean(d_loss))\n",
    "            schedulerG.step(np.mean(epoch_loss))\n",
    "        print('-'*10)\n",
    "    if val_adj is None:\n",
    "        return train_evaluation_metrics\n",
    "    return train_evaluation_metrics, val_evaluation_metrics, best_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GAVv8o-VJS-E"
   },
   "source": [
    "# Cross-Validation setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1716511361328,
     "user": {
      "displayName": "Wyal Xon",
      "userId": "14101716046877690904"
     },
     "user_tz": -60
    },
    "id": "dJwqh6i6Lg_l"
   },
   "outputs": [],
   "source": [
    "#def k_fold_cross_validation(lr_train_data, hr_train_data, ground_truth_metrics, hyperparameters=args, k=3, graph_metrics=False, plot_losses=False):\n",
    "def k_fold_cross_validation(hyperparameters=args):\n",
    "    print(\"Starting k-fold cross-validation...\")\n",
    "    print(\"Hyperparameters:\")\n",
    "    for key, value in hyperparameters.items():\n",
    "        print(f\"\\t{key}: {value}\")\n",
    "\n",
    "    # data_array = prepare_train_data_for_GSR(lr_train_data, embedding_dim=160)\n",
    "    # hr_data_array = prepare_train_data_for_GSR(hr_train_data, embedding_dim=268)\n",
    "    main_path = \"/content/drive/My Drive/Great Network Ninja/submission_file/DGL_project/code/\"\n",
    "    lr_data_path = [\"/Cluster-CV/Cluster-CV/Fold1/lr_clusterA.csv\", \"/Cluster-CV/Cluster-CV/Fold2/lr_clusterB.csv\", \"/Cluster-CV/Cluster-CV/Fold3/lr_clusterC.csv\"]\n",
    "    hr_data_path = [\"/Cluster-CV/Cluster-CV/Fold1/hr_clusterA.csv\", \"/Cluster-CV/Cluster-CV/Fold2/hr_clusterB.csv\", \"/Cluster-CV/Cluster-CV/Fold3/hr_clusterC.csv\"]\n",
    "\n",
    "\n",
    "    # Create a KFold object\n",
    "    # kf = KFold(n_splits=k, shuffle=True, random_state=random_seed)\n",
    "    # Initialize lists to store predictions and evaluation results for each fold\n",
    "    all_predictions = []\n",
    "    all_evaluation_results = []\n",
    "    folds_evaluations = []\n",
    "\n",
    "    fold_rotation = [[0,1, 2], [1,2, 0], [2,0, 1]]\n",
    "\n",
    "    # Perform cross-validation\n",
    "    # for fold_num, (train_index, test_index) in enumerate(kf.split(lr_train_data)):\n",
    "    for i in range(3):\n",
    "        print(\"Experiment no: {}\".format(i))\n",
    "        fold = fold_rotation[i]\n",
    "        data_train_1 = pd.read_csv(main_path + lr_data_path[fold[0]])\n",
    "        hr_data_train_1 = pd.read_csv(main_path + hr_data_path[fold[0]])\n",
    "        data_train_1 = prepare_train_data_for_GSR(data_train_1, embedding_dim=160)\n",
    "        hr_data_train_1 = prepare_train_data_for_GSR(hr_data_train_1, embedding_dim=268)\n",
    "\n",
    "        data_train_2 = pd.read_csv(main_path + lr_data_path[fold[1]])\n",
    "        hr_data_train_2 = pd.read_csv(main_path + hr_data_path[fold[1]])\n",
    "        data_train_2 = prepare_train_data_for_GSR(data_train_2, embedding_dim=160)\n",
    "        hr_data_train_2 = prepare_train_data_for_GSR(hr_data_train_2, embedding_dim=268)\n",
    "\n",
    "\n",
    "        data_test = pd.read_csv(main_path + lr_data_path[fold[2]])\n",
    "        hr_data_test = pd.read_csv(main_path + hr_data_path[fold[2]])\n",
    "        data_test = prepare_train_data_for_GSR(data_test, embedding_dim=160)\n",
    "        hr_data_test = prepare_train_data_for_GSR(hr_data_test, embedding_dim=268)\n",
    "\n",
    "\n",
    "        data_train = np.vstack((data_train_1[:data_train_1.shape[0]-20,:],data_train_2[:data_train_2.shape[0]-20,:]))\n",
    "        hr_data_train = np.vstack((hr_data_train_1[:hr_data_train_1.shape[0]-20,:],hr_data_train_2[:hr_data_train_2.shape[0]-20,:]))\n",
    "        data_val = np.vstack((data_train_1[-20:,:],data_train_2[-20:,:]))\n",
    "        hr_data_val = np.vstack((hr_data_train_1[-20:,:],hr_data_train_2[-20:,:]))\n",
    "\n",
    "        # data_train, data_val = data_array[train_index], data_array[test_index]\n",
    "        # hr_data_train, hr_data_val = hr_data_array[train_index], hr_data_array[test_index]\n",
    "\n",
    "        # train_gt_metrics = [ground_truth_metrics[i] for i in train_index.tolist()]\n",
    "        # val_gt_metrics = [ground_truth_metrics[i] for i in test_index.tolist()]\n",
    "\n",
    "        # Create an instance of model\n",
    "        model = Graph2Graph(ks, args)\n",
    "\n",
    "        # Train the model\n",
    "        train_metrics, val_metrics, best_epoch = train_g2g(model, data_train, hr_data_train, args, data_val, hr_data_val, device)\n",
    "        # folds_evaluations.append((train_metrics, val_metrics))\n",
    "\n",
    "        # Train the model with train + val\n",
    "        args[\"epochs\"] = best_epoch\n",
    "        final_model = Graph2Graph(ks, args) # create a new instance of model\n",
    "        data_train_full = np.vstack((data_train_1[:,:],data_train_2[:,:]))\n",
    "        hr_data_train_full = np.vstack((hr_data_train_1[:,:],hr_data_train_2[:,:]))\n",
    "        _ = train_g2g(final_model, data_train_full, hr_data_train_full, args, None, None, device)\n",
    "        args[\"epochs\"] = 80 # reset back to original after training is finished\n",
    "\n",
    "        # Make predictions\n",
    "        # predictions = model.predict(data_val, hyperparameters[\"lr_dim\"], hyperparameters[\"hr_dim\"], hyperparameters[\"padding\"], device)\n",
    "        predictions = model.predict(data_test, hyperparameters[\"lr_dim\"], hyperparameters[\"hr_dim\"], hyperparameters[\"padding\"], device)\n",
    "\n",
    "        save_test_result_path = main_path + \"Ninja-clusterCV_\" + str(i) + \".csv\"\n",
    "        evaluate_all(hr_data_test, np.array(predictions), save_test_result_path)\n",
    "\n",
    "        model_name = main_path + \"Ninja_model_full_clusterCV_\" + str(i)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.save(final_model.state_dict(), f\"{model_name}.pt\")\n",
    "        else:\n",
    "            torch.save(final_model.state_dict(), f\"{model_name}.pt\", map_location=torch.device('cpu'))\n",
    "        '''\n",
    "        # Append predictions for evaluation\n",
    "        pred_vectors = torch.stack([torch.Tensor(MatrixVectorizer.vectorize(preds)) for preds in predictions])\n",
    "        all_predictions.extend(pred_vectors)\n",
    "\n",
    "        # Step I: Flatten the prediction matrix\n",
    "        flattened_predictions = pred_vectors.flatten()\n",
    "\n",
    "        # Step II: Create a DataFrame with 'ID' and 'Predicted' columns\n",
    "        df = pd.DataFrame({'ID': range(1, len(flattened_predictions) + 1), 'Predicted': flattened_predictions})\n",
    "\n",
    "        # Step III: Save the DataFrame to a CSV file\n",
    "        df.to_csv(f'predictions_fold_{i + 1}.csv', index=False)\n",
    "\n",
    "        # Print message and append evaluation results\n",
    "        print(f'Submission file \"predictions_fold_{i + 1}.csv\" created successfully.')\n",
    "        # all_evaluation_results.append(get_evaluation_metrics(np.array(predictions), hr_data_val, val_gt_metrics, graph_metrics=graph_metrics))\n",
    "        '''\n",
    "\n",
    "    # return all_predictions, all_evaluation_results, predictions[0], hr_data_val[0], folds_evaluations\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5329819,
     "status": "ok",
     "timestamp": 1716516691139,
     "user": {
      "displayName": "Wyal Xon",
      "userId": "14101716046877690904"
     },
     "user_tz": -60
    },
    "id": "D_dTZcpBGAKR",
    "outputId": "50ffc761-26c3-44e3-d762-c0811500a23a"
   },
   "outputs": [],
   "source": [
    "k_fold_cross_validation(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nmPSgzkZti8e"
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E9UNnbtOGAKV"
   },
   "source": [
    "Using the data from the cross validation, we will plot the evaluation metrics for each fold and the average of all folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 52,
     "status": "aborted",
     "timestamp": 1716516691140,
     "user": {
      "displayName": "Wyal Xon",
      "userId": "14101716046877690904"
     },
     "user_tz": -60
    },
    "id": "EdbEMKvGGAKV"
   },
   "outputs": [],
   "source": [
    "colors = ['blue', 'orange', 'green', 'red']  # Colors for each fold and the average\n",
    "evaluation_measures = ['MAE', 'PCC', 'JSD', 'MAE (BC)', 'MAE (EC)', 'MAE (PC)']\n",
    "\n",
    "# Splitting the evaluation measures\n",
    "evaluation_measures_1 = evaluation_measures[:3]\n",
    "evaluation_measures_2 = evaluation_measures[3:]\n",
    "index_1 = np.arange(len(evaluation_measures_1))\n",
    "index_2 = np.arange(len(evaluation_measures_2))\n",
    "\n",
    "# Calculate averages and standard deviations\n",
    "average_results = np.mean(all_evaluation_results, axis=0)\n",
    "std_dev_results = np.std(all_evaluation_results, axis=0)\n",
    "\n",
    "# Layout settings\n",
    "bar_width = 0.15\n",
    "opacity = 0.6\n",
    "spacing = 0.02\n",
    "\n",
    "# Create a single figure with two subplots\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 12))\n",
    "\n",
    "# Plot for the first three metrics\n",
    "for i in range(len(all_evaluation_results)):\n",
    "    ax1.bar(index_1 + i * (bar_width + spacing), all_evaluation_results[i][:3], bar_width,\n",
    "            alpha=opacity, label=f'Fold {i+1}', color=colors[i])\n",
    "ax1.bar(index_1 + 3 * (bar_width + spacing) + spacing, average_results[:3], bar_width, yerr=std_dev_results[:3], alpha=opacity, capsize=5, label='Average', color=colors[-1])\n",
    "ax1.set_xlabel('Evaluation Measure')\n",
    "ax1.set_ylabel('Values')\n",
    "ax1.set_title('Evaluation Measures across Folds and Average with standard deviation')\n",
    "ax1.set_xticks(index_1 + 1.5 * bar_width + spacing)\n",
    "ax1.set_xticklabels(evaluation_measures_1)\n",
    "ax1.tick_params(axis='x', length=0)  # Remove x-tick marks\n",
    "ax1.legend(loc='upper center', bbox_to_anchor=(0.5, 1.15), ncol=4)\n",
    "\n",
    "# Second plot with a secondary y-axis for the MAE (PC) metric\n",
    "mae_pc_index = len(evaluation_measures_2) - 1\n",
    "all_evaluation_results_np = np.array(all_evaluation_results)\n",
    "\n",
    "# Plotting all metrics except MAE (PC) on the first axis\n",
    "for i in range(len(all_evaluation_results)):\n",
    "    ax2.bar(index_2[:-1] + i * (bar_width + spacing), all_evaluation_results[i][3:5], bar_width,\n",
    "            alpha=opacity, color=colors[i], label=f'Fold {i+1}' if i < 3 else 'Average')\n",
    "\n",
    "# Plot the average bars except for MAE (PC)\n",
    "ax2.bar(index_2[:-1] + 3 * (bar_width + spacing) + spacing, average_results[3:5], bar_width,\n",
    "        yerr=std_dev_results[3:5], alpha=opacity, color=colors[-1], capsize=5)\n",
    "\n",
    "ax2.set_xlabel('Evaluation Measure')\n",
    "ax2.set_ylabel(f'Values for {evaluation_measures_2[0]} and {evaluation_measures_2[1]}')\n",
    "ax2.set_title('Additional Evaluation Measures with Secondary Axis for MAE (PC)')\n",
    "ax2.set_xticks(index_2 + 1.5 * bar_width + spacing)\n",
    "ax2.set_xticklabels(evaluation_measures_2)\n",
    "\n",
    "# Add secondary Y-axis for MAE (PC)\n",
    "ax3 = ax2.twinx()\n",
    "for i in range(len(all_evaluation_results)):\n",
    "    ax3.bar(index_2[mae_pc_index] + i * (bar_width + spacing), all_evaluation_results_np[i][3+mae_pc_index], bar_width,\n",
    "            alpha=opacity, color=colors[i])\n",
    "\n",
    "# Plot the average bar for MAE (PC) on the secondary axis\n",
    "ax3.bar(index_2[mae_pc_index] + 3 * (bar_width + spacing) + spacing, average_results[3+mae_pc_index], bar_width,\n",
    "        yerr=std_dev_results[3+mae_pc_index], alpha=opacity, color=colors[-1], capsize=5)\n",
    "ax3.set_ylabel('Values for MAE (PC)')\n",
    "\n",
    "# Adjust layout for the combined figure\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 52,
     "status": "aborted",
     "timestamp": 1716516691140,
     "user": {
      "displayName": "Wyal Xon",
      "userId": "14101716046877690904"
     },
     "user_tz": -60
    },
    "id": "VG8hdc8oGAKV"
   },
   "outputs": [],
   "source": [
    "# Splitting the evaluation measures and their corresponding results into two sets for better visualization\n",
    "evaluation_measures = ['MAE', 'PCC', 'JSD', 'MAE (BC)', 'MAE (EC)', 'MAE (PC)']\n",
    "evaluation_measures_1 = evaluation_measures[:3]  # First set of metrics\n",
    "evaluation_measures_2 = evaluation_measures[3:]  # Second set of metrics\n",
    "index_1 = np.arange(len(evaluation_measures_1)) * 0.4\n",
    "index_2 = np.arange(len(evaluation_measures_1)) * 0.4\n",
    "colors = ['red', 'green', 'blue', 'orange', 'cyan', 'lightgreen'] # Colors for each metric\n",
    "\n",
    "# Calculate the average results and standard deviation across folds for each measure\n",
    "average_results = np.mean(all_evaluation_results, axis=0)\n",
    "std_dev_results = np.std(all_evaluation_results, axis=0)\n",
    "\n",
    "# Layout settings\n",
    "bar_width = 0.3\n",
    "opacity = 0.8\n",
    "\n",
    "# Plotting\n",
    "fig = plt.figure(constrained_layout=True, figsize=(10, 18))\n",
    "gs = fig.add_gridspec(nrows=4, ncols=1, hspace=0.1)\n",
    "subfigs = [fig.add_subfigure(gs[i]) for i in range(4)] # 4 rows for each fold and the average, 2 columns for two sets of metrics for better visualisation\n",
    "\n",
    "# Loop through each fold and the average\n",
    "for fold_index, subfig in enumerate(subfigs):\n",
    "    subfig.suptitle(f'Fold {fold_index + 1}' if fold_index < 3 else 'Average', fontsize=16)\n",
    "\n",
    "    axs = subfig.subplots(nrows=1, ncols=2, gridspec_kw={'wspace': 0.1})\n",
    "\n",
    "    results = all_evaluation_results[fold_index] if fold_index < 3 else average_results\n",
    "    # First set of metrics\n",
    "    axs[0].bar(index_1, results[:3], bar_width, yerr=std_dev_results[:3] if fold_index == 3 else None, alpha=opacity, capsize=5, color=colors[:3])\n",
    "    axs[0].set_ylabel('Values')\n",
    "    axs[0].set_xticks(index_1)\n",
    "    axs[0].set_xticklabels(evaluation_measures_1)\n",
    "    axs[0].tick_params(axis='x', length=0)\n",
    "\n",
    "    # Second set of metrics\n",
    "    axs[1].bar(index_2[:-1], results[3:5], bar_width, yerr=std_dev_results[3:5] if fold_index == 3 else None, alpha=opacity, capsize=5, color=colors[3:5])\n",
    "    ax3 = axs[1].twinx()\n",
    "    ax3.bar(index_2[-1], results[-1], bar_width, yerr=std_dev_results[-1] if fold_index == 3 else None, alpha=opacity, capsize=5, color=colors[-1])\n",
    "\n",
    "    axs[1].set_xticks(index_2)\n",
    "    axs[1].set_xticklabels(evaluation_measures_2)\n",
    "    axs[1].tick_params(axis='x', length=0)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 52,
     "status": "aborted",
     "timestamp": 1716516691141,
     "user": {
      "displayName": "Wyal Xon",
      "userId": "14101716046877690904"
     },
     "user_tz": -60
    },
    "id": "3QYcabdWPfvo"
   },
   "outputs": [],
   "source": [
    "# check 1 example\n",
    "\n",
    "# Find the maximum value among both matrices to set a common colorbar scale\n",
    "max_value = 1\n",
    "\n",
    "adjacency_matrix_1 = add_self_connections(torch.Tensor(example_predictions)).numpy()\n",
    "adjacency_matrix_2 = add_self_connections(torch.Tensor(example_ground_truth)).numpy()\n",
    "# Create a figure and axes\n",
    "fig, axs = plt.subplots(1, 2, figsize=(8, 4))\n",
    "\n",
    "# Plot the first adjacency matrix with a blue colormap\n",
    "cax1 = axs[0].matshow(adjacency_matrix_1, cmap='viridis', vmax=max_value)\n",
    "axs[0].set_title('Prediction')\n",
    "# Plot the second adjacency matrix with a blue colormap\n",
    "cax2 = axs[1].matshow(adjacency_matrix_2, cmap='viridis', vmax=max_value)\n",
    "axs[1].set_title('Ground Truth')\n",
    "\n",
    "\n",
    "# Add a colorbar using the same scale for both matrices\n",
    "cbar = fig.colorbar(cax1, ax=axs, shrink=0.8)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 52,
     "status": "aborted",
     "timestamp": 1716516691141,
     "user": {
      "displayName": "Wyal Xon",
      "userId": "14101716046877690904"
     },
     "user_tz": -60
    },
    "id": "SDB_nJTeGAKW"
   },
   "outputs": [],
   "source": [
    "all_evaluation_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ps0GRVYXVjcW"
   },
   "source": [
    "# Test model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cKVDrjfgGAKW"
   },
   "source": [
    "In this section we retrain the model on the whole dataset, to take full advantage of the data available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 52,
     "status": "aborted",
     "timestamp": 1716516691141,
     "user": {
      "displayName": "Wyal Xon",
      "userId": "14101716046877690904"
     },
     "user_tz": -60
    },
    "id": "-e9h7dw5qjjB"
   },
   "outputs": [],
   "source": [
    "# Load test data\n",
    "lr_test_data = pd.read_csv(\"data/lr_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 51,
     "status": "aborted",
     "timestamp": 1716516691141,
     "user": {
      "displayName": "Wyal Xon",
      "userId": "14101716046877690904"
     },
     "user_tz": -60
    },
    "id": "oYUq9wjfq79K"
   },
   "outputs": [],
   "source": [
    "# Train model on all data\n",
    "model = Graph2Graph(ks, args)\n",
    "train_g2g(model, lr_data_list, hr_data_list, args, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 51,
     "status": "aborted",
     "timestamp": 1716516691141,
     "user": {
      "displayName": "Wyal Xon",
      "userId": "14101716046877690904"
     },
     "user_tz": -60
    },
    "id": "KXsWEsM4GAKW"
   },
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "test_data_array = prepare_train_data_for_GSR(lr_test_data, embedding_dim=160)\n",
    "preds_list = []\n",
    "for lr in test_data_array:\n",
    "    lr = torch.from_numpy(lr).type(torch.FloatTensor)\n",
    "    preds, a, b, c = model(lr, args[\"lr_dim\"], args[\"hr_dim\"], device=device)\n",
    "    preds[preds < 0] = 0\n",
    "    preds = unpad(preds, args[\"padding\"])\n",
    "    preds_list.append(torch.Tensor(MatrixVectorizer.vectorize(preds.cpu().detach().numpy(), include_diagonal=False)))\n",
    "predictions = torch.stack(preds_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lzIMN_yXGAKX"
   },
   "source": [
    "We prepare data for submission in the format supported by the competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 52,
     "status": "aborted",
     "timestamp": 1716516691142,
     "user": {
      "displayName": "Wyal Xon",
      "userId": "14101716046877690904"
     },
     "user_tz": -60
    },
    "id": "DBGyvXSXsCEZ"
   },
   "outputs": [],
   "source": [
    "# Step I: Flatten the prediction matrix\n",
    "flattened_predictions = predictions.flatten()\n",
    "\n",
    "# Step II: Create a DataFrame with 'ID' and 'Predicted' columns\n",
    "df = pd.DataFrame({'ID': range(1, len(flattened_predictions) + 1), 'Predicted': flattened_predictions})\n",
    "\n",
    "# Step III: Save the DataFrame to a CSV file\n",
    "df.to_csv(f'submission.csv', index=False)\n",
    "\n",
    "# Display a summary or print a message\n",
    "print(f'Submission file \"submission.csv\" created successfully.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 52,
     "status": "aborted",
     "timestamp": 1716516691142,
     "user": {
      "displayName": "Wyal Xon",
      "userId": "14101716046877690904"
     },
     "user_tz": -60
    },
    "id": "B8SaXCavsPTy"
   },
   "outputs": [],
   "source": [
    "!wc -l submission.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 51,
     "status": "aborted",
     "timestamp": 1716516691142,
     "user": {
      "displayName": "Wyal Xon",
      "userId": "14101716046877690904"
     },
     "user_tz": -60
    },
    "id": "xF8QaWTUslmh"
   },
   "outputs": [],
   "source": [
    "!head -15 submission.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nntDbCBYGAKX"
   },
   "source": [
    "### Exploring Invariance and Equivariance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HnNLjku_GAKX"
   },
   "source": [
    "We perform multiple experiments to explore the invariance and equivariance properties of our model. We consider predictions on a sample from the test set and compare with the prediction for the same sample after applying a permutation. We inspect the distribution in the edge weights and the node embeddings to understand these properties. We also perform a WL test to check whether the two predicitons are isomorphic graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 51,
     "status": "aborted",
     "timestamp": 1716516691142,
     "user": {
      "displayName": "Wyal Xon",
      "userId": "14101716046877690904"
     },
     "user_tz": -60
    },
    "id": "u7PY4gIxGAKX"
   },
   "outputs": [],
   "source": [
    "# Permutation matrix\n",
    "model.eval()\n",
    "permutation = torch.randperm(160)\n",
    "\n",
    "# Convert to actual permutation matrix\n",
    "permutation_matrix = torch.zeros((160, 160), device=device)\n",
    "permutation_matrix[torch.arange(160), permutation] = 1\n",
    "\n",
    "sample_adj = torch.from_numpy(test_data_array[0]).type(torch.FloatTensor).to(device)\n",
    "permuted_adj = permutation_matrix @ sample_adj @ permutation_matrix.T\n",
    "\n",
    "preds = model(sample_adj, args[\"lr_dim\"], args[\"hr_dim\"], device=device)[0]\n",
    "preds[preds < 0] = 0\n",
    "preds_m = unpad(preds, args[\"padding\"])\n",
    "preds = torch.Tensor(MatrixVectorizer.vectorize(preds_m.cpu().detach().numpy(), include_diagonal=False))\n",
    "\n",
    "permuted_preds = model(permuted_adj, args[\"lr_dim\"], args[\"hr_dim\"], device=device)[0]\n",
    "permuted_preds[permuted_preds < 0] = 0\n",
    "permuted_preds_m = unpad(permuted_preds, args[\"padding\"])\n",
    "permuted_preds = torch.Tensor(MatrixVectorizer.vectorize(permuted_preds_m.cpu().detach().numpy(), include_diagonal=False))\n",
    "\n",
    "# Plot the values as histograms\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 4))\n",
    "axs[0].hist(preds, bins=100, color='blue', alpha=0.7)\n",
    "axs[0].set_title('Original')\n",
    "axs[1].hist(permuted_preds, bins=100, color='red', alpha=0.7)\n",
    "axs[1].set_title('Permuted')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 51,
     "status": "aborted",
     "timestamp": 1716516691142,
     "user": {
      "displayName": "Wyal Xon",
      "userId": "14101716046877690904"
     },
     "user_tz": -60
    },
    "id": "xa8Bxv7VGAKY"
   },
   "outputs": [],
   "source": [
    "G1 = nx.from_numpy_array(preds_m.cpu().detach().numpy(), edge_attr=\"weight\")\n",
    "G2 = nx.from_numpy_array(permuted_preds_m.cpu().detach().numpy(), edge_attr=\"weight\")\n",
    "\n",
    "# Degree Distribution\n",
    "degrees1 = [G1.degree(n) for n in G1.nodes()]\n",
    "degrees2 = [G2.degree(n) for n in G2.nodes()]\n",
    "degree_dist_similarity = np.linalg.norm(np.sort(degrees1) - np.sort(degrees2))\n",
    "print(\"Degree Distribution Difference:\", degree_dist_similarity)\n",
    "\n",
    "# Graph Hash (to check checking isomorphism)\n",
    "hash1 = nx.weisfeiler_lehman_graph_hash(G1)\n",
    "hash2 = nx.weisfeiler_lehman_graph_hash(G2)\n",
    "isomorphic = hash1 == hash2\n",
    "print(f\"Hashes: {hash1} vs. {hash2}, Isomorphic: {isomorphic}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "abFCseO0VXN1",
    "2PhMhgloGAKO",
    "2rqLasHodOd3",
    "hm8V0KEQkWgD",
    "0DzcmRWWKKT4",
    "nmPSgzkZti8e",
    "ps0GRVYXVjcW"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "dgl_cw_new",
   "language": "python",
   "name": "dgl_cw_new"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
