{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from src.utils import MatrixVectorizer\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import random\n",
    "from src.evaluation import evaluate\n",
    "import copy\n",
    "from evaluation_metric import evaluate_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a fixed random seed for reproducibility across multiple libraries\n",
    "random_seed = 42\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "# Check for CUDA (GPU support) and set device accordingly\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "    print(\"CUDA is available. Using GPU.\")\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed)  # For multi-GPU setups\n",
    "    # Additional settings for ensuring reproducibility on CUDA\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "    print(\"CUDA not available. Using CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datasets(lr_dim=160, hr_dim=268):\n",
    "    \"\"\" Retrieves train and test datasets from disk, and applies necessary pre-processing. \"\"\"\n",
    "    lr_train1 = []\n",
    "    lr_train2 = []\n",
    "    lr_train3 = []\n",
    "    lr_test = []\n",
    "    hr_train1 = []\n",
    "    hr_train2 = []\n",
    "    hr_train3 = []\n",
    "\n",
    "    lr_train_vec1 = pd.read_csv(\"../../RandomCV/Train/Split1/lr_clusterA.csv\").clip(lower=0).fillna(0).values\n",
    "    hr_train_vec1 = pd.read_csv(\"../../RandomCV/Train/Split1/hr_clusterA_modified.csv\").clip(lower=0).fillna(0).values\n",
    "\n",
    "    lr_train_vec2 = pd.read_csv(\"../../RandomCV/Train/Split2/lr_clusterB.csv\").clip(lower=0).fillna(0).values\n",
    "    hr_train_vec2 = pd.read_csv(\"../../RandomCV/Train/Split2/hr_clusterB_modified.csv\").clip(lower=0).fillna(0).values\n",
    "\n",
    "    lr_train_vec3 = pd.read_csv(\"../../RandomCV/Train/Split3/lr_clusterC.csv\").clip(lower=0).fillna(0).values\n",
    "    hr_train_vec3 = pd.read_csv(\"../../RandomCV/Train/Split3/hr_clusterC_modified.csv\").clip(lower=0).fillna(0).values\n",
    "\n",
    "    # lr_train_vec = pd.read_csv(\"data/lr_train.csv\").clip(lower=0).fillna(0).values\n",
    "    # lr_test_vec = pd.read_csv(\"data/lr_test.csv\").clip(lower=0).fillna(0).values\n",
    "    # hr_train_vec = pd.read_csv(\"data/hr_train.csv\").clip(lower=0).fillna(0).values\n",
    "    for x,y in zip(lr_train_vec1, lr_train_vec2):\n",
    "        # Reconstitute matrices from flattened representation\n",
    "        adj1 = MatrixVectorizer.anti_vectorize(x, lr_dim).astype(float)\n",
    "        lr_train1.append(adj1 + np.eye(lr_dim))\n",
    "\n",
    "        adj2 = MatrixVectorizer.anti_vectorize(y, lr_dim).astype(float)\n",
    "        lr_train2.append(adj2 + np.eye(lr_dim))\n",
    "\n",
    "    for x,y in zip(hr_train_vec1, hr_train_vec2):\n",
    "        # Reconstitute matrices from flattened representation\n",
    "        adj1 = MatrixVectorizer.anti_vectorize(x, hr_dim).astype(float)\n",
    "        hr_train1.append(adj1 + np.eye(hr_dim))\n",
    "\n",
    "        adj2 = MatrixVectorizer.anti_vectorize(y, hr_dim).astype(float)\n",
    "        hr_train2.append(adj2 + np.eye(hr_dim))\n",
    "\n",
    "    for x,y in zip(lr_train_vec3,hr_train_vec3):\n",
    "        adj1 = MatrixVectorizer.anti_vectorize(x, lr_dim).astype(float)\n",
    "        lr_train3.append(adj1 + np.eye(lr_dim))\n",
    "\n",
    "        adj2 = MatrixVectorizer.anti_vectorize(y, hr_dim).astype(float)\n",
    "        hr_train3.append(adj2 + np.eye(hr_dim))\n",
    "\n",
    "    return np.array(lr_train1), np.array(lr_train2), np.array(lr_train3), \\\n",
    "            np.array(hr_train1), np.array(hr_train2), np.array(hr_train3)\n",
    "\n",
    "\n",
    "def get_svd_dataset(adjs, k=1):\n",
    "    \"\"\" Convert adajcency matrices to their rank-k approximations. \"\"\"\n",
    "    svd_approxes = []\n",
    "    for adj in adjs:\n",
    "        U, S, Vt = np.linalg.svd(adj)\n",
    "        svd_approx = U[:, 0:k] @ np.diag(S[0:k]) @ Vt[0:k, :]\n",
    "        svd_approxes.append(svd_approx)\n",
    "    return np.array(svd_approxes)\n",
    "\n",
    "\n",
    "lr_train1, lr_train2, lr_train3, hr_train1, hr_train2, hr_train3 = get_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_items_train = [lr_train1, lr_train2, lr_train3]\n",
    "graph_items_test = [ hr_train1, hr_train2, hr_train3]\n",
    "\n",
    "print(lr_train1.shape)\n",
    "print(lr_train2.shape)\n",
    "print(lr_train3.shape)\n",
    "print(hr_train1.shape)\n",
    "print(hr_train2.shape)\n",
    "print(hr_train3.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GCNConv(torch.nn.Module):\n",
    "    \"\"\"A single GCN layer without non-linear activation.\"\"\"\n",
    "\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.W = self.init_parameters()\n",
    "\n",
    "    def init_parameters(self):\n",
    "        weight = torch.zeros(self.in_dim, self.out_dim)\n",
    "        stdv = 1.0 / np.sqrt(weight.size(1))\n",
    "        weight.uniform_(-stdv, stdv)\n",
    "        return nn.Parameter(weight)\n",
    "\n",
    "    def forward(self, features, adjacency):\n",
    "        return (adjacency) @ features @ self.W\n",
    "\n",
    "\n",
    "class GCNBlock(torch.nn.Module):\n",
    "    \"\"\"A GCN block with activation.\"\"\"\n",
    "\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.conv = GCNConv(in_dim, out_dim)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, features, adjacency):\n",
    "        x = features\n",
    "        x = self.conv(x, adjacency)\n",
    "        x = self.activation(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class GraphSvdModel(torch.nn.Module):\n",
    "    \"\"\"Graph SVD model for brain graph super-resolution.\"\"\"\n",
    "\n",
    "    def __init__(self, in_dim, gcn_hidden_dim, out_dim, rank):\n",
    "        \"\"\"\n",
    "        Constructs the GraphSVD model.\n",
    "\n",
    "        Args:\n",
    "            in_dim (int): the node attribute size\n",
    "            gcn_hidden_dim (int): the hidden layers node attribute size\n",
    "            out_dim (int): the number of nodes in the high resolution graph output\n",
    "            rank (int): the number of singular values & vectors to use\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.rank = rank\n",
    "\n",
    "        # Construct GCN encoder layers\n",
    "        self.conv1 = GCNBlock(in_dim, gcn_hidden_dim)\n",
    "        self.conv2 = GCNBlock(gcn_hidden_dim, gcn_hidden_dim)\n",
    "        self.conv3 = GCNBlock(gcn_hidden_dim, gcn_hidden_dim)\n",
    "        self.conv4 = GCNBlock(gcn_hidden_dim, gcn_hidden_dim)\n",
    "        self.conv5 = GCNBlock(gcn_hidden_dim, gcn_hidden_dim)\n",
    "\n",
    "        # Construct U matrix layers\n",
    "        self.svd_u_fc1 = nn.Linear(gcn_hidden_dim, gcn_hidden_dim, bias=False)\n",
    "        self.svd_u_fc2 = nn.Linear(gcn_hidden_dim, gcn_hidden_dim, bias=False)\n",
    "        self.svd_u_proj = nn.Linear(\n",
    "            gcn_hidden_dim, self.rank * self.out_dim, bias=False\n",
    "        )\n",
    "\n",
    "        # Construct V matrix layers\n",
    "        self.svd_v_fc1 = nn.Linear(gcn_hidden_dim, gcn_hidden_dim, bias=False)\n",
    "        self.svd_v_fc2 = nn.Linear(gcn_hidden_dim, gcn_hidden_dim, bias=False)\n",
    "        self.svd_v_proj = nn.Linear(\n",
    "            gcn_hidden_dim, self.rank * self.out_dim, bias=False\n",
    "        )\n",
    "\n",
    "        # Construct S matrix layers\n",
    "        self.svd_sv_fc1 = nn.Linear(gcn_hidden_dim, gcn_hidden_dim, bias=False)\n",
    "        self.svd_sv_fc2 = nn.Linear(gcn_hidden_dim, gcn_hidden_dim, bias=False)\n",
    "        self.sv_proj = nn.Linear(gcn_hidden_dim, rank, bias=False)\n",
    "\n",
    "    def forward(self, features, adjacency):\n",
    "        \"\"\"\n",
    "        Run a forward pass of the model.\n",
    "\n",
    "        Args:\n",
    "            features (torch.tensor): batched node features\n",
    "            adjacency (torch.tensor): batched adjacency matrics\n",
    "\n",
    "        Returns:\n",
    "            torch.tensor of batched super-resolved adjacency matrices\n",
    "\n",
    "        \"\"\"\n",
    "        x = features\n",
    "\n",
    "        # Run encoder message passing\n",
    "        x = self.conv1(x, adjacency)\n",
    "        x = self.conv2(x, adjacency)\n",
    "        x = self.conv3(x, adjacency)\n",
    "        x = self.conv4(x, adjacency)\n",
    "        x = self.conv5(x, adjacency)\n",
    "\n",
    "        # Calculate graph-level embedding\n",
    "        x = torch.sum(x, 1, keepdims=True)\n",
    "\n",
    "        # Decode U matrix\n",
    "        svd_u = self.svd_u_fc1(x)\n",
    "        svd_u = F.tanh(svd_u)\n",
    "        svd_u = self.svd_u_fc2(svd_u)\n",
    "        svd_u = F.tanh(svd_u)\n",
    "        svd_u = self.svd_u_proj(svd_u)\n",
    "\n",
    "        # Decode V matrix\n",
    "        svd_v = self.svd_v_fc1(x)\n",
    "        svd_v = F.tanh(svd_v)\n",
    "        svd_v = self.svd_v_fc2(svd_v)\n",
    "        svd_v = F.tanh(svd_v)\n",
    "        svd_v = self.svd_v_proj(svd_v)\n",
    "\n",
    "        # Deocde S matrix\n",
    "        sv = self.svd_sv_fc1(x)\n",
    "        sv = F.tanh(sv)\n",
    "        sv = self.svd_sv_fc2(sv)\n",
    "        sv = F.tanh(sv)\n",
    "        sv = self.sv_proj(sv)\n",
    "\n",
    "        # Reshape flattened matrices\n",
    "        svd_u = torch.reshape(svd_u, (features.shape[0], self.out_dim, self.rank))\n",
    "        svd_v = torch.reshape(svd_v, (features.shape[0], self.out_dim, self.rank))\n",
    "        sv = torch.diag_embed(sv).squeeze(1)\n",
    "\n",
    "        # Compute low-rank approximation\n",
    "        approx = svd_u @ sv @ torch.transpose(svd_v, 1, 2)\n",
    "\n",
    "        return approx\n",
    "\n",
    "def test_model(model, test_adj, test_labels,\n",
    "               source_res=160,feature_dim=50, HR_size=268):\n",
    "    \"\"\"\n",
    "    Test the GAN AGSR model function\n",
    "\n",
    "    :param model: The trained GAN model\n",
    "    :param test_adj: The adjacency matrices of the test subjects\n",
    "    :param test_labels: The labels of the test subjects\n",
    "    :param args: The arguments for the model\n",
    "    :return: The mean absolute error of the model on the test data\n",
    "    \"\"\"\n",
    "    def cal_error(model_outputs, hr):\n",
    "        return torch.nn.L1Loss(model_outputs, hr)\n",
    "\n",
    "    model.eval()\n",
    "    features = torch.ones(test_adj.shape[0], source_res, feature_dim).to(DEVICE)\n",
    "    test_error = []\n",
    "    predictions = []\n",
    "\n",
    "    # TESTING\n",
    "    with torch.no_grad():\n",
    "            test_adj = torch.from_numpy(test_adj).type(torch.FloatTensor).to(DEVICE)\n",
    "            test_labels = torch.from_numpy(test_labels).type(torch.FloatTensor).to(DEVICE)\n",
    "            preds = model(features,test_adj)\n",
    "            # evaluate_all(hr,preds)\n",
    "            predictions.append(preds)\n",
    "            error = torch.nn.functional.l1_loss(preds, test_labels)\n",
    "            \n",
    "\n",
    "    return error\n",
    "\n",
    "def train_model(\n",
    "    lr_train,\n",
    "    hr_train,\n",
    "    learning_rate=0.0001,\n",
    "    batch_size=16,\n",
    "    num_epochs=1000,\n",
    "    rank=10,\n",
    "    feature_dim=50,\n",
    "    hidden_dim=50,\n",
    "    source_res=160,\n",
    "    target_res=268,\n",
    "    step_size=500,\n",
    "    gamma=0.1,\n",
    "    test_adj=None,\n",
    "    test_ground_truth=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Main model training loop\n",
    "\n",
    "    Args:\n",
    "        lr_train (np.ndarray): batched low-resolution training dataset\n",
    "        hr_train (np.ndarray): batched high-resolution training dataset\n",
    "        learning_rate (float): learning rate to use in the optimiser\n",
    "        batch_size (int): mini-batch size\n",
    "        num_epochs (int): number of epochs to train for\n",
    "        rank (int): rank of approximation to use in model\n",
    "        feature_dim (int): dimension of input node features\n",
    "        hidden_dim (int): encoder hidden dimension\n",
    "        source_res (int): number of nodes in low-resolution graph\n",
    "        target_res (int): number of nodes in high-resolution graph\n",
    "        step_size (int): frequency (in steps) of learning rate scheduler decay\n",
    "        gamma (float): decay factor to be applied to learning rate every (step_size) steps\n",
    "\n",
    "    Returns:\n",
    "\n",
    "        model (GraphSVD): a trained model\n",
    "    \"\"\"\n",
    "\n",
    "    model = GraphSvdModel(\n",
    "        in_dim=feature_dim, gcn_hidden_dim=hidden_dim, out_dim=target_res, rank=rank\n",
    "    )\n",
    "    model = model.to(DEVICE)\n",
    "    model.train()\n",
    "    best_mae = np.inf  # Initialize best mean absolute error\n",
    "\n",
    "    print(\n",
    "        f\"Number of parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\\n\"\n",
    "    )\n",
    "\n",
    "    # Construct dataset\n",
    "    x_train = torch.ones(lr_train.shape[0], source_res, feature_dim).to(DEVICE)\n",
    "    lr_train_tensor = torch.tensor(lr_train, dtype=torch.float32).to(DEVICE)\n",
    "    hr_train_tensor = torch.tensor(hr_train, dtype=torch.float32).to(DEVICE)\n",
    "    train_dataset = TensorDataset(x_train, lr_train_tensor, hr_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    loss_fn = torch.nn.L1Loss()\n",
    "    optimizer = torch.optim.Adamax(model.parameters(), lr=learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "        optimizer, step_size=step_size, gamma=gamma\n",
    "    )\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, data in enumerate(train_loader):\n",
    "            x_train, lr, hr = data\n",
    "            optimizer.zero_grad()\n",
    "            out = model(x_train, lr)\n",
    "            loss = loss_fn(out, hr)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Evaluate on test data if provided\n",
    "        if test_adj is not None and test_ground_truth is not None:\n",
    "            test_error = test_model(model, test_adj, test_ground_truth)\n",
    "\n",
    "            if test_error < best_mae:\n",
    "                best_mae = test_error\n",
    "                early_stop_count = 0\n",
    "                best_model = copy.deepcopy(model)\n",
    "            elif early_stop_count >= 300:\n",
    "                # Early stopping condition met\n",
    "                if test_adj is not None and test_ground_truth is not None:\n",
    "                    test_error = test_model(\n",
    "                        best_model, test_adj, test_ground_truth)\n",
    "                    print(f\"Val Error: {test_error:.6f}\")\n",
    "                return best_model\n",
    "            else:\n",
    "                early_stop_count += 1\n",
    "\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch}: \\t loss: {loss.item()} val loss: {test_error}\")\n",
    "        scheduler.step()\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def predict(model, lr_data, source_res=160, feature_dim=50):\n",
    "    \"\"\"Get model predictions on provided dataset.\"\"\"\n",
    "    features = torch.ones(lr_data.shape[0], source_res, feature_dim).to(DEVICE)\n",
    "    lr_data = torch.tensor(lr_data, dtype=torch.float32).to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        preds = torch.clamp(model(features, lr_data).squeeze(0), 0, 1).cpu().numpy()\n",
    "    return preds\n",
    "\n",
    "\n",
    "def write_predictions(filename, preds):\n",
    "    \"\"\"Save model predictions to disk.\"\"\"\n",
    "    preds_vectorised = []\n",
    "    for pred in preds:\n",
    "        preds_vectorised.append(MatrixVectorizer.vectorize(pred))\n",
    "    preds_vectorised = np.array(preds_vectorised).flatten()\n",
    "\n",
    "    with open(filename, \"w\") as outfile:\n",
    "        outfile.write(\"ID,Predicted\\n\")\n",
    "        for i, pred in enumerate(preds_vectorised):\n",
    "            outfile.write(f\"{i+1},{pred}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "kf = KFold(n_splits=3, shuffle=True, random_state=random_seed)\n",
    "predictions = []\n",
    "ground_truths = []\n",
    "\n",
    "best_model_fold_list = []\n",
    "data_fold_list = []\n",
    "i = 1\n",
    "# Store the fold results\n",
    "fold_results = []\n",
    "models = []\n",
    "print(f\"Starting Cross Validation.\")\n",
    "for index in range(len(graph_items_test)):\n",
    "    print(f\"----- Fold {index} -----\")\n",
    "\n",
    "    train = graph_items_train[index]\n",
    "    test = graph_items_test[index]\n",
    "\n",
    "    # subjects_adj, test_adj, subjects_ground_truth, test_ground_truth = (\n",
    "    #     X[train_index],\n",
    "    #     X[test_index],\n",
    "    #     Y[train_index],\n",
    "    #     Y[test_index],\n",
    "    # )\n",
    "    # data_fold_list.append(\n",
    "    #     (subjects_adj, test_adj, subjects_ground_truth, test_ground_truth)\n",
    "    # )\n",
    "    # Create a deep copy of list1\n",
    "    new_train = copy.deepcopy(graph_items_train)\n",
    "    new_train.pop(index)\n",
    "    new_test = copy.deepcopy(graph_items_test)\n",
    "    new_test.pop(index)\n",
    "    new_train = np.concatenate(new_train, axis=0)\n",
    "    new_test = np.concatenate(new_test, axis=0)\n",
    "    \n",
    "    # lr_train_split = lr_train[train_idxs]\n",
    "    # hr_train_split = hr_train[train_idxs]\n",
    "    # lr_val_split = lr_train[val_idxs]\n",
    "    # hr_val_split = hr_train[val_idxs]\n",
    "\n",
    "    model = train_model(new_train, new_test, test_adj=train, test_ground_truth=test)\n",
    "    models.append(model)\n",
    "    preds = predict(model, train)\n",
    "    predictions.append(preds)\n",
    "    ground_truths.append(new_test)\n",
    "    evaluate_all(test,preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation_metric import evaluate_all\n",
    "# final_model = model\n",
    "# final_model.eval()\n",
    "with torch.no_grad():\n",
    "    for model in models:\n",
    "        pred_train_matrices = []\n",
    "        for j, test_adj in enumerate(train):\n",
    "            model.eval()\n",
    "            pred = model(torch.from_numpy(test_adj))[0]\n",
    "            pred = torch.clamp(pred, min=0.0, max=1.0)\n",
    "            pred = pred.cpu()\n",
    "            pred_train_matrices.append(pred)\n",
    "\n",
    "        print(\"Train\")\n",
    "        pred_train_matrices = np.array(pred_train_matrices)\n",
    "        evaluate_all(test, pred_train_matrices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise_prediction(prediction, target):\n",
    "    \"\"\"Plot prediction vs target matrix\"\"\"\n",
    "    fig, axs = plt.subplots(1, 3)\n",
    "    axs[0].imshow(target)\n",
    "    axs[0].set_title(\"Ground Truth\")\n",
    "\n",
    "    axs[1].imshow(prediction)\n",
    "    axs[1].set_title(\"Prediction\")\n",
    "\n",
    "    axs[2].imshow(target - prediction)\n",
    "    axs[2].set_title(\"Residual\")\n",
    "\n",
    "    fig.set_size_inches(15, 9)\n",
    "    fig.savefig(\"figures/prediction.pdf\", bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "i = 5\n",
    "target = ground_truths[0][i]\n",
    "prediction = predictions[0][i]\n",
    "visualise_prediction(prediction, target)\n",
    "print(f\"MAE: {np.abs(target-prediction).sum()/(target.shape[0]**2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise_prediction(prediction, target):\n",
    "    \"\"\"Plot prediction vs target matrix\"\"\"\n",
    "    fig, axs = plt.subplots(1, 3)\n",
    "    axs[0].imshow(target)\n",
    "    axs[0].set_title(\"Ground Truth\")\n",
    "\n",
    "    axs[1].imshow(prediction)\n",
    "    axs[1].set_title(\"Prediction\")\n",
    "\n",
    "    axs[2].imshow(target - prediction)\n",
    "    axs[2].set_title(\"Residual\")\n",
    "\n",
    "    fig.set_size_inches(15, 9)\n",
    "    fig.savefig(\"figures/prediction.pdf\", bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "i = 5\n",
    "target = ground_truths[0][i]\n",
    "prediction = predictions[0][i]\n",
    "visualise_prediction(prediction, target)\n",
    "print(f\"MAE: {np.abs(target-prediction).sum()/(target.shape[0]**2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a sample of predictions\n",
    "fig, axs = plt.subplots(1, 5)\n",
    "for i in range(5):\n",
    "    axs[i].imshow(predictions[1][i])\n",
    "    axs[i].get_xaxis().set_ticks([])\n",
    "    axs[i].get_yaxis().set_ticks([])\n",
    "fig.set_size_inches(14, 8)\n",
    "fig.savefig(\"figures/prediction_examples.pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute evaluations (note this is extremely slow due to the betweeness centrality calculation)\n",
    "evaluations = []\n",
    "for i, (pred, gt) in enumerate(zip(predictions, ground_truths)):\n",
    "    evaluation = evaluate(pred.shape[0], pred, gt)\n",
    "    evaluations.append(evaluation)\n",
    "    write_predictions(f\"predictions_fold_{i+1}.csv\", pred)\n",
    "print(evaluations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cv_results(evaluations):\n",
    "    \"\"\"Plot bar charts of evaluation results required in report.\"\"\"\n",
    "    df_dict = {\n",
    "        \"Fold\": [],\n",
    "        \"MAE\": [],\n",
    "        \"PCC\": [],\n",
    "        \"JSD\": [],\n",
    "        \"MAE (PC)\": [],\n",
    "        \"MAE (EC)\": [],\n",
    "        \"MAE (BC)\": [],\n",
    "    }\n",
    "    for i, evaluation in enumerate(evaluations):\n",
    "        df_dict[\"Fold\"].append(i + 1)\n",
    "        df_dict[\"MAE\"].append(evaluation[\"mae\"])\n",
    "        df_dict[\"PCC\"].append(evaluation[\"pcc\"])\n",
    "        df_dict[\"JSD\"].append(evaluation[\"js_dis\"])\n",
    "        df_dict[\"MAE (PC)\"].append(evaluation[\"avg_mae_pc\"])\n",
    "        df_dict[\"MAE (EC)\"].append(evaluation[\"avg_mae_ec\"])\n",
    "        df_dict[\"MAE (BC)\"].append(evaluation[\"avg_mae_bc\"])\n",
    "\n",
    "    df = pd.DataFrame(df_dict)\n",
    "    df.to_csv(\"evaluations.csv\")\n",
    "\n",
    "    # Split into distributional and topological measures\n",
    "    df_dbn = df[[\"MAE\", \"PCC\", \"JSD\"]]\n",
    "    df_centrality = df[[\"MAE (PC)\", \"MAE (EC)\", \"MAE (BC)\"]]\n",
    "\n",
    "    plot_folds(df_dbn, \"distribution_measures\")\n",
    "    plot_folds(df_centrality, \"centrality_measures\")\n",
    "\n",
    "\n",
    "def plot_folds(df, name):\n",
    "    \"\"\"Plot results across folds.\"\"\"\n",
    "    colors = [\"tab:blue\", \"tab:orange\", \"tab:green\"]\n",
    "    fig, axs = plt.subplots(2, 2)\n",
    "    fig.set_size_inches((8, 8))\n",
    "    axs[0][0].bar(df.columns.values, df.iloc[0, :], color=colors)\n",
    "    axs[0][1].bar(df.columns.values, df.iloc[1, :], color=colors)\n",
    "    axs[1][0].bar(df.columns.values, df.iloc[2, :], color=colors)\n",
    "    axs[1][1].bar(\n",
    "        df.columns.values, df.mean(), color=colors, yerr=2 * df.std().values, capsize=5\n",
    "    )\n",
    "    axs[0][0].set_title(\"Fold 1\")\n",
    "    axs[0][1].set_title(\"Fold 2\")\n",
    "    axs[1][0].set_title(\"Fold 3\")\n",
    "    axs[1][1].set_title(\"Avg Across Folds\")\n",
    "    fig.savefig(f\"figures/{name}.pdf\", bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_cv_results(evaluations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final model on all the data for kaggle submission\n",
    "final_model = train_model(lr_train, hr_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_preds = predict(final_model, lr_test, feature_dim=50)\n",
    "print(kaggle_preds.min(), kaggle_preds.max()) # sanity check: should be between 0 and 1\n",
    "write_predictions(\"predictions.csv\", kaggle_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the performance metrics for a random baseline model\n",
    "random_baseline = np.random.rand(hr_train.shape[0], 268, 268)\n",
    "print(random_baseline.shape)\n",
    "evaluate(random_baseline.shape[0], random_baseline, hr_train[0:random_baseline.shape[0]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dgl-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
