{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4226c705cb6f1373",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# GSAT-Net Structure\n",
    "```\n",
    "GSATNet(\n",
    "  (layer): GSRLayer()\n",
    "  (net): GraphUnet(\n",
    "    (start_gcnpro): GCNPro(\n",
    "      (proj): Linear(in_features=160, out_features=320, bias=True)\n",
    "      (drop): Dropout(p=0.1, inplace=False)\n",
    "    )\n",
    "    (bottom_gcnpro): GCNPro(\n",
    "      (proj): Linear(in_features=320, out_features=320, bias=True)\n",
    "      (drop): Dropout(p=0.1, inplace=False)\n",
    "    )\n",
    "    (end_gcnpro): GCNPro(\n",
    "      (proj): Linear(in_features=640, out_features=320, bias=True)\n",
    "      (drop): Dropout(p=0.1, inplace=False)\n",
    "    )\n",
    "  )\n",
    "  (gc1): GraphConvolution()\n",
    "  (gc2): GraphConvolution()\n",
    ")\n",
    "Discriminator(\n",
    "  (dense_1): Dense()\n",
    "  (relu_1): ReLU()\n",
    "  (dense_2): Dense()\n",
    "  (relu_2): ReLU()\n",
    "  (dense_3): Dense()\n",
    "  (sigmoid): Sigmoid()\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d087a986b3e38a9",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Libraries and Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08e000d-613c-46c4-b896-de8c22238bf8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T20:33:04.744868300Z",
     "start_time": "2024-05-23T20:33:03.436309200Z"
    },
    "id": "c08e000d-613c-46c4-b896-de8c22238bf8"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set up random seed\n",
    "random_seed = 42\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "# Check for CUDA (GPU support) and set device accordingly\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"CUDA is available. Using GPU.\")\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed)  # For multi-GPU setups\n",
    "    # Additional settings for ensuring reproducibility on CUDA\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.set_default_device(device)\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDA not available. Using CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09c7c706d958ae0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8174fb22fdbd33af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T20:33:05.177324300Z",
     "start_time": "2024-05-23T20:33:04.995918900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from MatrixVectorizer import MatrixVectorizer\n",
    "\n",
    "# Loading dataset\n",
    "lr_train_path = './data/lr_train.csv'\n",
    "lr_test_path = './data/lr_test.csv'\n",
    "hr_train_path = './data/hr_train.csv'\n",
    "\n",
    "lr_train_pd = pd.read_csv(lr_train_path)\n",
    "hr_train_pd = pd.read_csv(hr_train_path)\n",
    "lr_test_pd = pd.read_csv(lr_test_path)\n",
    "\n",
    "# Converting each row into a numpy.ndarray explicitly\n",
    "lr_train = np.array(lr_train_pd.apply(lambda x: np.array(x), axis=1).tolist())\n",
    "hr_train = np.array(hr_train_pd.apply(lambda x: np.array(x), axis=1).tolist())\n",
    "lr_test = np.array(lr_test_pd.apply(lambda x: np.array(x), axis=1).tolist())\n",
    "\n",
    "matrix_vectorizer = MatrixVectorizer()\n",
    "lr_train_A = []\n",
    "hr_train_A = []\n",
    "lr_test_A = []\n",
    "\n",
    "for g in lr_train:\n",
    "    lr_train_A.append(matrix_vectorizer.anti_vectorize(g, 160))\n",
    "for g in hr_train:\n",
    "    hr_train_A.append(matrix_vectorizer.anti_vectorize(g, 268))\n",
    "for g in lr_test:\n",
    "    lr_test_A.append(matrix_vectorizer.anti_vectorize(g, 160))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8650f10b71d72296",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a274a9-325f-4eae-9155-165a768b3841",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T20:33:06.506337500Z",
     "start_time": "2024-05-23T20:33:06.471678600Z"
    },
    "id": "b2a274a9-325f-4eae-9155-165a768b3841"
   },
   "outputs": [],
   "source": [
    "def weight_variable_glorot(output_dim):\n",
    "\n",
    "    input_dim = output_dim\n",
    "    init_range = np.sqrt(6.0 / (input_dim + output_dim))\n",
    "    initial = np.random.uniform(-init_range, init_range,\n",
    "                                (input_dim, output_dim))\n",
    "\n",
    "    return initial\n",
    "\n",
    "\n",
    "class GSRLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    GSR layer, from https://github.com/basiralab/AGSR-Net.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hr_dim):\n",
    "        super(GSRLayer, self).__init__()\n",
    "\n",
    "        self.weights = torch.from_numpy(\n",
    "            weight_variable_glorot(hr_dim)).type(torch.FloatTensor).to(device)\n",
    "        self.weights = torch.nn.Parameter(\n",
    "            data=self.weights, requires_grad=True)\n",
    "\n",
    "    def forward(self, A, X):\n",
    "        with torch.autograd.set_detect_anomaly(True):\n",
    "\n",
    "            lr = A\n",
    "            lr_dim = lr.shape[0]\n",
    "            f = X\n",
    "            eig_val_lr, U_lr = torch.linalg.eigh(lr, UPLO='U')\n",
    "\n",
    "            # U_lr = torch.abs(U_lr)\n",
    "            eye_mat = torch.eye(lr_dim).type(torch.FloatTensor).to(device)\n",
    "            s_d = torch.cat((eye_mat, eye_mat), 0)\n",
    "\n",
    "            a = torch.matmul(self.weights, s_d)\n",
    "            b = torch.matmul(a, torch.t(U_lr))\n",
    "            f_d = torch.matmul(b, f)\n",
    "            f_d = torch.abs(f_d)\n",
    "            f_d = f_d.fill_diagonal_(1)\n",
    "            adj = f_d\n",
    "\n",
    "            X = torch.mm(adj, adj.t())\n",
    "            X = (X + X.t())/2\n",
    "            X = X.fill_diagonal_(1)\n",
    "        return adj, torch.abs(X)\n",
    "\n",
    "\n",
    "class GraphConvolution(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, dropout, act=F.relu):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.dropout = dropout\n",
    "        self.act = act\n",
    "        self.weight = torch.nn.Parameter(\n",
    "            torch.FloatTensor(in_features, out_features))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        torch.nn.init.xavier_uniform_(self.weight)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        input = F.dropout(input, self.dropout, self.training)\n",
    "        support = torch.mm(input, self.weight)\n",
    "        output = torch.mm(adj, support)\n",
    "        output = self.act(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "class GraphUnpool(nn.Module):\n",
    "    \"\"\"\n",
    "    Unpooling, from https://github.com/basiralab/AGSR-Net.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(GraphUnpool, self).__init__()\n",
    "\n",
    "    def forward(self, A, X, idx):\n",
    "        new_X = torch.zeros([A.shape[0], X.shape[1]], device=device)\n",
    "        new_X[idx] = X\n",
    "        return A, new_X\n",
    "\n",
    "\n",
    "class GraphPool(nn.Module):\n",
    "    \"\"\"\n",
    "    Pooling, from https://github.com/basiralab/AGSR-Net.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, k, in_dim):\n",
    "        super(GraphPool, self).__init__()\n",
    "        self.k = k\n",
    "        self.proj = nn.Linear(in_dim, 1).to(device)\n",
    "        self.sigmoid = nn.Sigmoid().to(device)\n",
    "\n",
    "    def forward(self, A, X):\n",
    "        scores = self.proj(X)\n",
    "        # scores = torch.abs(scores)\n",
    "        scores = torch.squeeze(scores)\n",
    "        scores = self.sigmoid(scores/100)\n",
    "        num_nodes = A.shape[0]\n",
    "        values, idx = torch.topk(scores, int(self.k*num_nodes))\n",
    "        new_X = X[idx, :]\n",
    "        values = torch.unsqueeze(values, -1)\n",
    "        new_X = torch.mul(new_X, values)\n",
    "        A = A[idx, :]\n",
    "        A = A[:, idx]\n",
    "        return A, new_X, idx\n",
    "\n",
    "\n",
    "class GCNPro(nn.Module):\n",
    "    \"\"\"\n",
    "    GCN Pro layer.\n",
    "    It aggregates information from a node's neighbors using mean aggregation, processed by activation function and drop 10% nodes to prevent overfitting.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, activation=F.relu, p = 0):\n",
    "        super(GCNPro, self).__init__()\n",
    "        self.proj = nn.Linear(in_features, out_features).to(device)\n",
    "        self.activation = activation\n",
    "        self.drop = nn.Dropout(p=p).to(device)\n",
    "    \n",
    "    def mean_normalization(self, A):\n",
    "        e = 1e-7\n",
    "        D = torch.diag(A.sum(1))\n",
    "        D_inv = torch.diag(1.0 / (D.diag()+e))\n",
    "        A_tilde = A @ D_inv + torch.eye(A.size(0)).to(device)\n",
    "        return A_tilde.float()\n",
    "    \n",
    "    def forward(self, A, X):\n",
    "        X = self.drop(X)\n",
    "        X = self.proj(X)\n",
    "        A_norm = self.mean_normalization(A)\n",
    "        X = torch.matmul(A_norm, X)\n",
    "        X = self.activation(X)\n",
    "        return X\n",
    "    \n",
    "    \n",
    "class GAT(nn.Module):\n",
    "    \"\"\"\n",
    "    GAT layer.\n",
    "    It applies an attention mechanism in the graph convolution process, allowing the model to focus on different parts of the neighborhood of each node.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, activation = F.relu, p = 0):\n",
    "        super(GAT, self).__init__()\n",
    "        # Initialize the weights, bias, and attention parameters as\n",
    "        # trainable parameters\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        self.bias = nn.Parameter(torch.zeros(out_features))\n",
    "        self.phi = nn.Parameter(torch.FloatTensor(2 * out_features, 1))\n",
    "        self.activation = activation\n",
    "        self.drop = nn.Dropout(p=p).to(device)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / np.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "\n",
    "        stdv = 1. / np.sqrt(self.phi.size(1))\n",
    "        self.phi.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, adj, input):\n",
    "        # Apply linear transformation and add bias\n",
    "        ones_tensor = torch.ones(self.bias.shape[0])\n",
    "        input = self.drop(input)\n",
    "        X_new = self.bias + torch.matmul(input, self.weight)\n",
    "        N = X_new.size(0)\n",
    "        # Compute the attention scores\n",
    "        expanded = X_new.unsqueeze(0).expand(N, -1, -1)\n",
    "        combined = torch.cat((expanded, expanded.transpose(0, 1)), dim=2)\n",
    "        S = torch.matmul(combined, self.phi).squeeze(2)\n",
    "        S = nn.LeakyReLU()(S)\n",
    "        # Compute mask based on adjacency matrix and apply to the pre-attention matrix\n",
    "        mask = (adj + torch.eye(N)).bool()\n",
    "        S_masked = torch.where(mask, S, torch.tensor(float('-inf')))\n",
    "        # Compute attention weights using softmax\n",
    "        attention_activated = torch.softmax(S_masked, dim=1)\n",
    "        # Aggregate features based on attention weights\n",
    "        h = torch.matmul(attention_activated, X_new)\n",
    "    \n",
    "        return self.activation(h) if self.activation else h\n",
    "    \n",
    "\n",
    "class GraphUnet(nn.Module):\n",
    "    \"\"\"\n",
    "    Graph Unet layer, consisting of 3 GCN Pro layers and 2 GAT layers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ks, in_dim, out_dim, dim=320):\n",
    "        super(GraphUnet, self).__init__()\n",
    "        self.ks = ks\n",
    "\n",
    "        self.start_gcnpro = GCNPro(in_dim, dim, p=0.1).to(device)\n",
    "        self.bottom_gcnpro = GCNPro(dim, dim, p=0.1).to(device)\n",
    "        self.end_gcnpro = GCNPro(2*dim, out_dim, p=0.1).to(device)\n",
    "        self.down_gcns = []\n",
    "        self.up_gcns = []\n",
    "        self.pools = []\n",
    "        self.unpools = []\n",
    "        self.l_n = len(ks)\n",
    "        for i in range(self.l_n):\n",
    "            self.down_gcns.append(GAT(dim, dim).to(device))\n",
    "            self.up_gcns.append(GAT(dim, dim).to(device))\n",
    "            self.pools.append(GraphPool(ks[i], dim).to(device))\n",
    "            self.unpools.append(GraphUnpool().to(device))\n",
    "\n",
    "    def forward(self, A, X):\n",
    "        adj_ms = []\n",
    "        indices_list = []\n",
    "        down_outs = []\n",
    "        X = self.start_gcnpro(A, X)\n",
    "        start_gcn_outs = X\n",
    "        org_X = X\n",
    "        for i in range(self.l_n):\n",
    "\n",
    "            X = self.down_gcns[i](A, X)\n",
    "            adj_ms.append(A)\n",
    "            down_outs.append(X)\n",
    "            A, X, idx = self.pools[i](A, X)\n",
    "            indices_list.append(idx)\n",
    "        X = self.bottom_gcnpro(A, X)\n",
    "        for i in range(self.l_n):\n",
    "            up_idx = self.l_n - i - 1\n",
    "\n",
    "            A, idx = adj_ms[up_idx], indices_list[up_idx]\n",
    "            A, X = self.unpools[i](A, X, idx)\n",
    "            X = self.up_gcns[i](A, X)\n",
    "            X = X.add(down_outs[up_idx])\n",
    "        X = torch.cat([X, org_X], 1)\n",
    "        X = self.end_gcnpro(A, X)\n",
    "\n",
    "        return X, start_gcn_outs\n",
    "\n",
    "\n",
    "def pad_HR_adj(label, split):\n",
    "\n",
    "    label = np.pad(label, ((split, split), (split, split)), mode=\"constant\")\n",
    "    np.fill_diagonal(label, 1)\n",
    "    return label\n",
    "\n",
    "\n",
    "def normalize_adj_torch(mx):\n",
    "    rowsum = mx.sum(1)\n",
    "    r_inv_sqrt = torch.pow(rowsum, -0.5).flatten()\n",
    "    r_inv_sqrt[torch.isinf(r_inv_sqrt)] = 0.\n",
    "    r_mat_inv_sqrt = torch.diag(r_inv_sqrt)\n",
    "    mx = torch.matmul(mx, r_mat_inv_sqrt)\n",
    "    mx = torch.transpose(mx, 0, 1)\n",
    "    mx = torch.matmul(mx, r_mat_inv_sqrt)\n",
    "    return mx\n",
    "\n",
    "\n",
    "def unpad(data, split):\n",
    "\n",
    "    idx_0 = data.shape[0]-split\n",
    "    idx_1 = data.shape[1]-split\n",
    "    train = data[split:idx_0, split:idx_1]\n",
    "    return train\n",
    "\n",
    "class GSATNet(nn.Module):\n",
    "    \"\"\"\n",
    "    GSAT Net layer, consisting of 1 GSR layer, 1 Graph Unet layer, and 2 graph convolution layers. \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ks, lr_dim=160, hr_dim=320, hidden_dim=320):\n",
    "        super(GSATNet, self).__init__()\n",
    "\n",
    "        self.lr_dim = lr_dim\n",
    "        self.hr_dim = hr_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layer = GSRLayer(self.hr_dim).to(device)\n",
    "        self.net = GraphUnet(ks, self.lr_dim, self.hr_dim).to(device)\n",
    "        self.gc1 = GraphConvolution(\n",
    "            self.hr_dim, self.hidden_dim, 0, act=F.relu).to(device)\n",
    "        self.gc2 = GraphConvolution(\n",
    "            self.hidden_dim, self.hr_dim, 0, act=F.relu).to(device)\n",
    "\n",
    "    def forward(self, lr, lr_dim, hr_dim):\n",
    "        with torch.autograd.set_detect_anomaly(True):\n",
    "\n",
    "            I = torch.eye(self.lr_dim).type(torch.FloatTensor).to(device)\n",
    "            A = normalize_adj_torch(lr).type(torch.FloatTensor).to(device)\n",
    "            \n",
    "\n",
    "            self.net_outs, self.start_gcn_outs = self.net(A, I)\n",
    "\n",
    "            self.outputs, self.Z = self.layer(A, self.net_outs)\n",
    "\n",
    "            self.hidden1 = self.gc1(self.Z, self.outputs)\n",
    "            self.hidden2 = self.gc2(self.hidden1, self.outputs)\n",
    "            z = self.hidden2\n",
    "\n",
    "            z = (z + z.t())/2\n",
    "            z = z.fill_diagonal_(1)\n",
    "\n",
    "        return torch.abs(z), self.net_outs, self.start_gcn_outs, self.outputs\n",
    "\n",
    "\n",
    "class Dense(nn.Module):\n",
    "    \"\"\"\n",
    "    Dense layer, from https://github.com/basiralab/AGSR-Net.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n1, n2):\n",
    "        super(Dense, self).__init__()\n",
    "        self.weights = torch.nn.Parameter(\n",
    "            torch.FloatTensor(n1, n2), requires_grad=True)\n",
    "        nn.init.normal_(self.weights, mean=0, std=0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        np.random.seed(1)\n",
    "        torch.manual_seed(1)\n",
    "\n",
    "        out = torch.mm(x, self.weights)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    \"\"\"\n",
    "    Discriminator layer, from https://github.com/basiralab/AGSR-Net.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hr_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.dense_1 = Dense(hr_dim, hr_dim).to(device)\n",
    "        self.relu_1 = nn.ReLU(inplace=False)\n",
    "        self.dense_2 = Dense(hr_dim, hr_dim).to(device)\n",
    "        self.relu_2 = nn.ReLU(inplace=False)\n",
    "        self.dense_3 = Dense(hr_dim, 1).to(device)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        np.random.seed(1)\n",
    "        torch.manual_seed(1)\n",
    "        dc_den1 = self.relu_1(self.dense_1(inputs))\n",
    "        dc_den2 = self.relu_2(self.dense_2(dc_den1))\n",
    "        output = dc_den2\n",
    "        output = self.dense_3(dc_den2)\n",
    "        output = self.sigmoid(output)\n",
    "        return torch.abs(output)\n",
    "\n",
    "\n",
    "def gaussian_noise_layer(input_layer):\n",
    "    z = torch.empty_like(input_layer)\n",
    "    noise = z.normal_(mean=0, std=0.1)\n",
    "    z = torch.abs(input_layer + noise)\n",
    "\n",
    "    z = (z + z.t())/2\n",
    "    z = z.fill_diagonal_(1)\n",
    "    return z\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea450717429eaec0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c2098a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T20:33:07.419101500Z",
     "start_time": "2024-05-23T20:33:07.406103900Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict(model, lr_test_A, out_path):\n",
    "    \"\"\"\n",
    "    Predict model results and save them as csv file.\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pred_matrices = []\n",
    "    \n",
    "        for lr in lr_test_A:\n",
    "            lr = torch.from_numpy(lr).type(torch.FloatTensor).to(device)\n",
    "            model_outputs, net_outs, start_gcn_outs, layer_outs = model(lr, 160, 320)\n",
    "            pred_matrices.append(unpad(model_outputs, 26))\n",
    "    \n",
    "        # vectorize and flatten\n",
    "        pred_1d = torch.cat([torch.tensor(MatrixVectorizer.vectorize(m.cpu().detach()).flatten()) for m in pred_matrices])\n",
    "        pred_1d_np = pred_1d.cpu().numpy()\n",
    "    \n",
    "        print(pred_1d_np.shape)\n",
    "\n",
    "    # Create an ID array starting from 1 to the length of pred_1d_np\n",
    "    ids = np.arange(1, len(pred_1d_np) + 1)\n",
    "\n",
    "    # Create a DataFrame with two columns: ID and Predicted\n",
    "    df = pd.DataFrame({\n",
    "        'ID': ids,\n",
    "        'Predicted': pred_1d_np\n",
    "    })\n",
    "\n",
    "    # Output the DataFrame to a CSV file\n",
    "    df.to_csv(out_path, index=False)\n",
    "\n",
    "    return pred_1d_np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531b347321c27731",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa0b014d47c308d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T20:33:08.198405300Z",
     "start_time": "2024-05-23T20:33:08.146642Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set up loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "\n",
    "def train(subjects_adj, subjects_labels, epochs=50, ks=[0.9, 0.7, 0.6, 0.5]):\n",
    "    \"\"\"\n",
    "    Train model and print model structure.\n",
    "    \"\"\"\n",
    "\n",
    "    # Build up model structure\n",
    "    hr_dim=320\n",
    "    lr_dim=160\n",
    "    model = GSATNet(ks).to(device)\n",
    "    print(model)\n",
    "    bce_loss = nn.BCELoss()\n",
    "    netD = Discriminator(320).to(device)\n",
    "    print(netD)\n",
    "    optimizerG = optim.Adam(model.parameters(), lr=0.0001)\n",
    "    optimizerD = optim.Adam(netD.parameters(), lr=0.0001)\n",
    "\n",
    "    # Start training\n",
    "    all_epochs_loss = []\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        with torch.autograd.set_detect_anomaly(True):\n",
    "            epoch_loss = []\n",
    "            epoch_error = []\n",
    "            for lr, hr in zip(subjects_adj, subjects_labels):\n",
    "                optimizerD.zero_grad()\n",
    "                optimizerG.zero_grad()\n",
    "\n",
    "                hr = pad_HR_adj(hr, 26)\n",
    "                lr = torch.from_numpy(lr).type(torch.FloatTensor).to(device)\n",
    "                padded_hr = torch.from_numpy(hr).type(torch.FloatTensor).to(device)\n",
    "\n",
    "                eig_val_hr, U_hr = torch.linalg.eigh(\n",
    "                    padded_hr, UPLO='U')\n",
    "\n",
    "                model_outputs, net_outs, start_gcn_outs, layer_outs = model(\n",
    "                    lr, 160, 320)\n",
    "\n",
    "                mse_loss = 0.1 * criterion(net_outs, start_gcn_outs) + 0.8 * criterion(\n",
    "                    model.layer.weights, U_hr) + criterion(model_outputs, padded_hr)\n",
    "\n",
    "                error = criterion(model_outputs, padded_hr)\n",
    "                real_data = model_outputs.detach()\n",
    "                fake_data = gaussian_noise_layer(padded_hr)\n",
    "\n",
    "                d_real = netD(real_data)\n",
    "                d_fake = netD(fake_data)\n",
    "\n",
    "                dc_loss_real = bce_loss(d_real, torch.ones(hr_dim, 1).to(device))\n",
    "                dc_loss_fake = bce_loss(d_fake, torch.zeros(hr_dim, 1).to(device))\n",
    "                dc_loss = dc_loss_real + dc_loss_fake\n",
    "\n",
    "                dc_loss.backward()\n",
    "                optimizerD.step()\n",
    "\n",
    "                d_fake = netD(gaussian_noise_layer(padded_hr))\n",
    "\n",
    "                gen_loss = bce_loss(d_fake, torch.ones(hr_dim, 1).to(device))\n",
    "                generator_loss = gen_loss + mse_loss\n",
    "                generator_loss.backward()\n",
    "                optimizerG.step()\n",
    "\n",
    "                epoch_loss.append(generator_loss.item())\n",
    "                epoch_error.append(error.item())\n",
    "\n",
    "            # Print training results in every epoch\n",
    "            print(\"Epoch: \", epoch, \"Loss: \", np.mean(epoch_loss),\n",
    "                  \"Error: \", np.mean(epoch_error)*100, \"%\")\n",
    "            all_epochs_loss.append(np.mean(epoch_loss))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f84313922f4a3c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432b0189f14c6936",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T20:33:08.915871200Z",
     "start_time": "2024-05-23T20:33:08.910870100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def bar_plots(eval_data):\n",
    "    \"\"\"\n",
    "    Plot bar charts for evaluation metrics including error bar.\n",
    "    \"\"\"\n",
    "\n",
    "    eval_tags = [\"MAE\", \"PCC\", \"JSD\", \"MAE(PC)\", \"MAE(EC)\", \"MAE(BC)\"]\n",
    "    eval_color = [\"#ff5860\", \"#47b45d\", \"#6666ff\", \"#ffc650\", \"#00ffff\", \"#00ff48\"]\n",
    "\n",
    "    eval_avg = np.mean(eval_data, axis=0)\n",
    "    eval_std = np.std(eval_data, axis=0)\n",
    "    print(f\"Average: {eval_avg}\")\n",
    "    print(f\"Standard Deviation: {eval_std}\")\n",
    "\n",
    "    subplots_num = len(eval_data) + 1\n",
    "    col_num = 2\n",
    "    row_num = subplots_num // col_num if subplots_num % col_num == 0 else subplots_num // col_num + 1\n",
    "\n",
    "    fig, axs = plt.subplots(row_num, col_num, figsize=(10, row_num*4))\n",
    "\n",
    "    for i in range(subplots_num):\n",
    "        row, col = divmod(i, col_num)\n",
    "        if i == subplots_num - 1:\n",
    "            axs[row, col].bar(eval_tags, eval_avg, yerr=eval_std, capsize=5, color=eval_color)\n",
    "            axs[row, col].set_title(f\"Avg. Across Folds\")\n",
    "        else:\n",
    "            axs[row, col].bar(eval_tags, eval_data[i], color=eval_color)\n",
    "            axs[row, col].set_title(f\"Fold {i + 1}\")\n",
    "\n",
    "    if subplots_num % col_num != 0:\n",
    "        fig.delaxes(axs[-1, -1])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26214eea74593511",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f30ce74e4a707d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T20:33:09.831442200Z",
     "start_time": "2024-05-23T20:33:09.683717700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "import networkx as nx\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "padding_top = (320 - 268) // 2\n",
    "padding_bottom = 320 - 268 - padding_top\n",
    "padding_left = padding_top\n",
    "padding_right = padding_bottom\n",
    "\n",
    "\n",
    "# Unpadding matrix\n",
    "def unpad_matrix(matrix, padding_top, padding_bottom, padding_left, padding_right):\n",
    "    new_height = matrix.shape[0] - (padding_top + padding_bottom)\n",
    "    new_width = matrix.shape[1] - (padding_left + padding_right)\n",
    "\n",
    "    unpadded_matrix = matrix[padding_top:new_height+padding_top, padding_left:new_width+padding_left]\n",
    "    return unpadded_matrix\n",
    "\n",
    "\n",
    "def evaluation(model, lr_val_fold, hr_val_fold):\n",
    "    \"\"\"\n",
    "    Evaluate model performance in MAE, PCC, Jensen-Shannon Distance, Average MAE betweenness/eigenvector/PageRank centrality.\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pred_matrices = []\n",
    "        gt_matrices = []\n",
    "    \n",
    "        for lr, hr in zip(lr_val_fold, hr_val_fold):\n",
    "            model_outputs, net_outs, start_gcn_outs, layer_outs = model(torch.tensor(lr, dtype=torch.float32).to(device), 160, 320)\n",
    "            pred_matrices.append(unpad_matrix(model_outputs, padding_top, padding_bottom, padding_left, padding_right))\n",
    "            gt_matrices.append(torch.tensor(hr, dtype=torch.float32))\n",
    "\n",
    "    num_val = len(pred_matrices)\n",
    "    mae_bc = []\n",
    "    mae_ec = []\n",
    "    mae_pc = []\n",
    "    pred_1d_list = []\n",
    "    gt_1d_list = []\n",
    "\n",
    "    # Iterate over each test sample\n",
    "    for i in tqdm(range(num_val)):\n",
    "        # Convert adjacency matrices to NetworkX graphs\n",
    "        pred_graph = nx.from_numpy_array(pred_matrices[i].cpu().detach().numpy(), edge_attr=\"weight\")\n",
    "        gt_graph = nx.from_numpy_array(gt_matrices[i].cpu().detach().numpy(), edge_attr=\"weight\")\n",
    "\n",
    "        # Compute centrality measures\n",
    "        pred_bc = nx.betweenness_centrality(pred_graph, weight=\"weight\")\n",
    "        pred_ec = nx.eigenvector_centrality(pred_graph, weight=\"weight\")\n",
    "        pred_pc = nx.pagerank(pred_graph, weight=\"weight\")\n",
    "\n",
    "        gt_bc = nx.betweenness_centrality(gt_graph, weight=\"weight\")\n",
    "        gt_ec = nx.eigenvector_centrality(gt_graph, weight=\"weight\")\n",
    "        gt_pc = nx.pagerank(gt_graph, weight=\"weight\")\n",
    "\n",
    "        # Convert centrality dictionaries to lists\n",
    "        pred_bc_values = list(pred_bc.values())\n",
    "        pred_ec_values = list(pred_ec.values())\n",
    "        pred_pc_values = list(pred_pc.values())\n",
    "\n",
    "        gt_bc_values = list(gt_bc.values())\n",
    "        gt_ec_values = list(gt_ec.values())\n",
    "        gt_pc_values = list(gt_pc.values())\n",
    "\n",
    "        # Compute MAEs\n",
    "        mae_bc.append(mean_absolute_error(pred_bc_values, gt_bc_values))\n",
    "        mae_ec.append(mean_absolute_error(pred_ec_values, gt_ec_values))\n",
    "        mae_pc.append(mean_absolute_error(pred_pc_values, gt_pc_values))\n",
    "\n",
    "        # Vectorize matrices\n",
    "        pred_1d_list.append(MatrixVectorizer.vectorize(pred_matrices[i].cpu().detach().numpy()))\n",
    "        gt_1d_list.append(MatrixVectorizer.vectorize(gt_matrices[i].cpu().detach().numpy()))\n",
    "\n",
    "    # Compute average MAEs\n",
    "    avg_mae_bc = sum(mae_bc) / len(mae_bc)\n",
    "    avg_mae_ec = sum(mae_ec) / len(mae_ec)\n",
    "    avg_mae_pc = sum(mae_pc) / len(mae_pc)\n",
    "\n",
    "    # Concatenate flattened matrices\n",
    "    pred_1d = np.concatenate(pred_1d_list)\n",
    "    gt_1d = np.concatenate(gt_1d_list)\n",
    "\n",
    "    # Compute metrics\n",
    "    mae = mean_absolute_error(pred_1d, gt_1d)\n",
    "    pcc = pearsonr(pred_1d, gt_1d)[0]\n",
    "    js_dis = jensenshannon(pred_1d, gt_1d)\n",
    "\n",
    "    print(\"MAE: \", mae)\n",
    "    print(\"PCC: \", pcc)\n",
    "    print(\"Jensen-Shannon Distance: \", js_dis)\n",
    "    print(\"Average MAE betweenness centrality:\", avg_mae_bc)\n",
    "    print(\"Average MAE eigenvector centrality:\", avg_mae_ec)\n",
    "    print(\"Average MAE PageRank centrality:\", avg_mae_pc)\n",
    "    \n",
    "    return mae, pcc, js_dis, avg_mae_bc, avg_mae_ec, avg_mae_pc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e873a3da6eec5642",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Three Fold Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5e9e17-fdb2-4cde-b18d-63d670bd1496",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T20:33:10.504453100Z",
     "start_time": "2024-05-23T20:33:10.492451500Z"
    },
    "id": "fd5e9e17-fdb2-4cde-b18d-63d670bd1496"
   },
   "outputs": [],
   "source": [
    "def train_with_kfold(lr_train_A, hr_train_A, train_func, train_paras, fold=3):\n",
    "    \"\"\"\n",
    "    Train model with k(3 in this case) fold validation.\n",
    "    \"\"\"\n",
    "\n",
    "    kfold = KFold(n_splits=fold, shuffle=True, random_state=random_seed)\n",
    "    total_mae = 0\n",
    "    fold = 0\n",
    "    eval_data = []\n",
    "    models = []\n",
    "\n",
    "    lr_train_A_np = np.array(lr_train_A)\n",
    "    hr_train_A_np = np.array(hr_train_A)\n",
    "    for train_idx, val_idx in kfold.split(lr_train_A_np):\n",
    "        fold += 1\n",
    "        print(f\"Fold {fold}\")\n",
    "\n",
    "        # Training and validation split for the current fold\n",
    "        lr_train_fold, hr_train_fold = lr_train_A_np[train_idx], hr_train_A_np[train_idx]\n",
    "        lr_val_fold, hr_val_fold = lr_train_A_np[val_idx], hr_train_A_np[val_idx]\n",
    "\n",
    "        model = train_func(lr_train_fold, hr_train_fold, train_paras[0], train_paras[1])\n",
    "        models.append(model)\n",
    "        mae, pcc, js_dis, avg_mae_bc, avg_mae_ec, avg_mae_pc = evaluation(model, lr_val_fold, hr_val_fold)\n",
    "        eval_data.append([mae, pcc, js_dis, avg_mae_pc, avg_mae_ec, avg_mae_bc])\n",
    "        total_mae += mae\n",
    "\n",
    "    bar_plots(eval_data)\n",
    "\n",
    "    return total_mae / fold, models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56054560317cc10b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Main Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66072fa-4589-43a9-bee2-9cc2f4415719",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T03:15:38.933906100Z",
     "start_time": "2024-03-12T01:50:05.156050300Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d66072fa-4589-43a9-bee2-9cc2f4415719",
    "outputId": "8a18a875-22d8-44cb-cb0e-789d2eb23374"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "TEST = True     # Test mode\n",
    "KFOLD = True    # K fold validation\n",
    "PREDICT = True  # Output prediction results\n",
    "\n",
    "# Testing mode\n",
    "if TEST:\n",
    "    # Divide datasets\n",
    "    subjects_adj = lr_train_A[:132]\n",
    "    subjects_ground_truth = hr_train_A[:132]\n",
    "    adj_test = lr_train_A[132:]\n",
    "    ground_truth = hr_train_A[132:]\n",
    "\n",
    "    # Not use k fold validation\n",
    "    if not KFOLD:\n",
    "        print(f\"TEST TRAINING ON {device}\")\n",
    "        model = train(subjects_adj, subjects_ground_truth, 150, ks = [0.9, 0.7, 0.6, 0.5])\n",
    "        # Save model\n",
    "        with open('models_CUDA_GAT_TEST.pkl', 'wb') as file:\n",
    "            pickle.dump(model, file)\n",
    "        # Save prediction results\n",
    "        if PREDICT:\n",
    "            predicts = predict(model, lr_test_A, 'predicts_agsr_GAT_test.csv')\n",
    "    \n",
    "    # Use k fold validation\n",
    "    else:\n",
    "        print(f\"USING KFOLD ON {device}\")\n",
    "        # Set up training parameters\n",
    "        train_paras = (100, [0.9, 0.7, 0.6, 0.5])\n",
    "        # Train with k fold (3 fold in this case) validation\n",
    "        avg_mae, models = train_with_kfold(np.array(lr_train_A), np.array(hr_train_A), train, train_paras, fold=3)\n",
    "        print(f'Average MAE over folds: {avg_mae}')\n",
    "        # Save model\n",
    "        with open('models_CUDA_GAT_KFOLDS.pkl', 'wb') as file:\n",
    "            pickle.dump(models, file)\n",
    "        # Save prediction results for every fold\n",
    "        if PREDICT:\n",
    "            for i in range(len(models)):\n",
    "                predicts = predict(models[i], lr_test_A, f'predictions_fold_{i+1}.csv')\n",
    "\n",
    "# Training mode\n",
    "else:\n",
    "    print(f\"TRAINING KFOLD ON {device}\")\n",
    "    # Training with the whole dataset\n",
    "    best_model = train(np.array(lr_train_A), np.array(hr_train_A), 150, ks = [0.9, 0.7, 0.6, 0.5])\n",
    "    # Save model\n",
    "    with open('model_CUDA_GAT.pkl', 'wb') as file:\n",
    "        pickle.dump(best_model, file)\n",
    "    # Save prediction results\n",
    "    predicts = predict(best_model, lr_test_A, 'predicts_agsr_best.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8c50d11fcaf43e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Training Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75a4f805b2cf046",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set up loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "\n",
    "def paper_train(subjects_adj, subjects_labels, validation_lr, validation_hr, epochs=50, ks=[0.9, 0.7, 0.6, 0.5], search_es = False):\n",
    "    \"\"\"\n",
    "    Train model and print model structure.\n",
    "    \"\"\"\n",
    "    best_mae = float('inf')\n",
    "    ctr = 0\n",
    "    # Early stopping counter for fair comparison\n",
    "    es = -1\n",
    "\n",
    "    # Build up model structure\n",
    "    hr_dim=320\n",
    "    lr_dim=160\n",
    "    model = GSATNet(ks).to(device)\n",
    "    print(model)\n",
    "    bce_loss = nn.BCELoss()\n",
    "    netD = Discriminator(320).to(device)\n",
    "    print(netD)\n",
    "    optimizerG = optim.Adam(model.parameters(), lr=0.0001)\n",
    "    optimizerD = optim.Adam(netD.parameters(), lr=0.0001)\n",
    "\n",
    "    # Start training\n",
    "    all_epochs_loss = []\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        with torch.autograd.set_detect_anomaly(True):\n",
    "            epoch_loss = []\n",
    "            epoch_error = []\n",
    "            model.train()\n",
    "            for lr, hr in zip(subjects_adj, subjects_labels):\n",
    "                optimizerD.zero_grad()\n",
    "                optimizerG.zero_grad()\n",
    "\n",
    "                hr = pad_HR_adj(hr, 26)\n",
    "                lr = torch.from_numpy(lr).type(torch.FloatTensor).to(device)\n",
    "                padded_hr = torch.from_numpy(hr).type(torch.FloatTensor).to(device)\n",
    "\n",
    "                eig_val_hr, U_hr = torch.linalg.eigh(\n",
    "                    padded_hr, UPLO='U')\n",
    "\n",
    "                model_outputs, net_outs, start_gcn_outs, layer_outs = model(\n",
    "                    lr, 160, 320)\n",
    "\n",
    "                mse_loss = 0.1 * criterion(net_outs, start_gcn_outs) + 0.8 * criterion(\n",
    "                    model.layer.weights, U_hr) + criterion(model_outputs, padded_hr)\n",
    "\n",
    "                error = criterion(model_outputs, padded_hr)\n",
    "                real_data = model_outputs.detach()\n",
    "                fake_data = gaussian_noise_layer(padded_hr)\n",
    "\n",
    "                d_real = netD(real_data)\n",
    "                d_fake = netD(fake_data)\n",
    "\n",
    "                dc_loss_real = bce_loss(d_real, torch.ones(hr_dim, 1).to(device))\n",
    "                dc_loss_fake = bce_loss(d_fake, torch.zeros(hr_dim, 1).to(device))\n",
    "                dc_loss = dc_loss_real + dc_loss_fake\n",
    "\n",
    "                dc_loss.backward()\n",
    "                optimizerD.step()\n",
    "\n",
    "                d_fake = netD(gaussian_noise_layer(padded_hr))\n",
    "\n",
    "                gen_loss = bce_loss(d_fake, torch.ones(hr_dim, 1).to(device))\n",
    "                generator_loss = gen_loss + mse_loss\n",
    "                generator_loss.backward()\n",
    "                optimizerG.step()\n",
    "\n",
    "                epoch_loss.append(generator_loss.item())\n",
    "                epoch_error.append(error.item())\n",
    "\n",
    "            # Print training results in every epoch\n",
    "            print(\"Epoch: \", epoch, \"Loss: \", np.mean(epoch_loss),\n",
    "                  \"Error: \", np.mean(epoch_error)*100, \"%\")\n",
    "            all_epochs_loss.append(np.mean(epoch_loss))\n",
    "            \n",
    "            if search_es:\n",
    "                print(f\"Now check for es.\")\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    pred_matrices = []\n",
    "                    gt_matrices = []\n",
    "    \n",
    "                    for lr, hr in zip(validation_lr, validation_hr):\n",
    "                        model_outputs, net_outs, start_gcn_outs, layer_outs = model(torch.tensor(lr, dtype=torch.float32).to(device), 160, 320)\n",
    "                        pred_matrices.append(unpad_matrix(model_outputs, padding_top, padding_bottom, padding_left, padding_right))\n",
    "                        gt_matrices.append(torch.tensor(hr, dtype=torch.float32))\n",
    "                        \n",
    "                    num_val = len(pred_matrices)\n",
    "                    pred_1d_list = []\n",
    "                    gt_1d_list = []\n",
    "                \n",
    "                    # Iterate over each test sample\n",
    "                    for i in range(num_val):\n",
    "                        # Vectorize matrices\n",
    "                        pred_1d_list.append(MatrixVectorizer.vectorize(pred_matrices[i].cpu().detach().numpy()))\n",
    "                        gt_1d_list.append(MatrixVectorizer.vectorize(gt_matrices[i].cpu().detach().numpy()))\n",
    "                \n",
    "                \n",
    "                    # Concatenate flattened matrices\n",
    "                    pred_1d = np.concatenate(pred_1d_list)\n",
    "                    gt_1d = np.concatenate(gt_1d_list)\n",
    "                \n",
    "                    # Compute metrics\n",
    "                    mae = mean_absolute_error(pred_1d, gt_1d)\n",
    "                    print(f\"MAE_validate: {mae}\")\n",
    "                    \n",
    "                    if mae < best_mae:\n",
    "                        if best_mae - mae > 0.0001:\n",
    "                            best_mae = mae\n",
    "                            print(f\"Best MAE: {best_mae}, PASS\")\n",
    "                            ctr = 0\n",
    "                        else:\n",
    "                            ctr += 1\n",
    "                            print(f\"Best MAE: {best_mae}, BUT UNDER THRESHOLD\")\n",
    "                    else:\n",
    "                        ctr += 1\n",
    "                        if ctr >= 10:\n",
    "                            if epoch >= 99:\n",
    "                                print(f\"Early stopping at epoch {epoch}!!!!!!!\")\n",
    "                                es = epoch\n",
    "                                break\n",
    "                            else:\n",
    "                                print(f\"Worse MAE for {ctr} times, continue training.\")\n",
    "                                continue\n",
    "            \n",
    "                    \n",
    "\n",
    "    return model, all_epochs_loss, es"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d77ab7e518738f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Evaluation Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce99cbc03edc64a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from evaluation import evaluate_all\n",
    "\n",
    "\n",
    "def paper_eval(model, lr_val_fold, hr_val_fold):\n",
    "    \"\"\"\n",
    "    Evaluate model performance in MAE, PCC, Jensen-Shannon Distance, Average MAE betweenness/eigenvector/PageRank centrality.\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pred_matrices = []\n",
    "        gt_matrices = []\n",
    "    \n",
    "        for lr, hr in zip(lr_val_fold, hr_val_fold):\n",
    "            model_outputs, net_outs, start_gcn_outs, layer_outs = model(torch.tensor(lr, dtype=torch.float32).to(device), 160, 320)\n",
    "            pred_matrices.append(unpad_matrix(model_outputs, padding_top, padding_bottom, padding_left, padding_right).cpu().detach().numpy())\n",
    "            gt_matrices.append(np.array(hr))\n",
    "            \n",
    "    return pred_matrices, gt_matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48a7e2898fba091",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Kfold Method Searching Early Stopping (Fair comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadbe554cbbdd179",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from MatrixVectorizer import MatrixVectorizer\n",
    "import evaluation\n",
    "def paper_kfold_es(train_func, train_params):\n",
    "    \n",
    "    # Load the data\n",
    "    global lr_test_fold, hr_test_fold, lr_train_fold, hr_train_fold, lr_validate_fold, hr_validate_fold\n",
    "    lrs = []\n",
    "    hrs = []\n",
    "    vlrs = []\n",
    "    vhrs = []\n",
    "        \n",
    "    for i in range(1,4):\n",
    "        lr_train_path = f'../../Cluster-CV2/Fold{i}/lr_train_split_{i}.csv'\n",
    "        hr_train_path = f'../../Cluster-CV2/Fold{i}/hr_train_split_{i}.csv'\n",
    "        lr_train_pd = pd.read_csv(lr_train_path)\n",
    "        hr_train_pd = pd.read_csv(hr_train_path)\n",
    "        lr_train = np.array(lr_train_pd.iloc[:-10].apply(lambda x: np.array(x), axis=1).tolist())\n",
    "        hr_train = np.array(hr_train_pd.iloc[:-10].apply(lambda x: np.array(x), axis=1).tolist())\n",
    "        \n",
    "        lr_validate = np.array(lr_train_pd.tail(10).apply(lambda x: np.array(x), axis=1).tolist())\n",
    "        hr_validate = np.array(hr_train_pd.tail(10).apply(lambda x: np.array(x), axis=1).tolist())\n",
    "        \n",
    "        lr_train_A = []\n",
    "        hr_train_A = []\n",
    "        lr_validate_A = []\n",
    "        hr_validate_A = []\n",
    "        \n",
    "        matrix_vectorizer = MatrixVectorizer()\n",
    "        \n",
    "        for g in lr_train:\n",
    "            lr_train_A.append(matrix_vectorizer.anti_vectorize(g, 160))\n",
    "        for g in hr_train:\n",
    "            hr_train_A.append(matrix_vectorizer.anti_vectorize(g, 268))\n",
    "            \n",
    "        for g in lr_validate:\n",
    "            lr_validate_A.append(matrix_vectorizer.anti_vectorize(g, 160))\n",
    "        for g in hr_validate:\n",
    "            hr_validate_A.append(matrix_vectorizer.anti_vectorize(g, 268))\n",
    "        \n",
    "        lrs.append(np.array(lr_train_A))\n",
    "        hrs.append(np.array(hr_train_A))\n",
    "        vhrs.append(np.array(hr_validate_A))\n",
    "        vlrs.append(np.array(lr_validate_A))\n",
    "        \n",
    "    # Train with kfold\n",
    "    models = []\n",
    "    losses = []\n",
    "    ess = []\n",
    "    for fold in range(0, 3):\n",
    "        print(f\"Fold {fold+1}\")\n",
    "        if fold == 0:\n",
    "            lr_train_fold = np.concatenate((lrs[1], lrs[2]))\n",
    "            hr_train_fold = np.concatenate((hrs[1], hrs[2]))\n",
    "            lr_validate_fold = np.concatenate((vlrs[1], vlrs[2]))\n",
    "            hr_validate_fold = np.concatenate((vhrs[1], vhrs[2]))\n",
    "            lr_test_fold = lrs[0]\n",
    "            hr_test_fold = hrs[0]\n",
    "        elif fold == 1:\n",
    "            lr_train_fold = np.concatenate((lrs[0], lrs[2]))\n",
    "            hr_train_fold = np.concatenate((hrs[0], hrs[2]))\n",
    "            lr_validate_fold = np.concatenate((vlrs[0], vlrs[2]))\n",
    "            hr_validate_fold = np.concatenate((vhrs[0], vhrs[2]))\n",
    "            lr_test_fold = lrs[1]\n",
    "            hr_test_fold = hrs[1]\n",
    "        elif fold == 2:\n",
    "            lr_train_fold = np.concatenate((lrs[0], lrs[1]))\n",
    "            hr_train_fold = np.concatenate((hrs[0], hrs[1]))\n",
    "            lr_validate_fold = np.concatenate((vlrs[0], vlrs[1]))\n",
    "            hr_validate_fold = np.concatenate((vhrs[0], vhrs[1]))\n",
    "            lr_test_fold = lrs[2]\n",
    "            hr_test_fold = hrs[2]\n",
    "        model, loss, es = train_func(lr_train_fold, hr_train_fold, lr_validate_fold, hr_validate_fold, train_params[0], train_params[1], search_es = True)\n",
    "        ess.append(es)\n",
    "        models.append(model)\n",
    "        losses.append(loss)\n",
    "    \n",
    "    return models, losses, ess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aec2fe39258be36",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Kfold Method for Model Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90279fa76a63023",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from MatrixVectorizer import MatrixVectorizer\n",
    "import evaluation\n",
    "def paper_kfold(train_func, train_params, ess):\n",
    "    \n",
    "    # Load the data\n",
    "    global lr_test_fold, hr_test_fold, lr_train_fold, hr_train_fold\n",
    "    lrs = []\n",
    "    hrs = []\n",
    "\n",
    "        \n",
    "    for i in range(1,4):\n",
    "        # !!!!!!!Please Change the path to the correct path for Cluster CV and Random CV.\n",
    "        lr_train_path = f'../../Cluster-CV2/Fold{i}/lr_train_split_{i}.csv'\n",
    "        hr_train_path = f'../../Cluster-CV2/Fold{i}/hr_train_split_{i}.csv'\n",
    "        lr_train_pd = pd.read_csv(lr_train_path)\n",
    "        hr_train_pd = pd.read_csv(hr_train_path)\n",
    "        lr_train = np.array(lr_train_pd.apply(lambda x: np.array(x), axis=1).tolist())\n",
    "        hr_train = np.array(hr_train_pd.apply(lambda x: np.array(x), axis=1).tolist())\n",
    "\n",
    "        \n",
    "        lr_train_A = []\n",
    "        hr_train_A = []\n",
    "\n",
    "        \n",
    "        matrix_vectorizer = MatrixVectorizer()\n",
    "        \n",
    "        for g in lr_train:\n",
    "            lr_train_A.append(matrix_vectorizer.anti_vectorize(g, 160))\n",
    "        for g in hr_train:\n",
    "            hr_train_A.append(matrix_vectorizer.anti_vectorize(g, 268))\n",
    "            \n",
    "\n",
    "        \n",
    "        lrs.append(np.array(lr_train_A))\n",
    "        hrs.append(np.array(hr_train_A))\n",
    "\n",
    "        \n",
    "    # Train with kfold\n",
    "    models = []\n",
    "    losses = []\n",
    "    for fold in range(0, 3):\n",
    "        print(f\"Fold {fold+1}\")\n",
    "        if fold == 0:\n",
    "            lr_train_fold = np.concatenate((lrs[1], lrs[2]))\n",
    "            hr_train_fold = np.concatenate((hrs[1], hrs[2]))\n",
    "            lr_test_fold = lrs[0]\n",
    "            hr_test_fold = hrs[0]\n",
    "        elif fold == 1:\n",
    "            lr_train_fold = np.concatenate((lrs[0], lrs[2]))\n",
    "            hr_train_fold = np.concatenate((hrs[0], hrs[2]))\n",
    "            lr_test_fold = lrs[1]\n",
    "            hr_test_fold = hrs[1]\n",
    "        elif fold == 2:\n",
    "            lr_train_fold = np.concatenate((lrs[0], lrs[1]))\n",
    "            hr_train_fold = np.concatenate((hrs[0], hrs[1]))\n",
    "            lr_test_fold = lrs[2]\n",
    "            hr_test_fold = hrs[2]\n",
    "        model, loss, es = train_func(lr_train_fold, hr_train_fold, None, None, ess[fold]+1, train_params[1], search_es = False)\n",
    "        models.append(model)\n",
    "        losses.append(loss)\n",
    "        pred_matrices, gt_matrices = paper_eval(model, lr_test_fold, hr_test_fold)\n",
    "        evaluate_all(np.array(pred_matrices), np.array(gt_matrices), output_path=f'ID-randomCluster2-{fold}-fixed.csv')\n",
    "    return models, losses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d6ed7d8e395de3",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Main Method for Paper Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e8aeba46933688",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_params = (300, [0.9, 0.7, 0.6, 0.5])\n",
    "model, losses, ess = paper_kfold_es(paper_train, train_params)\n",
    "print(f\"Losses: {losses}\")\n",
    "print(f\"Early Stoppings: {ess}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
