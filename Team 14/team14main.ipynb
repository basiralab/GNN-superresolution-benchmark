{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "\n",
    "from evaluation_metric import evaluate_all\n",
    "import pandas as pd\n",
    "import torch\n",
    "import random \n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm\n",
    "\n",
    "from MatrixVectorizer import MatrixVectorizer           \n",
    "from preprocessing import antivectorize_df\n",
    "from model import GSRNet, Discriminator\n",
    "from train import train_gan, test_gan\n",
    "from utils import track_memory, compute_degree_matrix_normalization_batch_numpy, get_parser, evaluate, plot_metrics_fold, LR_size, HR_size"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reproducibility code\n",
    " - Our code is adjusted to run on gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 42\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "# Check for CUDA (GPU support) and set device accordingly\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"CUDA is available. Using GPU.\")\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed)  # For multi-GPU setups\n",
    "    # Additional settings for ensuring reproducibility on CUDA\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data\n",
    " - The following code is used on the first run to read the dataset and antivectorize it in a form where it can be loaded for further runs. For first time run uncomment the following code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_LR_train1 = pd.read_csv(\"lr_train_split_1.csv\")\n",
    "A_HR_train1 = pd.read_csv(\"hr_train_split_1.csv\")\n",
    "\n",
    "A_LR_train2 = pd.read_csv(\"lr_train_split_2.csv\")\n",
    "A_HR_train2 = pd.read_csv(\"hr_train_split_2.csv\")\n",
    "\n",
    "A_LR_train3 = pd.read_csv(\"lr_train_split_3.csv\")\n",
    "A_HR_train3 = pd.read_csv(\"hr_train_split_3.csv\")\n",
    "\n",
    "np.save('A_LR_train_matrix1.npy', antivectorize_df(A_LR_train1, LR_size))\n",
    "np.save('A_HR_train_matrix1.npy', antivectorize_df(A_HR_train1, HR_size))\n",
    "np.save('A_LR_train_matrix2.npy', antivectorize_df(A_LR_train2, LR_size))\n",
    "np.save('A_HR_train_matrix2.npy', antivectorize_df(A_HR_train2, HR_size))\n",
    "np.save('A_LR_train_matrix3.npy', antivectorize_df(A_LR_train3, LR_size))\n",
    "np.save('A_HR_train_matrix3.npy', antivectorize_df(A_HR_train3, HR_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_LR_train_matrix1 = np.load('A_LR_train_matrix1.npy')\n",
    "A_HR_train_matrix1 = np.load('A_HR_train_matrix1.npy')\n",
    "A_LR_train_matrix2 = np.load('A_LR_train_matrix2.npy')\n",
    "A_HR_train_matrix2 = np.load('A_HR_train_matrix2.npy')\n",
    "A_LR_train_matrix3 = np.load('A_LR_train_matrix3.npy')\n",
    "A_HR_train_matrix3 = np.load('A_HR_train_matrix3.npy')\n",
    "\n",
    "\n",
    "print(A_LR_train_matrix1.shape)\n",
    "print(A_HR_train_matrix1.shape)\n",
    "print(A_LR_train_matrix2.shape)\n",
    "print(A_HR_train_matrix2.shape)\n",
    "print(A_LR_train_matrix3.shape)\n",
    "print(A_HR_train_matrix3.shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters\n",
    " - The optimal parameters based on our carried out experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = get_parser()\n",
    "# Create an empty Namespace to hold the default arguments\n",
    "args = parser.parse_args([])\n",
    "pprint(args.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_items_train = [A_LR_train_matrix1, A_LR_train_matrix2, A_LR_train_matrix3]\n",
    "graph_items_test = [ A_HR_train_matrix1, A_HR_train_matrix2, A_HR_train_matrix3]\n",
    "for matrix1, matrix2 in zip(graph_items_train, graph_items_test):\n",
    "    matrix1 = compute_degree_matrix_normalization_batch_numpy(matrix1)\n",
    "    matrix2 = compute_degree_matrix_normalization_batch_numpy(matrix2)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Fold Cross Validation\n",
    " - This is the cross-validation loop. We use KFold cross-validation to split \n",
    "the data into training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv = KFold(n_splits=args.splits, random_state=random_seed, shuffle=True)\n",
    "\n",
    "best_model_fold_list = []\n",
    "data_fold_list = []\n",
    "i = 1\n",
    "# Store the fold results\n",
    "fold_results = []\n",
    "\n",
    "print(f\"Starting Cross Validation.\")\n",
    "track_memory()\n",
    "for index in range(len(graph_items_test)):\n",
    "    print(f\"----- Fold {i} -----\")\n",
    "    track_memory()\n",
    "\n",
    "    train = graph_items_train[index]\n",
    "    test = graph_items_test[index]\n",
    "\n",
    "    # subjects_adj, test_adj, subjects_ground_truth, test_ground_truth = (\n",
    "    #     X[train_index],\n",
    "    #     X[test_index],\n",
    "    #     Y[train_index],\n",
    "    #     Y[test_index],\n",
    "    # )\n",
    "    # data_fold_list.append(\n",
    "    #     (subjects_adj, test_adj, subjects_ground_truth, test_ground_truth)\n",
    "    # )\n",
    "    # Create a deep copy of list1\n",
    "    new_train = copy.deepcopy(graph_items_train)\n",
    "    new_train.pop(index)\n",
    "    new_test = copy.deepcopy(graph_items_test)\n",
    "    new_test.pop(index)\n",
    "    new_train = np.concatenate(new_train, axis=0)\n",
    "    new_test = np.concatenate(new_test, axis=0)\n",
    "\n",
    "    # # Remove the item at the given index\n",
    "    # new_list.pop(index_to_remove)\n",
    "\n",
    "    netG = GSRNet(args).to(device)\n",
    "    optimizerG = optim.Adam(netG.parameters(), lr=args.lr)\n",
    "\n",
    "    netD = Discriminator(args).to(device)\n",
    "    optimizerD = optim.Adam(netD.parameters(), lr=args.lr)\n",
    "\n",
    "    # track_memory()\n",
    "    # GAN model\n",
    "\n",
    "    return_model = train_gan(\n",
    "        netG,\n",
    "        optimizerG,\n",
    "        netD,\n",
    "        optimizerD,\n",
    "        new_train,\n",
    "        new_test,\n",
    "        args,\n",
    "        test_adj=train,\n",
    "        test_ground_truth=test,\n",
    "    )\n",
    "\n",
    "    test_mae, _ = test_gan(return_model, train, test, args, to_file=False)\n",
    "    train_mae,_ = test_gan(return_model, new_train, new_test, args)\n",
    "    pred_val_matrices = np.zeros((268, 268))\n",
    "    with torch.no_grad():\n",
    "        pred_train_matrices = []\n",
    "        for j, test_adj in enumerate(train):\n",
    "            return_model.eval()\n",
    "            pred = return_model(torch.from_numpy(test_adj))[0]\n",
    "            pred = torch.clamp(pred, min=0.0, max=1.0)\n",
    "            pred = pred.cpu()\n",
    "            pred_train_matrices.append(pred)\n",
    "\n",
    "        print(\"Train\")\n",
    "        pred_train_matrices = np.array(pred_train_matrices)\n",
    "        evaluate_all(test, pred_train_matrices)\n",
    "    print(f\"Train MAE: {train_mae:.6f}, Val MAE: {test_mae:.6f}\")\n",
    "    best_model_fold_list.append(return_model)\n",
    "    # Evaluate the model on the test set and log the results\n",
    "    # predicted_test_output_matrices = return_model(torch.Tensor(train))\n",
    "    # metrics = evaluate_all(\n",
    "    #     train, predictions\n",
    "    # )\n",
    "\n",
    "    track_memory()\n",
    "\n",
    "    i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CAL_GRAPH = False\n",
    "\n",
    "res_list = []\n",
    "\n",
    "for i in range(args.splits):\n",
    "    _, test_adjs, _, gt_matrices = data_fold_list[i]\n",
    "    model = best_model_fold_list[i]\n",
    "    model.eval()\n",
    "    pred_matrices = np.zeros(gt_matrices.shape)\n",
    "    with torch.no_grad():\n",
    "        for j, test_adj in enumerate(test_adjs):\n",
    "            pred = model(torch.from_numpy(test_adj))[0]\n",
    "            pred = torch.clamp(pred, min=0.0, max=1.0)\n",
    "            pred = pred.cpu()\n",
    "            pred_matrices[j] = pred\n",
    "    res_list.append(evaluate(pred_matrices, gt_matrices, cal_graph=CAL_GRAPH))\n",
    "\n",
    "pd.DataFrame(res_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics_fold(res_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Discussion:* This is the final scores of our 3-Fold cross validation based on the MAE, PCC and JSD metrics. The first plot is generated by setting the CAL_GRAPH Flag to false. On our complete final plot we can see that Mean Absolute Error (MAE) ranges from 0.1281 to 0.1378. The model predicts HR samples with a level of accuracy, but there is still room for improvement. Pearson Correlation Coefficients (PCC) are consistently above 0.63, indicating a moderately strong positive correlation between the predicted HR value and the ground truth. This shows that the model successfully captures the general trend of the data. Jensen-Shannon Distance (JSD) remains around 0.28, showing that the predicted HR value partially diverged from the ground truth. Lastly, the average MAE with 3 different centrality types is very low, signifying that the model’s prediction captures ground truth’s network structure very well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(args.splits):\n",
    "    _, test_adjs, _, gt_matrices = data_fold_list[i]\n",
    "    model = best_model_fold_list[i]\n",
    "    model.eval()\n",
    "\n",
    "    output_pred_list = []\n",
    "    with torch.no_grad():\n",
    "        for test_adj in tqdm(test_adjs):\n",
    "            output_pred = model(torch.from_numpy(test_adj))[0].cpu()\n",
    "            output_pred = torch.clamp(output_pred, min=0.0, max=1.0)\n",
    "            output_pred = MatrixVectorizer.vectorize(output_pred).tolist()\n",
    "            output_pred_list.append(output_pred)\n",
    "\n",
    "    output_pred_stack = np.stack(output_pred_list, axis=0)\n",
    "    output_pred_1d = output_pred_stack.flatten()\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"ID\": [i + 1 for i in range(len(output_pred_1d))],\n",
    "            \"Predicted\": output_pred_1d.tolist(),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    df.to_csv(\"predictions_fold_\" + str(i + 1) + \".csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Model\n",
    " - Below is the final train split performed on the parameter combination that performed the best on our KFold cross validation experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_HR_train = pd.read_csv(\"../data/hr_train.csv\")\n",
    "\n",
    "pca = PCA(n_components=0.99, whiten=False)\n",
    "A_HR_train_pca = pca.fit_transform(A_HR_train)\n",
    "print(f\"HR Train PCA shape: {A_HR_train_pca.shape}\")\n",
    "\n",
    "gm = GaussianMixture(n_components=5, random_state=random_seed)\n",
    "A_HR_train_label = gm.fit_predict(A_HR_train_pca)\n",
    "unique, counts = np.unique(A_HR_train_label, return_counts=True)\n",
    "print(np.asarray((unique, counts)).T)\n",
    "\n",
    "X = np.load(\"A_LR_train_matrix.npy\")\n",
    "y = np.load(\"A_HR_train_matrix.npy\")\n",
    "\n",
    "X = compute_degree_matrix_normalization_batch_numpy(X)\n",
    "\n",
    "n_sample = X.shape[0]\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X.reshape(n_sample, -1),\n",
    "    y.reshape(n_sample, -1),\n",
    "    test_size=0.10,\n",
    "    random_state=random_seed,\n",
    "    stratify=A_HR_train_label,\n",
    ")\n",
    "\n",
    "X_train = X_train.reshape(-1, LR_size, LR_size)\n",
    "X_val = X_val.reshape(-1, LR_size, LR_size)\n",
    "y_train = y_train.reshape(-1, HR_size, HR_size)\n",
    "y_val = y_val.reshape(-1, HR_size, HR_size)\n",
    "\n",
    "print(\"Train size:\", len(X_train))\n",
    "print(\"Val size:\", len(X_val))\n",
    "\n",
    "netG = GSRNet(args).to(device)\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=args.lr)\n",
    "\n",
    "netD = Discriminator(args).to(device)\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=args.lr)\n",
    "\n",
    "track_memory()\n",
    "# GAN model\n",
    "final_model = train_gan(\n",
    "    netG,\n",
    "    optimizerG,\n",
    "    netD,\n",
    "    optimizerD,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    args,\n",
    "    test_adj=X_val,\n",
    "    test_ground_truth=y_val,\n",
    ")\n",
    "track_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(args.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation_metric import evaluate_all\n",
    "# final_model = model\n",
    "# final_model.eval()\n",
    "pred_val_matrices = np.zeros((268, 268))\n",
    "with torch.no_grad():\n",
    "    for model in fold_results:\n",
    "        pred_train_matrices = []\n",
    "        for j, test_adj in enumerate(train):\n",
    "            model.eval()\n",
    "            pred = model(torch.from_numpy(test_adj))[0]\n",
    "            pred = torch.clamp(pred, min=0.0, max=1.0)\n",
    "            pred = pred.cpu()\n",
    "            pred_train_matrices.append(pred)\n",
    "\n",
    "        print(\"Train\")\n",
    "        pred_train_matrices = np.array(pred_train_matrices)\n",
    "        evaluate_all(test, pred_train_matrices)\n",
    "\n",
    "    # for j, test_adj in enumerate(new_train):\n",
    "    #     pred = final_model(torch.from_numpy(test_adj))[0]\n",
    "    #     pred = torch.clamp(pred, min=0.0, max=1.0)\n",
    "    #     pred = pred.cpu()\n",
    "    #     pred_val_matrices[j] = pred\n",
    "\n",
    "    # print(\"Val\")\n",
    "    # evaluate_all(new_test, pred_val_matrices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_pred_list = []\n",
    "final_model = return_model\n",
    "final_model.eval()\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(A_LR_test_matrix.shape[0])):\n",
    "        output_pred = final_model(torch.Tensor(A_LR_test_matrix[i]))[0]\n",
    "        output_pred = torch.clamp(output_pred, min=0.0, max=1.0)\n",
    "        output_pred = output_pred.cpu()\n",
    "        output_pred = MatrixVectorizer.vectorize(output_pred).tolist()\n",
    "        output_pred_list.append(output_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_pred_stack = np.stack(output_pred_list, axis=0)\n",
    "output_pred_1d = output_pred_stack.flatten()\n",
    "assert output_pred_1d.shape == (4007136,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"ID\": [i + 1 for i in range(len(output_pred_1d))],\n",
    "        \"Predicted\": output_pred_1d.tolist(),\n",
    "    }\n",
    ")\n",
    "\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note:* These are the predicted outputs of our best model that were submitted to the Kaggle competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"final_model.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
